\documentclass{statsoc}



\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}

\bibliographystyle{chicago} 




\title[Trend-Seasonal Interactions]{A Bayesian Semiparametric Approach for Trend-Seasonal Interactions: an Application to Migration Forecasts}
\author[A. Milivinti and G. Benini]{Alice Milivinti}
\address{Universit\'{e} de Gen\`{e}ve, 1211 Gen\`{e}ve, Switzerland.}
\email{a.lice.milivinti@gmail.com}
\author{Giacomo Benini}
\address{Universit\'{e} de Gen\`{e}ve, 1211 Gen\`{e}ve, Switzerland.}


\begin{document}


\begin{abstract}

The current paper models complex trend-seasonal interactions within a Bayesian framework. The contribution devides in two parts. First, it proves, via a set of simulations, that a semiparametric specification of the interplay between the seasonal cycle and the global time trend outperforms parametric and nonparametric alternatives when the seasonal behavior is represented by Fourier series of order bigger than one. Second, the paper uses a Bayesian framework to forecast Swiss immigration merging the simulations' outcome with a set of priors derived from alternative hypothesis about the future number of incomers. The result is an effective symbiosis between Bayesian probability and semiparametric flexibility able to reconcile past observations with unprecedented expectations. 

\end{abstract}

\keywords{Trend-Seasonal Interaction, Bayesian Forecast, Fourier Series, Semiparametric, Immigration.} \\


<<data import, cache=TRUE, echo=FALSE, include=FALSE, warning=FALSE>>=
rm(list = ls())

library(bayesplot)
library(fields)
library(knitr)
library(tidyr)
library(dplyr)
library(LaplacesDemon)
library(forecast)
#library(tseries)
library(nlme)
library(mgcv)
library(gamm4)
library(hydroGOF)
library(brms)
library(loo)
library(MuMIn)
library(xtable)
library(car)
library(rgl)
library(ggplot2)
library(cowplot)
library(gridExtra)
library(rstan)
#library(ggplot2)
#library(reshape2)
#library(dplyr)
library(DataCombine)
library(data.table)

#devtools::install_github("paul-buerkner/brms")

ts <- read.csv("/path/to/file/Immigration time series.csv", sep = ",", header = TRUE, stringsAsFactors = FALSE)


ts$X<-NULL
ts$interaction <- ts$trend*ts$Month
ts$Month <- as.factor(ts$Month)
ts$Date <- as.Date(ts$Date)

#Subset time series into train data and test data
train <- ts[which(ts$Year < 2004),]
test <- ts[which(ts$Year > 2003),]

#Declare data as time series
ts1 <- ts[, c("xlog")]
ts1 <- ts(ts1, start = c(1981, 1), end = c(2013, 12), frequency=12)

#MAPE
mape <- function(y, yhat){
  mean(abs((y - yhat)/y))
}


@



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In a world characterized by low fertility and increasing life expectancy, human mobility has gained prominence in driving population change. Any forward-looking policy should be designed around its forecast. However, a common belief around its non-repeatability and non-traceability has left most migration projections dependent upon deterministic methods until the mid 1990s. Nevertheless, being unquestionable that forecasting is not only about modelling, but also about quantifying uncertainty, these na\"{\i}ve approaches were progressively abandoned and substituted by standard probabilistic techniques like time series analysis \citep{lee1994stochastic, de1997effect, keilman2002population, wilson2004australia, raymer2012does} and generalized linear regressions \citep{schmidt2000aggregate, alvarez2003potential, cohen2008international, cappelen2014forecasting}. \\ 
Even though this transition represented a significant leap forward in statistical rigor \citep{lutz2004}, it was not immune from criticism. In particular, the substitution of a subjective approach with a set of data-driven methods has created a series of specifications unable to reconcile the expectations of the researchers with the information contained in the data. This major shortcoming represents a significant threat to the precision of out-of-sample forecasts, especially in the case of unique episodes. Suppose, for example, that a country signs a treaty which guarantees the free movement of people from neighbouring nations. Any prediction which ignores such unprecedented event would most probably produce large errors. \\
A possible way to prevent this type of mistakes is to adopt a Bayesian prospective \citep{bijak2010bayesian, bijak2010forecasting, Billari2014, azose2015bayesian, azose2016probabilistic} able to condition the future uncertainty on past information and future intuitions, conjugating the "\textit{correspondence to the observed reality}" with "\textit{the awareness of multiple perspectives}" \citep{gelman2017beyond}. This more convoluted practice is implemented at the expenses of a closed-form solution and the corresponding reliance on numerically intensive algorithms which require an \textit{a priori} hypothesis about the prior distribution of the structural parameters and an \textit{a posteriori} sampling of their probability distribution. Frequently, such a computationally intense way to manage the data is compensated assuming a linear relation between the dependent and the independent variables. This simplification fastens the speed of the algorithm's rate-of-convergence and delivers easy-to-interpret estimates \citep{blake2012applied}. Nonetheless, a rigid linear specification might fail to describe the migration process ignoring the complexity of global and seasonal trends, as well as, the anomalies related to the interdependency across migration flows.\\   
The present paper explores the possibility of a non-linear interaction between long and short run migration within a Bayesian framework. Empirically, the global tendency is defined as a time trend and the seasonality as a sum of Fourier series. Being particularly malleable, the latter can conveniently model multiple seasonal spikes by simply increasing its order. These two components become the arguments of an unknown bivariate smooth function, which relaxes the hypothesis that trend and seasonality evolve independently \citep{koopman2009seasonality, hindrayanto2014trend}. The nonparametric nature of the interaction does not impose a rigid structure to the trend-seasonal co-movements, returning an Additive Model (AMI). At the same time the Bayesian prospective reconciles the AMI with the long standing demographic tradition of including experts' opinions into the predictions through the use of informative priors.\\
In order to test the statistical properties of the model we run a set of simulations. The AMI outperforms, in terms of predictive accuracy, the alternatives, especially if the seasonal component has more than one spike. Given its good performance, we train the AMI on Swiss monthly data to test its capacity to predict the number of arrivals. Like in the simulation case, the AMI predicts the left-out data better than all the other specifications, suggesting, both for long and short run predictions, a non negligible amount of non-linear trend-seasonal interaction. Consequently, we set up altervative forecast scenarios for Swiss immigration flows until 2023 by making an active use of informative priors. Finally, we extend the model to a longitudinal analysis, producing age specific forecasts. This last case reduces the impact of the interaction, while still returning high degrees of non-linearity.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodological Framework}\label{methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model Construction}\label{model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The basic model of the paper decomposes the log-transformation of the number of monthly arrivals, $ \log Y_{t} = y_{t} $, into a trend and a seasonal component,  
\begin{align}\label{decompose}
y_{t} = \text{trend}_{t}+\text{seasonality}_{t}+\text{e}_{t}, \quad t = 1, 2, ..., T.
\end{align}
The first element of equation (\ref{decompose}) is the global direction of the series, defined as $ t/T $, while the second one is the seasonal cycle obtained by averaging the "de-trended" series for each month over all periods. The random part of the series, $  \text{e}_{t} $, is obtained removing the sum $( \text{trend}_{t} + \text{seasonality}_{t} )$ from the original time series. \\
The seasonal part of the regression can be parsimoniously modeled using a harmonic series. Recalling Fourier theorem, a periodic function, with period $p = 2 \pi / \varpi $, can be rewritten using the cosine function $\cos(\varpi(t))$. For example, an annual seasonal pattern ($p=12$) has a frequency $\varpi= \pi / 6$. All the same, a model which includes only a cosine function would assume by default that the annual peak is the first period of the season, normally January. In order to generalize this result, and allow the model to shift the initial spike, it is sufficient to add a phase shifter, such that
\begin{align}\label{shift_cos}
\text{seasonality}_{t} = a_{t} \cos(\varpi (t)+\theta), 
\end{align}
where $ \theta $ is the magnitude of the shift. Given that equation (\ref{shift_cos}) can be rewritten as the sum of cosine and sine, it is possible to express the whole seasonality as
\begin{align}\label{fourier_1}
\text{seasonality}_{t} = a_{t} \cos(\varpi (t))+b_{t} \sin(\varpi (t)).
\end{align}

Equation (\ref{fourier_1}) reproduces a cyclic behaviour with a single peak. In order to allow for multiple spikes it is necessary to add further sine and cosine terms up to the desired order. For example the N-th seasonality would be 
\begin{align}\label{fourier_N}
\text{seasonality}_{t} = a_{t} \cos(\varpi (t)) + b_{t} \sin(\varpi (t)) + ... + v_{t} \cos(\varpi (Nt)) + z_{t} \sin(\varpi (Nt)), 
\end{align}
where $ [ a_{t},  b_{t}, ...,  v_{t},  z_{t}] $ define discretionary amplitudes. Substituting equation (\ref{fourier_N}) into the fundamental model, allows us to rewrite (\ref{decompose}) as a Linear Model with No Interaction (LMNI),
\begin{align}\label{fourier_lm_no}
y_{t}=\beta_{0}+\beta_{1}\text{trend}_{t}+\sum_{i=1}^{N}\beta_{2i}(\cos_{it}+\sin_{it})+\epsilon_{t}, \quad \epsilon_{t} \sim N(0, \sigma_{\epsilon}^2),
\end{align}
where, we simply sum the trend and the cyclical components using $\cos_{it} = \cos(\varpi (it))$ and $\sin_{it} = \sin(\varpi (it))$.

Regression (\ref{fourier_lm_no}) allows for multiple peaks of a seasonal component which does not need to be in January. Furthermore, it accounts for the possibility that the unconditional mean of the weakly stationary process $y_{t}$ is different from zero and equal to $\beta_{0}$. However, it leaves unchanged the fundamental assumption that $ y_{t} $ can be expressed as a combination of short-run waves and of a long-run component. Nonetheless, this does not always hold since seasonal times series often display a cycle amplitude which varies for different stages of the trend. Said differently, there might be an interaction between seasonality and trend, which transforms equation (\ref{fourier_lm_no}) into a Linear Model with Interaction (LMI),
\begin{align}\label{fourier_lm_int}
y_{t}=\beta_{0}+\beta_{1}\text{trend}_{t}+\sum_{i=1}^{N}\beta_{2i}(\cos_{it}+\sin_{it})+\sum_{i=1}^{N}\beta_{3i}\text{trend}_{t}*(\sin_{it}+\cos_{it})+\epsilon_{t}.
\end{align}

A frequentist estimation of equation (\ref{fourier_lm_int}) presents two major shortcomings. First, in contrast to a long standing tradition in demographic studies, it does not allow to introduce a judgmental component to the empirical analysis. Second, the interaction can impact on the number of incomers only linearly. This last restriction can be particularly limiting since there is no empirical evidence supporting the idea that the interaction is linear or of any other specific functional form like $ \exp(\text{trend}_{t}*(\sin_{it}+\cos_{it})) $ \citep{koopman2009seasonality}. The combination of these two limitations might translates into estimates far away in probability from the true unobserved parameters. In order to see why, let us consider the following example. A researcher collects monthly migration inflows data to a small open economy well integrated in the global division of labour. An explanatory analysis of the time series suggests that, historically, there is an interaction between the evolution of the trend and the amplitude of the seasonality. In particular, the interaction changes for different stages of the global business cycle, which is expected to expand in the upcoming months. \\*
A short-run forecast based on a frequentist interpretation of equation (\ref{fourier_lm_no}) would ignore the trend-cycle interaction, while equation (\ref{fourier_lm_int}) would only allows for a very specific impact of $ (\text{trend}, \cos + \sin) $ on $ y $. A Bayesian Additive Model with Interaction (AMI), 
\begin{align}\label{fourier_gam_int}
y_{t}=\beta_{0}+f_{1}(\text{trend}_{t})+\sum_{i=1}^{N}f_{2i}(\cos_{it}+\sin_{it})+\sum_{i=1}^{N}f_{3i}(\text{trend}_{t}, \cos_{it}+\sin_{it})+\epsilon_{t},
\end{align}
generalizes the previous expressions allowing the impact of all the explanatory variables to be non-linear while including the expected distribution of the model's parameter $\beta_{0}$ and functions $ [f_{1} \quad f_{21} \quad \sum_{i} f_{2i} \quad \sum_{i} f_{3i}] $. The flexible form of $f_{3i}(.)$ allows to compute complex and potentially non-linear interactions between trend and seasonality, which might be driven by the business cycle. Furthermore, a prior about the distribution of the explanatory variables can incorporate experts' expectations about the increasing variability brought about by an unprecedented event such as the completion of a commercial agreement or the burst of a war potentially improving the model's performance.\\*
In order to see these two properties working in practice it is necessary to introduce the estimation technique used to fit (\ref{fourier_gam_int}). Using a first order Fourier series with interaction, 
\begin{align}\label{fourier_gam_int_1st}
y_{t}=\beta_{0}+f_{1}(\text{trend}_{t})+f_{21}(\cos_{1t}+\sin_{1t})+f_{31}(\text{trend}_{t}, \cos_{1t}+\sin_{1t})+\epsilon_{t},
\end{align}
we illustrate the estimation process step-by-step. The first one is to choose which nonparametric technique to implement. Among the different options, thin plate regression splines tend to outperform, at least in finite samples, more traditional alternatives, such as kernels, marginal integration and local polynomial approximation \citep{wood2003thin}. This method requires to choose a base for each unknown function. For example, the nonparametric terms of equation (\ref{fourier_gam_int_1st}) can be re-expressed as
\begin{align}
f_{1}(\cdot)  =& \sum_{k=1}^{K_{1}} \beta_{1k}b_{1k}(\text{trend}_{t}) \label{gam_trend1}  \\ 
f_{21}(\cdot) =& \sum_{k=1}^{K_{21}} \beta_{21k}b_{21k}(\cos_{1t}+\sin_{1t}) \label{gam_trend2}  \\ 
f_{31}(\cdot) =& \sum_{k=1}^{K_{31}} \beta_{31k}b_{31k}(\text{trend}_{t}, \cos_{1t}+\sin_{1t}), \label{gam_trend3}
\end{align}
where $ (\beta_{1k}, \beta_{21k}, \beta_{31k}) $ are unknown vectors of parameters, $ (b_{1k}(.), b_{21k}(.), b_{31k}(.))  $ are basis functions and $ k = [1, 2, ..., K_{1}] $, $ k = [1, 2, ..., K_{21}] $ and $ k = [1, 2, ..., K_{31}] $ are, respectively, the number of knots used to fit $ f_{1}(.) $, $ f_{21}(.)$ and $ f_{31}(.)$. Estimating equation (\ref{fourier_gam_int_1st}) using (\ref{gam_trend1}), (\ref{gam_trend2}) and (\ref{gam_trend3}) usually leads to an identification problem. A possible wayout is to center each smooth, such that, $ 1^{T}\mathbf{\tilde{X}}_{k}\tilde{\beta}_{k} = 0 $, where
\begin{align*}\label{}
\mathbf{\tilde{X}}_{k} = [\mathbf{\tilde{X}}_{K_{1}} : \mathbf{\tilde{X}}_{K_{21}} : \mathbf{\tilde{X}}_{K_{31}}] \quad \text{and} \quad \tilde{\beta}_{k} = [\tilde{\beta}_{K_{1}} : \tilde{\beta}_{K_{21}} : \tilde{\beta}_{K_{31}}],
\end{align*}
with $ \mathbf{\tilde{X}}_{K_{1}} = [b_{11}(.), b_{12}(.), ..., b_{1K_{1}}(.)] $, $ \mathbf{\tilde{X}}_{K_{21}} = [b_{211}(.), b_{212}(.), ..., b_{21K_{21}}(.)] $, \\ $ \mathbf{\tilde{X}}_{K_{31}} = [b_{311}(.), b_{312}(.), ..., b_{31K_{31}}(.)] $, $ \tilde{\beta}_{K_{1}} = [\beta_{11}, \beta_{12}, ..., \beta_{1K_{1}}]^{T} $, $ \tilde{\beta}_{K_{21}} = [\beta_{211}, \beta_{212}, ..., \beta_{21K_{21}}]^{T} $ and $ \tilde{\beta}_{K_{31}} = [\beta_{311}, \beta_{312}, ..., \beta_{31K_{31}}]^{T} $.  To ensure  $ 1^{T}\mathbf{\tilde{X}}_{k}\tilde{\beta}_{k} = 0 $,  $ \mathbf{\tilde{X}}_{k}\tilde{\beta}_{k} $ is reparametrized using a single Householder matrix $ \mathbf{Z} $, such that
\begin{align*}\label{}
\mathbf{X}_{k} = \mathbf{\tilde{X}}_{k}\mathbf{Z} \quad \text{and} \quad \tilde{\beta}_{k} = \mathbf{Z}\beta_{k}.
\end{align*}
Once the matrices have been centered the expected value of equation (\ref{fourier_gam_int_1st}),
\begin{align}\label{conditional exp}
\mathbb{E}[y_{t} | \text{trend}_{t}, \cos_{1t} + \sin_{1t}] = \mathbf{X}_{kt} \mathbf{\beta},
\end{align}

with $\beta^{T} = [\beta_{0}^{T}, \beta_{K1}^{T}, \beta_{K21}^{T}, \beta_{K31}^{T}] $ can be estimated using a standard likelihood function. However, if the number of knots is large enough, specification (\ref{gam_trend1}), (\ref{gam_trend2}) and (\ref{gam_trend3}) would probably overfit the data. Therefore, thin plate regression splines replace the standard likelihood with a penalized one, 
\begin{align}\label{like}
l_{p}(\beta)=l(\beta)-\frac{1}{2}\sum_{k}\lambda_{k}\beta^{T} \mathbf{S} \beta,
\end{align}
where, $ \lambda $ is an unknown smoothing parameter and $ \mathbf{S} $ is a linear modification of a penalty, which measures the wiggliness of $f_{1}(\cdot)$, $f_{21}(\cdot) $ and $ f_{31}(\cdot) $ as a quadratic form in the coefficients of the function. Likelihood (\ref{like}) can be insert into a standard Bayesian procedure, setting the prior distributions of $\beta$ and $\lambda$, computing the likelihoods and sampling from the posterior distributions. Further mathematical details are available in the Additional Material.     


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Simulations and Priors' Selection}\label{prior_sim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Equations (\ref{fourier_lm_no}), (\ref{fourier_lm_int}) and (\ref{fourier_gam_int}) are estimated using the Stan language provided by the \texttt{brms} package of the statistical software R \citep{buerkner2016package}. The first step to calculate the regressions is to set the priors. Given that, we want to test the properties of the different models via a set of statistical simulations, we opt for weakly informative priors as provided by the default options of the R-package \texttt{brms}. 
In particular, for the parametric part, we use a multivariate uniform distribution defined between minus infinity and plus infinity without correlation among the betas, such that $ \mathbf{\beta} = [\beta_{0}, \beta_{1}, \sum_{i} \beta_{2i}, \sum_{i} \beta_{3i}]$, is distributed as 
\begin{align}
\mathbf{\beta} \sim U(-\infty, +\infty, \mathbf{0}).
\end{align}
The nonparametric part of equation (\ref{fourier_gam_int}) requires a prior on the distribution of the explanatory variables. This reflects the need to make an assumption about every point in some continuous input space to which we associate a particular statistical process. The most common choice is to assume that all the arguments are normally distributed random variables, so that the functions $ f_{1}(.), f_{21}(.), f_{31}(.) $ follow a Standardized Gaussian Process (GP),
\begin{align}\label{GP}
f_1(\cdot) \sim GP(\cdot | 0, 1) \quad f_{21}(\cdot) \sim GP(\cdot | 0, 1) \quad f_{31}(\cdot) \sim GP(\cdot | 0, 1),
\end{align}
and, consequently, the splines' coefficients are normally distributed
\begin{align}\label{matrix prior1}
\begin{bmatrix}
\beta_{11} \\
\vdots \\
\beta_{1K_{1}}\\
\beta_{211}\\
\vdots \\
\beta_{21K_{21}}\\
\beta_{31}\\
\vdots\\
\beta_{31K_{31}}\\
\end{bmatrix}
\sim N
\begin{bmatrix}
\begin{pmatrix}
0 \\
\vdots \\
0 \\
0 \\
\vdots \\
0 \\
0 \\
\vdots \\ 
0 \\ 
\end{pmatrix}
,
\begin{pmatrix}
1 & 0  & \dots & \dots & \dots & \dots  & \dots & 0      \\
0 & 1  & \dots & \dots & \dots & \dots  & \dots & 0      \\
0 & 0  & \ddots & \dots & \dots & \dots & \dots & 0      \\
\vdots & \dots & \dots & \ddots & \dots & \dots & \dots & \vdots \\
\vdots & \dots & \dots & \dots & \ddots& \dots & \dots & \vdots \\
\vdots & \dots & \dots & \dots & \dots & \ddots & \dots & \vdots \\
0 & 0  & \dots & \dots & \dots & \dots &  1 & 0  \\ 
0 & 0  & \dots & \dots & \dots & \dots &  0 & 1
\end{pmatrix}
\end{bmatrix}.
\end{align}

In the same way the smoothing parameter $\lambda$ follows a Normal distribution, $\lambda \sim N(0, \sigma_{\lambda}^2)$.
The set of priors is completed by the distributions of error's variance term and of the variance of $ \lambda $. Following the suggestion of \citet{StanPrior}, we propose two standardized half t-student with three degrees of freedom, 
\begin{align}\label{variance}
\sigma_{\epsilon} \sim t_{3}(0,1) \quad \quad \sigma_{\lambda} \sim t_{3}(0,1).
\end{align}
In practice, the sampling is run on four Markov chains and repeated for 4000 iterations, a reasonable number given the sampling efficacy granted by a Hamiltonian Monte Carlo Sampler, which uses the No-U-Turn Sampler (NUTS) \citep{neal2011mcmc}. It is interesting to notice that the NUTS-Sampler, used by the software, does not require any special behaviour for conjugate priors, which much impact the priors' choice \citep{hoffman2014no}. For more details see also \citet{stan2015}.

In order to test the model we construct three simulated data generating processes containing a first order Fourier seasonality for $t=1,..,360$; 


\begin{align*}%\label{sim_1}
y_{t}^{sim_{1}} =& 3 + 0.4 \text{trend}_{t} + 30 \cos\bigg(\frac{\pi}{6}x_{t}\bigg) + 60 \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + \epsilon_{t}, \\
y_{t}^{sim_{2}} =& 3 + 0.4 \text{trend}_{t} + 30 \cos\bigg(\frac{\pi}{6}x_{t}\bigg) + 60 \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + 0.8 \text{trend}_{t} \bigg(\cos\bigg(\frac{\pi}{6}x_{t}\bigg) + \sin\bigg(\frac{\pi}{6}x_{t}\bigg) \bigg) + \epsilon_{t}, \\
y_{t}^{sim_{3}} =& 3 + 0.4 \text{trend}_{t} + 30 \cos\bigg(\frac{\pi}{6}x_{t}\bigg) + 60 \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + 0.8 \text{trend}_{t} \exp\bigg(\bigg(\cos\bigg(\frac{\pi}{6}x_{t}\bigg) + \sin\bigg(\frac{\pi}{6}x_{t}\bigg) \bigg) + \epsilon_{t}.
\end{align*}

$ sim_{1} $ presents no trend-seasonal interaction, $ sim_{2} $ a linear interaction and $ sim_{3} $ an interaction where seasonality is not linear with respect to $y_t$. Both in $sim_2$ and $sim_3$, the interaction's coefficient is kept small to contain the eventual advantages of the AMI. In all three regressions $ x_{t} $ are drawn from a standardized normal distribution $ x_{t} \sim N(0,1) $, the trend is the cumulative sum of a linear sum of those draws, $ \text{trend}_{t} = \sum_{t=1}^{360} [\sum_{j=1}^{360} x_{tj} + x_{t}]  $, and $ \epsilon_{t} \sim N(0, 0.4) $. \\
In order to test the properties of the AMI we compare its performances with the ones of a linear model without (LMNI) and with interaction (LMI) and an additive model without interaction (AMNI),
\begin{align}\label{fourier_am_no_int}
y_{t} = \beta_{0} + f_{1}(\text{trend}_{t}) + \sum_{i=1}^{N} f_{21} (\cos_{it} + \sin_{it}) + \epsilon_{t}.
\end{align}
To make the comparison independent form any type of structure we further introduce, as a limit case, a purely nonparametric model (NPM), 
\begin{align}\label{fourier_non_par}
y_{t} = \beta_{0} + f\bigg(\text{trend}_t, \sum_{i=1}^{N} (\cos_{it} + \sin_{it}) \bigg) + \epsilon_{t},
\end{align}
which is the most flexible option. 

Three standard measures have been chosen to compare the \textit{ex post} average forecast with the observed values: 1) the root mean squared forecast error (RMSFE), which measures the differences between the values predicted by a model ($\hat{\theta}$) and the values observed ($\theta$), RMSFE$(\hat{\theta})=\sqrt{E((\hat{\theta} - \theta)^2)})$; 2) the mean absolute percentage error (MAPE), which expresses accuracy as a percentage, MAPE$(\hat{\theta}) = E\left( \left|\frac{\theta - \hat{\theta}}{\theta}\right| \right)$; 3) the coverage of the 95\% and 90\% prediction intervals, which computes the actual coverage percentage of the prediction intervals on hold-out samples, the larger the coverage, the better the model \citep{hyndman2006another, azose2015bayesian}.
Table \ref{RMSFE_sim_1st} shows that for $ sim_1 $, which does not include an interaction term, the LMNI does slightly better than the LMI, but that the AMI delivers more accurate results with respect to the AMNI. For $sim_2$ there are clearly two clusters in terms of RMSFE and MAPE. As expected the LMNI and the AMNI have a poor performance compared to the interaction models. However, the absence of a significant difference between the LMI, the AMI and the NPM, suggests that, once the interaction is introduced, the gain in using a nonparametric or a semiparametric strategy in presence of a linear interaction is trivial. None of the models is particularly accurate in capturing the uncertainty related to observations far away from the mean since the coverage intervals falls between 45\% and 55\% for the 95\% CI and between 36\% and 45\% for the 90\% CI.
$ sim_3 $ reverses this last finding returning preciser credible intervals, which range from 76\% to 92.5\% at the 95\% CI and from 76\% to 86\% for the 90\% CI. The non-linear interaction highlights the benefits of a flexible functional form. Nevertheless, there is no significant difference between the NPM and the AMI. Therefore, unless the curse of dimensionality implies a sub-optimal choice of knots, it might be better to simply put all the explanatory variables into a single unspecified function. \\*
Based on the previous simulation exercise we introduce a second order Fourier seasonality, 
\begin{align*}
y_{t}^{sim_{4}} &= 12 + 0.1 \text{trend}_{t} + 20 \cos\bigg(\frac{\pi}{6}x_{t}\bigg) + 48 \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + 24 \cos\bigg(\frac{\pi}{3}x_{t}\bigg) + 8 \sin\bigg(\frac{\pi}{3}x_{t}\bigg) + \epsilon_{t}, \\
y_{t}^{sim_{5}} &= 12 + 0.1 \text{trend}_{t} + 20 \cos\bigg(\frac{\pi}{6}x_{t}\bigg) + 48 \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + 24 \cos\bigg(\frac{\pi}{3}x_{t}\bigg) + 8 \sin\bigg(\frac{\pi}{3}x_{t}\bigg) + \\
&+ 0.8 \text{trend}_{t} \bigg(\cos\bigg(\frac{\pi}{6}x_{t}\bigg) + \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + \cos\bigg(\frac{\pi}{3}x_{t}\bigg) + \sin\bigg(\frac{\pi}{3}x_{t}\bigg)\bigg) + \epsilon_{t}, \\
y_{t}^{sim_{6}} &= 12 + 0.1 \text{trend}_{t} + 20 \cos\bigg(\frac{\pi}{6}x_{t}\bigg) + 48 \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + 24 \cos\bigg(\frac{\pi}{3}x_{t}\bigg) + 8 \sin\bigg(\frac{\pi}{3}x_{t}\bigg) + \\
&+ 0.8 \text{trend}_{t} \exp\bigg(\bigg(\cos\bigg(\frac{\pi}{6}x_{t}\bigg) + \sin\bigg(\frac{\pi}{6}x_{t}\bigg) + \cos\bigg(\frac{\pi}{3}x_{t}\bigg) + \sin\bigg(\frac{\pi}{3}x_{t}\bigg)\bigg) + \epsilon_{t}.
\end{align*}

As expected in $ sim_4 $, the models without interaction outperform the ones with interaction. The AMNI improves the LMNI and it substantially increases the coverage capacity shifting it from 80\% to 94\% for the 95\% CI and from 74\% to 86\% for the 90\% CI.  
When in $sim_5$ the interaction is introduced the behaviour of the different models is comparable to the one presented in Table \ref{RMSFE_sim_1st}. In other words, the models with interaction outperform the ones without interaction in terms of RMSFE, MAPE and coverage intervals, with the NPM being the most accurate among them. 
To the contrary, $sim_6$ reports more remarkable differences between the models. The AMI does not only outperform all the parametric alternatives and the AMNI, but also the NPM with a final improvement of the RMSFE of the 14\%, see Table \ref{RMSFE_sim_2nd}. This last outcome shows that equation (\ref{fourier_gam_int}) gives a non negligible advantage, compared to the other specifications, if different parts of the seasonality interact with the trend, especially in the coverage capacity of the credible intervals. 
A detailed visualization of the results is provided in section 2 of the Additional Material.

<<simulations 1st_order_1, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=

freq <- 12
years <- 30
N <- freq*years
x <- (1:N)/freq 

set.seed(1)

##########################################################################
# SIMULATION 1.1: 1st order Fourier trend, seasonality no interaction
##########################################################################

# Trend
trend_11 <- cumsum(cumsum(rnorm(N))+rnorm(N))

# Seasonality
sea_11 <- .3 + 3*cos(2*pi*x) + 6*sin(2*pi*x)

# Combine Time Series data
sim_11 <- as.data.frame(abs(0.4*trend_11 + 10*sea_11 + .2*rnorm(N)))
colnames(sim_11) <- c('y_11')
sim_11$t <- as.numeric(rownames(sim_11))
sim_11$trend <- sim_11$t / 360
sim_11$Date <- seq(as.Date("1981/1/1"), by = "month", length.out = 360)

plot_11_ts <- ggplot(data = sim_11, aes(x = Date, y = y_11)) +
  geom_line(stat = 'identity') +
  labs(y = "y", title = '1st Order no Interaction') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) 
plot_11_ts

##########################################################################
# SIMULATION 2.1: 2nd order Fourier trend, seasonality no interaction
##########################################################################
set.seed(1)
# Trend
trend_21 <- cumsum(cumsum(rnorm(N))+rnorm(N))

# Seasonality
sea_21 <- .3 + .5*cos(2*pi*x) + 1.2*sin(2*pi*x) +
  .6*cos(2*2*pi*x) + .2*sin(2*2*pi*x) 

# Combine Time Series data
sim_21 <- as.data.frame(abs(0.1*trend_21 + 40*sea_21 + .2*rnorm(N)))
colnames(sim_21) <- c('y_21')
sim_21$t <- as.numeric(rownames(sim_21))
sim_21$trend <- sim_21$t / 360
sim_21$Date <- seq(as.Date("1981/1/1"), by = "month", length.out = 360)

plot_21_ts <- ggplot(data = sim_21, aes(x = Date, y = y_21)) +
  geom_line(stat = 'identity') +
  labs(y = "y", title = '2nd Order no Interaction') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) 
plot_21_ts


##########################################################################
# SIMULATION 1.2: 1st order Fourier trend, seasonality with interaction
##########################################################################
set.seed(1)

# Trend
trend_12 <- cumsum(cumsum(rnorm(N))+rnorm(N))

# Seasonality
sea_12 <- .3 + 3*cos(2*pi*x) + 6*sin(2*pi*x)

# Combine Time Series data
sim_12 <- as.data.frame(0.4*trend_12 + 24*sea_12 + 0.8*trend_12*sea_12 + .2*rnorm(N))
colnames(sim_12) <- c('y_12')
sim_12$t <- as.numeric(rownames(sim_12))
sim_12$trend <- sim_12$t / 360
sim_12$Date <- seq(as.Date("1981/1/1"), by = "month", length.out = 360)

plot_12_ts <- ggplot(data = sim_12, aes(x = Date, y = y_12)) +
  geom_line(stat = 'identity') +
  labs(y = "y", title = '1st Order with Interaction') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) 
plot_12_ts


##########################################################################
# SIMULATION 2.2: 2nd order Fourier trend, seasonality with interaction
##########################################################################

set.seed(1)
# Trend
trend_22 <- cumsum(cumsum(rnorm(N))+rnorm(N))
# Seasonality
sea_22 <- .3 + .5*cos(2*pi*x) + 1.2*sin(2*pi*x) +
  .6*cos(2*2*pi*x) + .2*sin(2*2*pi*x) 

# Combine Time Series data
sim_22 <- as.data.frame(0.4*trend_22 + 24*sea_22 + 0.8*trend_22*sea_22 + .2*rnorm(N))
colnames(sim_22) <- c('y_22')
sim_22$t <- as.numeric(rownames(sim_22))
sim_22$trend <- sim_22$t / 360
sim_22$Date <- seq(as.Date("1981/1/1"), by = "month", length.out = 360)

plot_22_ts <- ggplot(data = sim_22, aes(x = Date, y = y_22)) +
  geom_line(stat = 'identity') +
  labs(y = "y", title = '2nd Order with Interaction') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) 
plot_22_ts

###################################################################################
# SIMULATION 1.3: 1st order Fourier trend, seasonality with complex interaction
###################################################################################
set.seed(1)

# Trend
trend_13 <- sqrt(cumsum(cumsum(rnorm(N)))+rnorm(N))

# Seasonality
sea_13 <- .3 + 3*cos(2*pi*x) + 6*sin(2*pi*x)

# Combine Time Series data
sim_13 <- as.data.frame(3 + 15*trend_13 + 10*exp(sea_13)*trend_13 + .4*rnorm(N))
colnames(sim_13) <- c('y_13')
sim_13$t <- as.numeric(rownames(sim_13))
sim_13$trend <- sim_13$t / 360
sim_13$Date <- seq(as.Date("1981/1/1"), by = "month", length.out = 360)


plot_13_ts <- ggplot(data = sim_13, aes(x = Date, y = y_13)) +
  geom_line(stat = 'identity') +
  labs(y = "y", title = '1st Order with Complex Interaction') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) 
plot_13_ts



###################################################################################
# SIMULATION 2.3: 2nd order Fourier trend, seasonality with complex interaction
###################################################################################
set.seed(1)
# Trend
trend_23 <- sqrt(cumsum(cumsum(rnorm(N)))+rnorm(N))

# Seasonality
sea_23 <- .3 + .5*cos(2*pi*x) + 1.2*sin(2*pi*x) +
  .6*cos(2*2*pi*x) + .2*sin(2*2*pi*x) 

# Combine  Time Series data
sim_23 <- as.data.frame(3 + 15*trend_23 + 10*exp(sea_23)*trend_23 + .4*rnorm(N))

colnames(sim_23) <- c('y_23')
sim_23$t <- as.numeric(rownames(sim_23))
sim_23$trend <- sim_23$t / 360
sim_23$Date <- seq(as.Date("1981/1/1"), by = "month", length.out = 360)

plot_23_ts <- ggplot(data = sim_23, aes(x = Date, y = y_23)) +
  geom_line(stat = 'identity') +
  labs(y = "y", title = '2nd Order with Complex Interaction') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) 
plot_23_ts


plots_ts <- grid.arrange(plot_11_ts, plot_21_ts, plot_12_ts, plot_22_ts, plot_13_ts, plot_23_ts, ncol=2)
ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Plot_simulations_ts.pdf", plots_ts,  width = 20, height = 15, units = "cm")


@


<<1st order no interaction, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
# Add Fourier data
set.seed(123)

fourier <- as.data.table(fourier(ts(sim_11$y_11, frequency=12), K=1))
colnames(fourier) <- c("S1_12", "C1_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
sim_11 <- cbind(sim_11, fourier)

# Train and Test Data
train_sim_11 <- sim_11[1:280,]
test_sim_11 <- sim_11[281:360,]

# Fitting 
# LINEAR MODEL NO INTERACTION 
lm_no_11 <- brm(y_11 ~ trend + first_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_11, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no_11))
train_sim_11$fit_lm_no_11 <- fit$Estimate
pred_lm_no_11 <- as.data.frame(predict(lm_no_11, newdata = test_sim_11, type = 'response'))
test_sim_11$pr_lm_no_11 <- pred_lm_no_11$Estimate
test_sim_11$pr_lm_no_11_upr <- pred_lm_no_11$Q97.5
test_sim_11$pr_lm_no_11_lwr <- pred_lm_no_11$Q2.5

#90%
pred_lm_no_11_90 <- as.data.frame(predict(lm_no_11, newdata = test_sim_11, type = 'response', probs = c(0.05, 0.95)))
test_sim_11$pr_lm_no_11_90lwr <- pred_lm_no_11_90$Q5
test_sim_11$pr_lm_no_11_90upr <- pred_lm_no_11_90$Q95
test_sim_11$lm_no_11_CI95_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_lm_no_11_lwr | test_sim_11$y_11 > test_sim_11$pr_lm_no_11_upr, 1, 0) 
100 - mean(test_sim_11$lm_no_11_CI95_out)*100 # 63.75 \%
test_sim_11$lm_no_11_CI90_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_lm_no_11_90lwr | test_sim_11$y_11 > test_sim_11$pr_lm_no_11_90upr, 1, 0) 
100 - mean(test_sim_11$lm_no_11_CI90_out)*100 # 60 \% 

rmse(test_sim_11$pr_lm_no_11, test_sim_11$y_11) # 75.77473
mape(test_sim_11$pr_lm_no_11, test_sim_11$y_11) # 0.05737656

plot_lm_no_11 <- ggplot() +
  geom_line(data=sim_11, aes(x=Date, y=y_11)) +
  geom_line(data=train_sim_11, aes(x=Date, y=fit_lm_no_11), col='red') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_lm_no_11), col='red') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_lm_no_11_upr), col='blue') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_lm_no_11_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no_11

# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no_11 <- brm(y_11 ~ s(trend)  + s(first_sum, k=6), iter=4000, family = gaussian(), prior = NULL, data = train_sim_11, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

plot(marginal_effects(gam_no_11), ylab ="Immigration")

fit <- as.data.frame(fitted(gam_no_11))
train_sim_11$fit_gam_no_11 <- fit$Estimate
pred_gam_no_11 <- as.data.frame(predict(gam_no_11, newdata = test_sim_11, type = 'response'))
test_sim_11$pr_gam_no_11 <- pred_gam_no_11$Estimate
test_sim_11$pr_gam_no_11_upr <- pred_gam_no_11$Q97.5
test_sim_11$pr_gam_no_11_lwr <- pred_gam_no_11$Q2.5

#90%
pred_gam_no_11_90 <- as.data.frame(predict(gam_no_11, newdata = test_sim_11, type = 'response', probs = c(0.05, 0.95)))
test_sim_11$pr_gam_no_11_90lwr <- pred_gam_no_11_90$Q5
test_sim_11$pr_gam_no_11_90upr <- pred_gam_no_11_90$Q95
test_sim_11$gam_no_11_CI95_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_gam_no_11_lwr | test_sim_11$y_11 > test_sim_11$pr_gam_no_11_upr, 1, 0) 
100 - mean(test_sim_11$gam_no_11_CI95_out)*100 #  52.5 \%
test_sim_11$gam_no_11_CI90_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_gam_no_11_90lwr | test_sim_11$y_11 > test_sim_11$pr_gam_no_11_90upr, 1, 0) 
100 - mean(test_sim_11$gam_no_11_CI90_out)*100 # 50 \% 

rmse(test_sim_11$pr_gam_no_11, test_sim_11$y_11) # 89.01603
mape(test_sim_11$pr_gam_no_11, test_sim_11$y_11) # 0.06881475

plot_gam_no_11 <- ggplot() +
  geom_line(data=sim_11, aes(x=Date, y=y_11)) +
  geom_line(data=train_sim_11, aes(x=Date, y=fit_gam_no_11), col='red') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_no_11), col='red') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_no_11_upr), col='blue') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_no_11_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no_11

# LINEAR MODEL WITH INTERACTION 
lm_11 <- brm(y_11 ~ trend * first_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_11, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_11))
train_sim_11$fit_lm_11 <- fit$Estimate
pred_lm_11 <- as.data.frame(predict(lm_11, newdata = test_sim_11, type = 'response'))
test_sim_11$pr_lm_11 <- pred_lm_11$Estimate
test_sim_11$pr_lm_11_upr <- pred_lm_11$Q97.5
test_sim_11$pr_lm_11_lwr <- pred_lm_11$Q2.5

#90%
pred_lm_11_90 <- as.data.frame(predict(lm_11, newdata = test_sim_11, type = 'response', probs = c(0.05, 0.95)))
test_sim_11$pr_lm_11_90lwr <- pred_lm_11_90$Q5
test_sim_11$pr_lm_11_90upr <- pred_lm_11_90$Q95
test_sim_11$lm_11_CI95_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_lm_11_lwr | test_sim_11$y_11 > test_sim_11$pr_lm_11_upr, 1, 0) 
100 - mean(test_sim_11$lm_11_CI95_out)*100 # 65 \%
test_sim_11$lm_11_CI90_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_lm_11_90lwr | test_sim_11$y_11 > test_sim_11$pr_lm_11_90upr, 1, 0) 
100 - mean(test_sim_11$lm_11_CI90_out)*100 #  60 \% 

rmse(test_sim_11$pr_lm_11, test_sim_11$y_11) # 77.47626
mape(test_sim_11$pr_lm_11, test_sim_11$y_11) # 0.05897644

plot_lm_11 <- ggplot() +
  geom_line(data=sim_11, aes(x=Date, y=y_11)) +
  geom_line(data=train_sim_11, aes(x=Date, y=fit_lm_11), col='red') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_lm_11_upr), col='blue') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_lm_11_lwr), col='blue') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_lm_11), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm_11

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam_11 <- brm(y_11 ~ s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_11, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

plot(marginal_effects(gam_11), ylab ="Immigration")


fit <- as.data.frame(fitted(gam_11))
train_sim_11$fit_gam_11 <- fit$Estimate
pred_gam_11 <- as.data.frame(predict(gam_11, newdata = test_sim_11, type = 'response'))
test_sim_11$pr_gam_11 <- pred_gam_11$Estimate
test_sim_11$pr_gam_11_upr <- pred_gam_11$Q97.5
test_sim_11$pr_gam_11_lwr <- pred_gam_11$Q2.5

#90%
pred_gam_11_90 <- as.data.frame(predict(gam_11, newdata = test_sim_11, type = 'response', probs = c(0.05, 0.95)))
test_sim_11$pr_gam_11_90lwr <- pred_gam_11_90$Q5
test_sim_11$pr_gam_11_90upr <- pred_gam_11_90$Q95
test_sim_11$gam_11_CI95_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_gam_11_lwr | test_sim_11$y_11 > test_sim_11$pr_gam_11_upr, 1, 0) 
100 - mean(test_sim_11$gam_11_CI95_out)*100 #  66.25 \%
test_sim_11$gam_11_CI90_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_gam_11_90lwr | test_sim_11$y_11 > test_sim_11$pr_gam_11_90upr, 1, 0) 
100 - mean(test_sim_11$gam_11_CI90_out)*100 # 55 \% 

rmse(test_sim_11$pr_gam_11, test_sim_11$y_11) # 60.74109
mape(test_sim_11$pr_gam_11, test_sim_11$y_11) # 0.04805214

plot_gam_11 <- ggplot() +
  geom_line(data=sim_11, aes(x=Date, y=y_11)) +
  geom_line(data=train_sim_11, aes(x=Date, y=fit_gam_11), col='red') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_11_upr), col='blue') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_11_lwr), col='blue') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_11), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Nonparametric Model (NPM)")
plot_gam_11


# ADDITIVE MODEL COMPLEX
gam_11_comp <- brm(y_11 ~ s(trend) + s(first_sum, k=6) + s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_11, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_11_comp))
train_sim_11$fit_gam_11_comp <- fit$Estimate
pred_gam_11_comp <- as.data.frame(predict(gam_11_comp, newdata = test_sim_11, type = 'response'))
test_sim_11$pr_gam_11_comp <- pred_gam_11_comp$Estimate
test_sim_11$pr_gam_11_comp_upr <- pred_gam_11_comp$Q97.5
test_sim_11$pr_gam_11_comp_lwr <- pred_gam_11_comp$Q2.5
#90%
pred_gam_11_comp_90 <- as.data.frame(predict(gam_11_comp, newdata = test_sim_11, type = 'response', probs = c(0.05, 0.95)))
test_sim_11$pr_gam_11_comp_90lwr <- pred_gam_11_comp_90$Q5
test_sim_11$pr_gam_11_comp_90upr <- pred_gam_11_comp_90$Q95
test_sim_11$gam_11_comp_CI95_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_gam_11_comp_lwr | test_sim_11$y_11 > test_sim_11$pr_gam_11_comp_upr, 1, 0) 
100 - mean(test_sim_11$gam_11_comp_CI95_out)*100 # 48.75  \%
test_sim_11$gam_11_comp_CI90_out <- ifelse(test_sim_11$y_11 < test_sim_11$pr_gam_11_comp_90lwr | test_sim_11$y_11 > test_sim_11$pr_gam_11_comp_90upr, 1, 0) 
100 - mean(test_sim_11$gam_11_comp_CI90_out)*100 # 45  \% 

rmse(test_sim_11$pr_gam_11_comp, test_sim_11$y_11) # 93.19774
mape(test_sim_11$pr_gam_11_comp, test_sim_11$y_11) # 0.07279617

plot_gam_11_comp <- ggplot() +
  geom_line(data=sim_11, aes(x=Date, y=y_11)) +
  geom_line(data=train_sim_11, aes(x=Date, y=fit_gam_11_comp), col='red') +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_11_comp_upr), col='blue', size = 1) +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_11_comp_lwr), col='blue', size = 1) +
  geom_line(data=test_sim_11, aes(x=Date, y=pr_gam_11_comp), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_11_comp


plots_11 <- grid.arrange(plot_lm_no_11, plot_gam_no_11, plot_lm_11, plot_gam_11_comp, plot_gam_11, ncol=2)
ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/1st_order_no_inter.pdf", plots_11,  width = 20, height = 17, units = "cm")


@

<<2nd order no interaction, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
set.seed(123)

# Add Fourier data
fourier <- as.data.table(fourier(ts(sim_21$y_21, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12
sim_21 <- cbind(sim_21, fourier)

# Train and Test Data
train_sim_21 <- sim_21[1:280,]
test_sim_21 <- sim_21[281:360,]

# Fitting 
# LINEAR MODEL NO INTERACTION 
lm_no_21 <- brm(y_21 ~ trend + first_sum + second_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_21, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no_21))
train_sim_21$fit_lm_no_21 <- fit$Estimate
pred_lm_no_21 <- as.data.frame(predict(lm_no_21, newdata = test_sim_21, type = 'response'))
test_sim_21$pr_lm_no_21 <- pred_lm_no_21$Estimate
test_sim_21$pr_lm_no_21_upr <- pred_lm_no_21$Q97.5
test_sim_21$pr_lm_no_21_lwr <- pred_lm_no_21$Q2.5

#90%
pred_lm_no_21_90 <- as.data.frame(predict(lm_no_21, newdata = test_sim_21, type = 'response', probs = c(0.05, 0.95)))
test_sim_21$pr_lm_no_21_90lwr <- pred_lm_no_21_90$Q5
test_sim_21$pr_lm_no_21_90upr <- pred_lm_no_21_90$Q95
test_sim_21$lm_no_21_CI95_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_lm_no_21_lwr | test_sim_21$y_21 > test_sim_21$pr_lm_no_21_upr, 1, 0) 
100 - mean(test_sim_21$lm_no_21_CI95_out)*100 # 80 \%
test_sim_21$lm_no_21_CI90_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_lm_no_21_90lwr | test_sim_21$y_21 > test_sim_21$pr_lm_no_21_90upr, 1, 0) 
100 - mean(test_sim_21$lm_no_21_CI90_out)*100 # 73.75 \% 

rmse(test_sim_21$pr_lm_no_21, test_sim_21$y_21) # 30.06624
mape(test_sim_21$pr_lm_no_21, test_sim_21$y_21) # 0.1018506

plot_lm_no_21 <- ggplot() +
  geom_line(data=sim_21, aes(x=Date, y=y_21)) +
  geom_line(data=train_sim_21, aes(x=Date, y=fit_lm_no_21), col='red') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_lm_no_21), col='red') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_lm_no_21_upr), col='blue') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_lm_no_21_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no_21

# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no_21 <- brm(y_21 ~ s(trend)  + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = NULL, data = train_sim_21, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_no_21))
train_sim_21$fit_gam_no_21 <- fit$Estimate
pred_gam_no_21 <- as.data.frame(predict(gam_no_21, newdata = test_sim_21, type = 'response'))
test_sim_21$pr_gam_no_21 <- pred_gam_no_21$Estimate
test_sim_21$pr_gam_no_21_upr <- pred_gam_no_21$Q97.5
test_sim_21$pr_gam_no_21_lwr <- pred_gam_no_21$Q2.5

#90%
pred_gam_no_21_90 <- as.data.frame(predict(gam_no_21, newdata = test_sim_21, type = 'response', probs = c(0.05, 0.95)))
test_sim_21$pr_gam_no_21_90lwr <- pred_gam_no_21_90$Q5
test_sim_21$pr_gam_no_21_90upr <- pred_gam_no_21_90$Q95
test_sim_21$gam_no_21_CI95_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_gam_no_21_lwr | test_sim_21$y_21 > test_sim_21$pr_gam_no_21_upr, 1, 0) 
100 - mean(test_sim_21$gam_no_21_CI95_out)*100 #  93.75 \%
test_sim_21$gam_no_21_CI90_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_gam_no_21_90lwr | test_sim_21$y_21 > test_sim_21$pr_gam_no_21_90upr, 1, 0) 
100 - mean(test_sim_21$gam_no_21_CI90_out)*100 # 86.26  \% 

rmse(test_sim_21$pr_gam_no_21, test_sim_21$y_21) # 28.39931
mape(test_sim_21$pr_gam_no_21, test_sim_21$y_21) # 0.0949503

plot_gam_no_21 <- ggplot() +
  geom_line(data=sim_21, aes(x=Date, y=y_21)) +
  geom_line(data=train_sim_21, aes(x=Date, y=fit_gam_no_21), col='red') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_no_21), col='red') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_no_21_upr), col='blue') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_no_21_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no_21

# LINEAR MODEL WITH INTERACTION 
lm_21 <- brm(y_21 ~ trend * first_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_21, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_21))
train_sim_21$fit_lm_21 <- fit$Estimate
pred_lm_21 <- as.data.frame(predict(lm_21, newdata = test_sim_21, type = 'response'))
test_sim_21$pr_lm_21 <- pred_lm_21$Estimate
test_sim_21$pr_lm_21_upr <- pred_lm_21$Q97.5
test_sim_21$pr_lm_21_lwr <- pred_lm_21$Q2.5

#90%
pred_lm_21_90 <- as.data.frame(predict(lm_21, newdata = test_sim_21, type = 'response', probs = c(0.05, 0.95)))
test_sim_21$pr_lm_21_90lwr <- pred_lm_21_90$Q5
test_sim_21$pr_lm_21_90upr <- pred_lm_21_90$Q95
test_sim_21$lm_21_CI95_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_lm_21_lwr | test_sim_21$y_21 > test_sim_21$pr_lm_21_upr, 1, 0) 
100 - mean(test_sim_21$lm_21_CI95_out)*100 # 80 \%
test_sim_21$lm_21_CI90_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_lm_21_90lwr | test_sim_21$y_21 > test_sim_21$pr_lm_21_90upr, 1, 0) 
100 - mean(test_sim_21$lm_21_CI90_out)*100 # 75  \% 

rmse(test_sim_21$pr_lm_21, test_sim_21$y_21) # 36.17179
mape(test_sim_21$pr_lm_21, test_sim_21$y_21) # 0.1271104

plot_lm_21 <- ggplot() +
  geom_line(data=sim_21, aes(x=Date, y=y_21)) +
  geom_line(data=train_sim_21, aes(x=Date, y=fit_lm_21), col='red') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_lm_21_upr), col='blue', size=1) +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_lm_21_lwr), col='blue', size=1) +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_lm_21), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm_21

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam_21 <- brm(y_21 ~ s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_21, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_21))
train_sim_21$fit_gam_21 <- fit$Estimate
pred_gam_21 <- as.data.frame(predict(gam_21, newdata = test_sim_21, type = 'response'))
test_sim_21$pr_gam_21 <- pred_gam_21$Estimate
test_sim_21$pr_gam_21_upr <- pred_gam_21$Q97.5
test_sim_21$pr_gam_21_lwr <- pred_gam_21$Q2.5

#90%
pred_gam_21_90 <- as.data.frame(predict(gam_21, newdata = test_sim_21, type = 'response', probs = c(0.05, 0.95)))
test_sim_21$pr_gam_21_90lwr <- pred_gam_21_90$Q5
test_sim_21$pr_gam_21_90upr <- pred_gam_21_90$Q95
test_sim_21$gam_21_CI95_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_gam_21_lwr | test_sim_21$y_21 > test_sim_21$pr_gam_21_upr, 1, 0) 
100 - mean(test_sim_21$gam_21_CI95_out)*100 #  87.5 \%
test_sim_21$gam_21_CI90_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_gam_21_90lwr | test_sim_21$y_21 > test_sim_21$pr_gam_21_90upr, 1, 0) 
100 - mean(test_sim_21$gam_21_CI90_out)*100 # 76.25 \% 

rmse(test_sim_21$pr_gam_21, test_sim_21$y_21) # 30.46784
mape(test_sim_21$pr_gam_21, test_sim_21$y_21) # 0.1041388

plot_gam_21 <- ggplot() +
  geom_line(data=sim_21, aes(x=Date, y=y_21)) +
  geom_line(data=train_sim_21, aes(x=Date, y=fit_gam_21), col='red') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_21_upr), col='blue', size = 1) +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_21_lwr), col='blue', size = 1) +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_21), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Nonparametric Model (NPM)")
plot_gam_21


# ADDITIVE MODEL COMPLEX
gam_21_comp <- brm(y_21 ~ s(trend) + s(first_sum, k=6) + s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_21, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_21_comp))
train_sim_21$fit_gam_21_comp <- fit$Estimate
pred_gam_21_comp <- as.data.frame(predict(gam_21_comp, newdata = test_sim_21, type = 'response'))
test_sim_21$pr_gam_21_comp <- pred_gam_21_comp$Estimate
test_sim_21$pr_gam_21_comp_upr <- pred_gam_21_comp$Q97.5
test_sim_21$pr_gam_21_comp_lwr <- pred_gam_21_comp$Q2.5
#90%
pred_gam_21_comp_90 <- as.data.frame(predict(gam_21_comp, newdata = test_sim_21, type = 'response', probs = c(0.05, 0.95)))
test_sim_21$pr_gam_21_comp_90lwr <- pred_gam_21_comp_90$Q5
test_sim_21$pr_gam_21_comp_90upr <- pred_gam_21_comp_90$Q95
test_sim_21$gam_21_comp_CI95_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_gam_21_comp_lwr | test_sim_21$y_21 > test_sim_21$pr_gam_21_comp_upr, 1, 0) 
100 - mean(test_sim_21$gam_21_comp_CI95_out)*100 # 85  \%
test_sim_21$gam_21_comp_CI90_out <- ifelse(test_sim_21$y_21 < test_sim_21$pr_gam_21_comp_90lwr | test_sim_21$y_21 > test_sim_21$pr_gam_21_comp_90upr, 1, 0) 
100 - mean(test_sim_21$gam_21_comp_CI90_out)*100 # 75\% 

rmse(test_sim_21$pr_gam_21_comp, test_sim_21$y_21) # 36.3592
mape(test_sim_21$pr_gam_21_comp, test_sim_21$y_21) # 0.1274264

plot_gam_21_comp <- ggplot() +
  geom_line(data=sim_21, aes(x=Date, y=y_21)) +
  geom_line(data=train_sim_21, aes(x=Date, y=fit_gam_21_comp), col='red') +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_21_comp_upr), col='blue', size = 1) +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_21_comp_lwr), col='blue', size = 1) +
  geom_line(data=test_sim_21, aes(x=Date, y=pr_gam_21_comp), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_21_comp

plots_21 <- grid.arrange(plot_lm_no_21, plot_gam_no_21, plot_lm_21, plot_gam_21_comp, plot_gam_21, ncol=2)

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/2nd_order_no_inter.pdf", plots_21,  width = 20, height = 17, units = "cm")


@


<<1st order with interaction, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
set.seed(123)

# Add Fourier data
fourier <- as.data.table(fourier(ts(sim_12$y_12, frequency=12), K=1))
colnames(fourier) <- c("S1_12", "C1_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
sim_12 <- cbind(sim_12, fourier)

# Train and Test Data
train_sim_12 <- sim_12[1:280,]
test_sim_12 <- sim_12[281:360,]

########################################################
# FITTING
########################################################

# LINEAR MODEL NO INTERACTION 
lm_no_12 <- brm(y_12 ~ trend + first_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_12, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no_12))
train_sim_12$fit_lm_no_12 <- fit$Estimate
pred_lm_no_12 <- as.data.frame(predict(lm_no_12, newdata = test_sim_12, type = 'response'))
test_sim_12$pr_lm_no_12 <- pred_lm_no_12$Estimate
test_sim_12$pr_lm_no_12_upr <- pred_lm_no_12$Q97.5
test_sim_12$pr_lm_no_12_lwr <- pred_lm_no_12$Q2.5
#90%
pred_lm_no_12_90 <- as.data.frame(predict(lm_no_12, newdata = test_sim_12, type = 'response', probs = c(0.05, 0.95)))
test_sim_12$pr_lm_no_12_90lwr <- pred_lm_no_12_90$Q5
test_sim_12$pr_lm_no_12_90upr <- pred_lm_no_12_90$Q95
test_sim_12$lm_no_12_CI95_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_lm_no_12_lwr | test_sim_12$y_12 > test_sim_12$pr_lm_no_12_upr, 1, 0) 
100 - mean(test_sim_12$lm_no_12_CI95_out)*100 # 45 \%
test_sim_12$lm_no_12_CI90_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_lm_no_12_90lwr | test_sim_12$y_12 > test_sim_12$pr_lm_no_12_90upr, 1, 0) 
100 - mean(test_sim_12$lm_no_12_CI90_out)*100 # 36.26 \% 

rmse(test_sim_12$pr_lm_no_12, test_sim_12$y_12) # 6441.346
mape(test_sim_12$pr_lm_no_12, test_sim_12$y_12) # 3.119799

plot_lm_no_12 <- ggplot() +
  geom_line(data=sim_12, aes(x=Date, y=y_12)) +
  geom_line(data=train_sim_12, aes(x=Date, y=fit_lm_no_12), col='red') +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_lm_no_12), col='red') +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_lm_no_12_upr), col='blue') +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_lm_no_12_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no_12

# SEMIPARAMETRIC MODEL NO INTERACTION 
# k=3, k=4, k=6 is working with no warnings. k=9 not working
gam_no_12 <- brm(y_12 ~ s(trend) + s(first_sum, k=6), iter=4000, family = gaussian(), prior = NULL, data = train_sim_12, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_no_12))
train_sim_12$fit_gam_no_12 <- fit$Estimate
pred_gam_no_12 <- as.data.frame(predict(gam_no_12, newdata = test_sim_12, type = 'response'))
test_sim_12$pr_gam_no_12 <- pred_gam_no_12$Estimate
test_sim_12$pr_gam_no_12_upr <- pred_gam_no_12$Q97.5
test_sim_12$pr_gam_no_12_lwr <- pred_gam_no_12$Q2.5

#90%
pred_gam_no_12_90 <- as.data.frame(predict(gam_no_12, newdata = test_sim_12, type = 'response', probs = c(0.05, 0.95)))
test_sim_12$pr_gam_no_12_90lwr <- pred_gam_no_12_90$Q5
test_sim_12$pr_gam_no_12_90upr <- pred_gam_no_12_90$Q95
test_sim_12$gam_no_12_CI95_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_gam_no_12_lwr | test_sim_12$y_12 > test_sim_12$pr_gam_no_12_upr, 1, 0) 
100 - mean(test_sim_12$gam_no_12_CI95_out)*100 # 48.75  \%
test_sim_12$gam_no_12_CI90_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_gam_no_12_90lwr | test_sim_12$y_12 > test_sim_12$pr_gam_no_12_90upr, 1, 0) 
100 - mean(test_sim_12$gam_no_12_CI90_out)*100 # 40  \% 

rmse(test_sim_12$pr_gam_no_12, test_sim_12$y_12) # 6461.39
mape(test_sim_12$pr_gam_no_12, test_sim_12$y_12) # 2.818572

plot_gam_no_12 <- ggplot() +
  geom_line(data=sim_12, aes(x=Date, y=y_12)) +
  geom_line(data=train_sim_12, aes(x=Date, y=fit_gam_no_12), col='red') +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_no_12_upr), col='blue', size=1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_no_12_lwr), col='blue', size=1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_no_12), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no_12


# LINEAR MODEL WITH INTERACTION 
lm_12 <- brm(y_12 ~ trend * first_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_12, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_12))
train_sim_12$fit_lm_12 <- fit$Estimate
pred_lm_12 <- as.data.frame(predict(lm_12, newdata = test_sim_12, type = 'response'))
test_sim_12$pr_lm_12 <- pred_lm_12$Estimate
test_sim_12$pr_lm_12_upr <- pred_lm_12$Q97.5
test_sim_12$pr_lm_12_lwr <- pred_lm_12$Q2.5

#90%
pred_lm_12_90 <- as.data.frame(predict(lm_12, newdata = test_sim_12, type = 'response', probs = c(0.05, 0.95)))
test_sim_12$pr_lm_12_90lwr <- pred_lm_12_90$Q5
test_sim_12$pr_lm_12_90upr <- pred_lm_12_90$Q95
test_sim_12$lm_12_CI95_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_lm_12_lwr | test_sim_12$y_12 > test_sim_12$pr_lm_12_upr, 1, 0) 
100 - mean(test_sim_12$lm_12_CI95_out)*100 # 55 \%
test_sim_12$lm_12_CI90_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_lm_12_90lwr | test_sim_12$y_12 > test_sim_12$pr_lm_12_90upr, 1, 0) 
100 - mean(test_sim_12$lm_12_CI90_out)*100 # 36.25  \% 

rmse(test_sim_12$pr_lm_12, test_sim_12$y_12) # 3139.968
mape(test_sim_12$pr_lm_12, test_sim_12$y_12) # 0.722347

plot_lm_12 <- ggplot() +
  geom_line(data=sim_12, aes(x=Date, y=y_12)) +
  geom_line(data=train_sim_12, aes(x=Date, y=fit_lm_12), col='red') +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_lm_12_upr), col='blue', size=1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_lm_12_lwr), col='blue', size=1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_lm_12), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm_12

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam_12 <- brm(y_12 ~ s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_12, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_12))
train_sim_12$fit_gam_12 <- fit$Estimate
pred_gam_12 <- as.data.frame(predict(gam_12, newdata = test_sim_12, type = 'response'))
test_sim_12$pr_gam_12 <- pred_gam_12$Estimate
test_sim_12$pr_gam_12_upr <- pred_gam_12$Q97.5
test_sim_12$pr_gam_12_lwr <- pred_gam_12$Q2.5

#90%
pred_gam_12_90 <- as.data.frame(predict(gam_12, newdata = test_sim_12, type = 'response', probs = c(0.05, 0.95)))
test_sim_12$pr_gam_12_90lwr <- pred_gam_12_90$Q5
test_sim_12$pr_gam_12_90upr <- pred_gam_12_90$Q95
test_sim_12$gam_12_CI95_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_gam_12_lwr | test_sim_12$y_12 > test_sim_12$pr_gam_12_upr, 1, 0) 
100 - mean(test_sim_12$gam_12_CI95_out)*100 #  50 \%
test_sim_12$gam_12_CI90_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_gam_12_90lwr | test_sim_12$y_12 > test_sim_12$pr_gam_12_90upr, 1, 0) 
100 - mean(test_sim_12$gam_12_CI90_out)*100 # 45 \% 

rmse(test_sim_12$pr_gam_12, test_sim_12$y_12) # 3354.045
mape(test_sim_12$pr_gam_12, test_sim_12$y_12) # 0.7521507

plot_gam_12 <- ggplot() +
  geom_line(data=sim_12, aes(x=Date, y=y_12)) +
  geom_line(data=train_sim_12, aes(x=Date, y=fit_gam_12), col='red') +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_12_upr), col='blue', size = 1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_12_lwr), col='blue', size = 1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_12), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Nonparametric Model (NPM)")
plot_gam_12


# ADDITIVE MODEL COMPLEX
gam_12_comp <- brm(y_12 ~ s(trend) + s(first_sum, k=6) + s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_12, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_12_comp))
train_sim_12$fit_gam_12_comp <- fit$Estimate
pred_gam_12_comp <- as.data.frame(predict(gam_12_comp, newdata = test_sim_12, type = 'response'))
test_sim_12$pr_gam_12_comp <- pred_gam_12_comp$Estimate
test_sim_12$pr_gam_12_comp_upr <- pred_gam_12_comp$Q97.5
test_sim_12$pr_gam_12_comp_lwr <- pred_gam_12_comp$Q2.5
#90%
pred_gam_12_comp_90 <- as.data.frame(predict(gam_12_comp, newdata = test_sim_12, type = 'response', probs = c(0.05, 0.95)))
test_sim_12$pr_gam_12_comp_90lwr <- pred_gam_12_comp_90$Q5
test_sim_12$pr_gam_12_comp_90upr <- pred_gam_12_comp_90$Q95
test_sim_12$gam_12_comp_CI95_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_gam_12_comp_lwr | test_sim_12$y_12 > test_sim_12$pr_gam_12_comp_upr, 1, 0) 
100 - mean(test_sim_12$gam_12_comp_CI95_out)*100 # 50  \%
test_sim_12$gam_12_comp_CI90_out <- ifelse(test_sim_12$y_12 < test_sim_12$pr_gam_12_comp_90lwr | test_sim_12$y_12 > test_sim_12$pr_gam_12_comp_90upr, 1, 0) 
100 - mean(test_sim_12$gam_12_comp_CI90_out)*100 # 43.75\% 

rmse(test_sim_12$pr_gam_12_comp, test_sim_12$y_12) # 3363.527
mape(test_sim_12$pr_gam_12_comp, test_sim_12$y_12) # 0.7753826

plot_gam_12_comp <- ggplot() +
  geom_line(data=sim_12, aes(x=Date, y=y_12)) +
  geom_line(data=train_sim_12, aes(x=Date, y=fit_gam_12_comp), col='red') +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_12_comp_upr), col='blue', size = 1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_12_comp_lwr), col='blue', size = 1) +
  geom_line(data=test_sim_12, aes(x=Date, y=pr_gam_12_comp), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_12_comp


plots_12 <- grid.arrange(plot_lm_no_12, plot_gam_no_12, plot_lm_12, plot_gam_12_comp, plot_gam_12, ncol=2)
ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/1st_order_inter.pdf", plots_12,  width = 20, height = 17, units = "cm")



@


<<2nd order with interaction, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
set.seed(1)

# Add Fourier data
fourier <- as.data.table(fourier(ts(sim_22$y_22, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12
sim_22 <- cbind(sim_22, fourier)

# Train and Test Data
train_sim_22 <- sim_22[1:280,]
test_sim_22 <- sim_22[281:360,]

########################################################
# FITTING
########################################################

# LINEAR MODEL NO INTERACTION 
lm_no_22 <- brm(y_22 ~ trend + first_sum + second_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_22, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no_22))
train_sim_22$fit_lm_no_22 <- fit$Estimate
pred_lm_no_22 <- as.data.frame(predict(lm_no_22, newdata = test_sim_22, type = 'response'))
test_sim_22$pr_lm_no_22 <- pred_lm_no_22$Estimate
test_sim_22$pr_lm_no_22_upr <- pred_lm_no_22$Q97.5
test_sim_22$pr_lm_no_22_lwr <- pred_lm_no_22$Q2.5
#90%
pred_lm_no_22_90 <- as.data.frame(predict(lm_no_22, newdata = test_sim_22, type = 'response', probs = c(0.05, 0.95)))
test_sim_22$pr_lm_no_22_90lwr <- pred_lm_no_22_90$Q5
test_sim_22$pr_lm_no_22_90upr <- pred_lm_no_22_90$Q95
test_sim_22$lm_no_22_CI95_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_lm_no_22_lwr | test_sim_22$y_22 > test_sim_22$pr_lm_no_22_upr, 1, 0) 
100 - mean(test_sim_22$lm_no_22_CI95_out)*100 # 58.75 \%
test_sim_22$lm_no_22_CI90_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_lm_no_22_90lwr | test_sim_22$y_22 > test_sim_22$pr_lm_no_22_90upr, 1, 0) 
100 - mean(test_sim_22$lm_no_22_CI90_out)*100 # 48.75 \% 

rmse(test_sim_22$pr_lm_no_22, test_sim_22$y_22) # 1461.826
mape(test_sim_22$pr_lm_no_22, test_sim_22$y_22) # 1.171854


plot_lm_no_22 <- ggplot() +
  geom_line(data=sim_22, aes(x=Date, y=y_22)) +
  geom_line(data=train_sim_22, aes(x=Date, y=fit_lm_no_22), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_lm_no_22), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_lm_no_22_upr), col='blue') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_lm_no_22_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no_22

# SEMIPARAMETRIC MODEL NO INTERACTION 

gam_no_22 <- brm(y_22 ~ s(trend)  + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = NULL, data = train_sim_22, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_no_22))
train_sim_22$fit_gam_no_22 <- fit$Estimate
pred_gam_no_22 <- as.data.frame(predict(gam_no_22, newdata = test_sim_22, type = 'response'))
test_sim_22$pr_gam_no_22 <- pred_gam_no_22$Estimate
test_sim_22$pr_gam_no_22_upr <- pred_gam_no_22$Q97.5
test_sim_22$pr_gam_no_22_lwr <- pred_gam_no_22$Q2.5
#90%
pred_gam_no_22_90 <- as.data.frame(predict(gam_no_22, newdata = test_sim_22, type = 'response', probs = c(0.05, 0.95)))
test_sim_22$pr_gam_no_22_90lwr <- pred_gam_no_22_90$Q5
test_sim_22$pr_gam_no_22_90upr <- pred_gam_no_22_90$Q95
test_sim_22$gam_no_22_CI95_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_gam_no_22_lwr | test_sim_22$y_22 > test_sim_22$pr_gam_no_22_upr, 1, 0) 
100 - mean(test_sim_22$gam_no_22_CI95_out)*100 # 57.5  \%
test_sim_22$gam_no_22_CI90_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_gam_no_22_90lwr | test_sim_22$y_22 > test_sim_22$pr_gam_no_22_90upr, 1, 0) 
100 - mean(test_sim_22$gam_no_22_CI90_out)*100 #  51.25 \% 

rmse(test_sim_22$pr_gam_no_22, test_sim_22$y_22) #1426.601
mape(test_sim_22$pr_gam_no_22, test_sim_22$y_22) # 1.28253

plot_gam_no_22 <- ggplot() +
  geom_line(data=sim_22, aes(x=Date, y=y_22)) +
  geom_line(data=train_sim_22, aes(x=Date, y=fit_gam_no_22), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_no_22), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_no_22_upr), col='blue') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_no_22_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no_22


# LINEAR MODEL WITH INTERACTION 
lm_22 <- brm(y_22 ~ trend * first_sum + trend * second_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_22, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_22))
train_sim_22$fit_lm_22 <- fit$Estimate
pred_lm_22 <- as.data.frame(predict(lm_22, newdata = test_sim_22, type = 'response'))
test_sim_22$pr_lm_22 <- pred_lm_22$Estimate
test_sim_22$pr_lm_22_upr <- pred_lm_22$Q97.5
test_sim_22$pr_lm_22_lwr <- pred_lm_22$Q2.5
#90%
pred_lm_22_90 <- as.data.frame(predict(lm_22, newdata = test_sim_22, type = 'response', probs = c(0.05, 0.95)))
test_sim_22$pr_lm_22_90lwr <- pred_lm_22_90$Q5
test_sim_22$pr_lm_22_90upr <- pred_lm_22_90$Q95
test_sim_22$lm_22_CI95_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_lm_22_lwr | test_sim_22$y_22 > test_sim_22$pr_lm_22_upr, 1, 0) 
100 - mean(test_sim_22$lm_22_CI95_out)*100 # 55 \%
test_sim_22$lm_22_CI90_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_lm_22_90lwr | test_sim_22$y_22 > test_sim_22$pr_lm_22_90upr, 1, 0) 
100 - mean(test_sim_22$lm_22_CI90_out)*100 #  51.25 \% 

rmse(test_sim_22$pr_lm_22, test_sim_22$y_22) # 846.9175
mape(test_sim_22$pr_lm_22, test_sim_22$y_22) # 1.464989

plot_lm_22 <- ggplot() +
  geom_line(data=sim_22, aes(x=Date, y=y_22)) +
  geom_line(data=train_sim_22, aes(x=Date, y=fit_lm_22), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_lm_22_upr), col='blue', size=1) +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_lm_22_lwr), col='blue', size=1) +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_lm_22), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm_22


# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam_22 <- brm(y_22 ~ s(trend, first_sum, second_sum, k=30), iter=4000, family = gaussian(), prior = NULL, data = train_sim_22, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_22))
train_sim_22$fit_gam_22 <- fit$Estimate
pred_gam_22 <- as.data.frame(predict(gam_22, newdata = test_sim_22, type = 'response'))
test_sim_22$pr_gam_22 <- pred_gam_22$Estimate
test_sim_22$pr_gam_22_upr <- pred_gam_22$Q97.5
test_sim_22$pr_gam_22_lwr <- pred_gam_22$Q2.5
#90%
pred_gam_22_90 <- as.data.frame(predict(gam_22, newdata = test_sim_22, type = 'response', probs = c(0.05, 0.95)))
test_sim_22$pr_gam_22_90lwr <- pred_gam_22_90$Q5
test_sim_22$pr_gam_22_90upr <- pred_gam_22_90$Q95
test_sim_22$gam_22_CI95_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_gam_22_lwr | test_sim_22$y_22 > test_sim_22$pr_gam_22_upr, 1, 0) 
100 - mean(test_sim_22$gam_22_CI95_out)*100 # 60  \%
test_sim_22$gam_22_CI90_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_gam_22_90lwr | test_sim_22$y_22 > test_sim_22$pr_gam_22_90upr, 1, 0)
100 - mean(test_sim_22$gam_22_CI90_out)*100 # 48.75 \% 

rmse(test_sim_22$pr_gam_22, test_sim_22$y_22) # 250.8187
mape(test_sim_22$pr_gam_22, test_sim_22$y_22) # 0.1166619

plot_gam_22 <- ggplot() +
  geom_line(data=sim_22, aes(x=Date, y=y_22)) +
  geom_line(data=train_sim_22, aes(x=Date, y=fit_gam_22), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_22_upr), col='blue', size=1) +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_22_lwr), col='blue', size=1) +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_22), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Nonparametric Model (NPM)")
plot_gam_22

# ADDITIVE MODEL COMPLEX 
gam_22_comp <- brm(y_22 ~ s(trend) + s(first_sum, k=6) + s(second_sum, k=6) + s(trend, first_sum) + s(trend, second_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_22, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_22_comp))
train_sim_22$fit_gam_22_comp <- fit$Estimate
pred_gam_22_comp <- as.data.frame(predict(gam_22_comp, newdata = test_sim_22, type = 'response'))
test_sim_22$pr_gam_22_comp <- pred_gam_22_comp$Estimate
test_sim_22$pr_gam_22_comp_upr <- pred_gam_22_comp$Q97.5
test_sim_22$pr_gam_22_comp_lwr <- pred_gam_22_comp$Q2.5
#90%
pred_gam_22_comp_90 <- as.data.frame(predict(gam_22_comp, newdata = test_sim_22, type = 'response', probs = c(0.05, 0.95)))
test_sim_22$pr_gam_22_comp_90lwr <- pred_gam_22_comp_90$Q5
test_sim_22$pr_gam_22_comp_90upr <- pred_gam_22_comp_90$Q95
test_sim_22$gam_22_comp_CI95_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_gam_22_comp_lwr | test_sim_22$y_22 > test_sim_22$pr_gam_22_comp_upr, 1, 0) 
100 - mean(test_sim_22$gam_22_comp_CI95_out)*100 #   60 \%
test_sim_22$gam_22_comp_CI90_out <- ifelse(test_sim_22$y_22 < test_sim_22$pr_gam_22_comp_90lwr | test_sim_22$y_22 > test_sim_22$pr_gam_22_comp_90upr, 1, 0)
100 - mean(test_sim_22$gam_22_comp_CI90_out)*100 # 40 \% 

rmse(test_sim_22$pr_gam_22_comp, test_sim_22$y_22) #  815.5382
mape(test_sim_22$pr_gam_22_comp, test_sim_22$y_22) # 1.390653


plot_gam_22_comp <- ggplot() +
  geom_line(data=sim_22, aes(x=Date, y=y_22)) +
  geom_line(data=train_sim_22, aes(x=Date, y=fit_gam_22_comp), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_22_comp), col='red') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_22_comp_upr), col='blue') +
  geom_line(data=test_sim_22, aes(x=Date, y=pr_gam_22_comp_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model With Interaction (AMI)")
plot_gam_22_comp

plots_22 <- grid.arrange(plot_lm_no_22, plot_gam_no_22, plot_lm_22, plot_gam_22_comp, plot_gam_22, ncol=2)

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/2nd_order_inter.pdf", plots_22,  width = 20, height = 17, units = "cm")



@

<<1st order with complex interaction, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
# Add Fourier data
fourier <- as.data.table(fourier(ts(sim_13$y_13, frequency=12), K=1))
colnames(fourier) <- c("S1_12", "C1_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
sim_13 <- cbind(sim_13, fourier)

# Train and Test Data
train_sim_13 <- sim_13[8:280,]
test_sim_13 <- sim_13[281:360,]

set.seed(123456)

########################################################
# FITTING
########################################################

# LINEAR MODEL NO INTERACTION 
lm_no_13 <- brm(y_13 ~ trend + first_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_13, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no_13))
train_sim_13$fit_lm_no_13 <- fit$Estimate
pred_lm_no_13 <- as.data.frame(predict(lm_no_13, newdata = test_sim_13, type = 'response'))
test_sim_13$pr_lm_no_13 <- pred_lm_no_13$Estimate
test_sim_13$pr_lm_no_13_upr <- pred_lm_no_13$Q97.5
test_sim_13$pr_lm_no_13_lwr <- pred_lm_no_13$Q2.5
#90%
pred_lm_no_13_90 <- as.data.frame(predict(lm_no_13, newdata = test_sim_13, type = 'response', probs = c(0.05, 0.95)))
test_sim_13$pr_lm_no_13_90lwr <- pred_lm_no_13_90$Q5
test_sim_13$pr_lm_no_13_90upr <- pred_lm_no_13_90$Q95
test_sim_13$lm_no_13_CI95_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_lm_no_13_lwr | test_sim_13$y_13 > test_sim_13$pr_lm_no_13_upr, 1, 0) 
100 - mean(test_sim_13$lm_no_13_CI95_out)*100 # 92.5 \%
test_sim_13$lm_no_13_CI90_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_lm_no_13_90lwr | test_sim_13$y_13 > test_sim_13$pr_lm_no_13_90upr, 1, 0) 
100 - mean(test_sim_13$lm_no_13_CI90_out)*100 # 86.25 \% 

rmse(test_sim_13$pr_lm_no_13, test_sim_13$y_13) # 127144.8
mape(test_sim_13$pr_lm_no_13, test_sim_13$y_13) # 0.9360433

plot_lm_no_13 <- ggplot() +
  geom_line(data=sim_13, aes(x=Date, y=y_13)) +
  geom_line(data=train_sim_13, aes(x=Date, y=fit_lm_no_13), col='red') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_lm_no_13), col='red') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_lm_no_13_upr), col='blue') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_lm_no_13_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no_13

# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no_13 <- brm(y_13 ~ s(trend) + s(first_sum, k=6), iter=4000, family = gaussian(), prior = NULL, data = train_sim_13, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_no_13))
train_sim_13$fit_gam_no_13 <- fit$Estimate
pred_gam_no_13 <- as.data.frame(predict(gam_no_13, newdata = test_sim_13, type = 'response'))
test_sim_13$pr_gam_no_13 <- pred_gam_no_13$Estimate
test_sim_13$pr_gam_no_13_upr <- pred_gam_no_13$Q97.5
test_sim_13$pr_gam_no_13_lwr <- pred_gam_no_13$Q2.5
#90%
pred_gam_no_13_90 <- as.data.frame(predict(gam_no_13, newdata = test_sim_13, type = 'response', probs = c(0.05, 0.95)))
test_sim_13$pr_gam_no_13_90lwr <- pred_gam_no_13_90$Q5
test_sim_13$pr_gam_no_13_90upr <- pred_gam_no_13_90$Q95
test_sim_13$gam_no_13_CI95_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_gam_no_13_lwr | test_sim_13$y_13 > test_sim_13$pr_gam_no_13_upr, 1, 0) 
100 - mean(test_sim_13$gam_no_13_CI95_out)*100 #  90 \%
test_sim_13$gam_no_13_CI90_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_gam_no_13_90lwr | test_sim_13$y_13 > test_sim_13$pr_gam_no_13_90upr, 1, 0) 
100 - mean(test_sim_13$gam_no_13_CI90_out)*100 #  78.75 \% 

rmse(test_sim_13$pr_gam_no_13, test_sim_13$y_13) # 105791.3
mape(test_sim_13$pr_gam_no_13, test_sim_13$y_13) # 0.8970141

plot_gam_no_13 <- ggplot() +
  geom_line(data=sim_13, aes(x=Date, y=y_13)) +
  geom_line(data=train_sim_13, aes(x=Date, y=fit_gam_no_13), col='red') +
    geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_no_13), col='red') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_no_13_upr), col='blue') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_no_13_lwr), col='blue')+
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no_13

# LINEAR MODEL WITH INTERACTION 
lm_13 <- brm(y_13 ~ trend * first_sum, prior = NULL, data = train_sim_13, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_13))
train_sim_13$fit_lm_13 <- fit$Estimate
pred_lm_13 <- as.data.frame(predict(lm_13, newdata = test_sim_13, type = 'response'))
test_sim_13$pr_lm_13 <- pred_lm_13$Estimate
test_sim_13$pr_lm_13_upr <- pred_lm_13$Q97.5
test_sim_13$pr_lm_13_lwr <- pred_lm_13$Q2.5
#90%
pred_lm_13_90 <- as.data.frame(predict(lm_13, newdata = test_sim_13, type = 'response', probs = c(0.05, 0.95)))
test_sim_13$pr_lm_13_90lwr <- pred_lm_13_90$Q5
test_sim_13$pr_lm_13_90upr <- pred_lm_13_90$Q95
test_sim_13$lm_13_CI95_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_lm_13_lwr | test_sim_13$y_13 > test_sim_13$pr_lm_13_upr, 1, 0) 
100 - mean(test_sim_13$lm_13_CI95_out)*100 # 82.5 \%
test_sim_13$lm_13_CI90_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_lm_13_90lwr | test_sim_13$y_13 > test_sim_13$pr_lm_13_90upr, 1, 0) 
100 - mean(test_sim_13$lm_13_CI90_out)*100 # 76.25  \% 

rmse(test_sim_13$pr_lm_13, test_sim_13$y_13) # 121757.6
mape(test_sim_13$pr_lm_13, test_sim_13$y_13) # 0.8779458

plot_lm_13 <- ggplot() +
  geom_line(data=sim_13, aes(x=Date, y=y_13)) +
  geom_line(data=train_sim_13, aes(x=Date, y=fit_lm_13), col='red') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_lm_13), col='red') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_lm_13_upr), col='blue') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_lm_13_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm_13

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam_13 <- brm(y_13 ~ s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_13, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_13))
train_sim_13$fit_gam_13 <- fit$Estimate
pred_gam_13 <- as.data.frame(predict(gam_13, newdata = test_sim_13, type = 'response'))
test_sim_13$pr_gam_13 <- pred_gam_13$Estimate
test_sim_13$pr_gam_13_upr <- pred_gam_13$Q97.5
test_sim_13$pr_gam_13_lwr <- pred_gam_13$Q2.5
#90%
pred_gam_13_90 <- as.data.frame(predict(gam_13, newdata = test_sim_13, type = 'response', probs = c(0.05, 0.95)))
test_sim_13$pr_gam_13_90lwr <- pred_gam_13_90$Q5
test_sim_13$pr_gam_13_90upr <- pred_gam_13_90$Q95
test_sim_13$gam_13_CI95_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_gam_13_lwr | test_sim_13$y_13 > test_sim_13$pr_gam_13_upr, 1, 0) 
100 - mean(test_sim_13$gam_13_CI95_out)*100 #  76.25 \%
test_sim_13$gam_13_CI90_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_gam_13_90lwr | test_sim_13$y_13 > test_sim_13$pr_gam_13_90upr, 1, 0)
100 - mean(test_sim_13$gam_13_CI90_out)*100 #  76.25 \% 

rmse(test_sim_13$pr_gam_13, test_sim_13$y_13) #  92478.75
mape(test_sim_13$pr_gam_13, test_sim_13$y_13) # 0.7643018

plot_gam_13 <- ggplot() +
  geom_line(data=sim_13, aes(x=Date, y=y_13)) +
  geom_line(data=train_sim_13, aes(x=Date, y=fit_gam_13), col='red') +
    geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_13), col='red') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_13_upr), col='blue') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_13_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Nonparametric Model (NPM)")
plot_gam_13

# ADDITIVE MODEL COMPLEX
gam_13_comp <- brm(y_13 ~ s(trend) + s(first_sum, k=6) + s(trend, first_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_13, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_13_comp))
train_sim_13$fit_gam_13_comp <- fit$Estimate
pred_gam_13_comp <- as.data.frame(predict(gam_13_comp, newdata = test_sim_13, type = 'response'))
test_sim_13$pr_gam_13_comp <- pred_gam_13_comp$Estimate
test_sim_13$pr_gam_13_comp_upr <- pred_gam_13_comp$Q97.5
test_sim_13$pr_gam_13_comp_lwr <- pred_gam_13_comp$Q2.5
#90%
pred_gam_13_comp_90 <- as.data.frame(predict(gam_13_comp, newdata = test_sim_13, type = 'response', probs = c(0.05, 0.95)))
test_sim_13$pr_gam_13_comp_90lwr <- pred_gam_13_comp_90$Q5
test_sim_13$pr_gam_13_comp_90upr <- pred_gam_13_comp_90$Q95
test_sim_13$gam_13_comp_CI95_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_gam_13_comp_lwr | test_sim_13$y_13 > test_sim_13$pr_gam_13_comp_upr, 1, 0) 
100 - mean(test_sim_13$gam_13_comp_CI95_out)*100 # 76.25  \%
test_sim_13$gam_13_comp_CI90_out <- ifelse(test_sim_13$y_13 < test_sim_13$pr_gam_13_comp_90lwr | test_sim_13$y_13 > test_sim_13$pr_gam_13_comp_90upr, 1, 0) 
100 - mean(test_sim_13$gam_13_comp_CI90_out)*100 # 76.25 \% 

rmse(test_sim_13$pr_gam_13_comp, test_sim_13$y_13) # 91830
mape(test_sim_13$pr_gam_13_comp, test_sim_13$y_13) # 0.7211827

plot_gam_13_comp <- ggplot() +
  geom_line(data=sim_13, aes(x=Date, y=y_13)) +
  geom_line(data=train_sim_13, aes(x=Date, y=fit_gam_13_comp), col='red') +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_13_comp_upr), col='blue', size = 1) +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_13_comp_lwr), col='blue', size = 1) +
  geom_line(data=test_sim_13, aes(x=Date, y=pr_gam_13_comp), col='red') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_13_comp



plots_13 <- grid.arrange(plot_lm_no_13, plot_gam_no_13, plot_lm_13, plot_gam_13_comp, plot_gam_13, ncol=2)

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/1st_order_complex.pdf", plots_13,  width = 20, height = 17, units = "cm")


@

<<2nd order with complex interaction, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=

# Add Fourier data
fourier <- as.data.table(fourier(ts(sim_23$y_23, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12
sim_23 <- cbind(sim_23, fourier)

# Train and Test Data
train_sim_23 <- sim_23[8:280,]
test_sim_23 <- sim_23[281:360,]

set.seed(123456)
########################################################
# FITTING
########################################################

# LINEAR MODEL NO INTERACTION 
lm_no_23 <- brm(y_23 ~ trend + first_sum + second_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_23, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no_23))
train_sim_23$fit_lm_no_23 <- fit$Estimate
pred_lm_no_23 <- as.data.frame(predict(lm_no_23, newdata = test_sim_23, type = 'response'))
test_sim_23$pr_lm_no_23 <- pred_lm_no_23$Estimate
test_sim_23$pr_lm_no_23_upr <- pred_lm_no_23$Q97.5
test_sim_23$pr_lm_no_23_lwr <- pred_lm_no_23$Q2.5
#90%
pred_lm_no_23_90 <- as.data.frame(predict(lm_no_23, newdata = test_sim_23, type = 'response', probs = c(0.05, 0.95)))
test_sim_23$pr_lm_no_23_90lwr <- pred_lm_no_23_90$Q5
test_sim_23$pr_lm_no_23_90upr <- pred_lm_no_23_90$Q95
test_sim_23$lm_no_23_CI95_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_lm_no_23_lwr | test_sim_23$y_23 > test_sim_23$pr_lm_no_23_upr, 1, 0) 
100 - mean(test_sim_23$lm_no_23_CI95_out)*100 # 60 \%
test_sim_23$lm_no_23_CI90_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_lm_no_23_90lwr | test_sim_23$y_23 > test_sim_23$pr_lm_no_23_90upr, 1, 0) 
100 - mean(test_sim_23$lm_no_23_CI90_out)*100 # 51.25 \% 

rmse(test_sim_23$pr_lm_no_23, test_sim_23$y_23) # 498.4015
mape(test_sim_23$pr_lm_no_23, test_sim_23$y_23) # 0.2298072


plot_lm_no_23 <- ggplot() +
  geom_line(data=sim_23, aes(x=Date, y=y_23)) +
  geom_line(data=train_sim_23, aes(x=Date, y=fit_lm_no_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_lm_no_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_lm_no_23_upr), col='blue') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_lm_no_23_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no_23

# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no_23 <- brm(y_23 ~ s(trend)  + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = NULL, data = train_sim_23, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_no_23))
train_sim_23$fit_gam_no_23 <- fit$Estimate
pred_gam_no_23 <- as.data.frame(predict(gam_no_23, newdata = test_sim_23, type = 'response'))
test_sim_23$pr_gam_no_23 <- pred_gam_no_23$Estimate
test_sim_23$pr_gam_no_23_upr <- pred_gam_no_23$Q97.5
test_sim_23$pr_gam_no_23_lwr <- pred_gam_no_23$Q2.5
#90%
pred_gam_no_23_90 <- as.data.frame(predict(gam_no_23, newdata = test_sim_23, type = 'response', probs = c(0.05, 0.95)))
test_sim_23$pr_gam_no_23_90lwr <- pred_gam_no_23_90$Q5
test_sim_23$pr_gam_no_23_90upr <- pred_gam_no_23_90$Q95
test_sim_23$gam_no_23_CI95_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_gam_no_23_lwr | test_sim_23$y_23 > test_sim_23$pr_gam_no_23_upr, 1, 0) 
100 - mean(test_sim_23$gam_no_23_CI95_out)*100 #  92.5 \%
test_sim_23$gam_no_23_CI90_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_gam_no_23_90lwr | test_sim_23$y_23 > test_sim_23$pr_gam_no_23_90upr, 1, 0) 
100 - mean(test_sim_23$gam_no_23_CI90_out)*100 # 81.25 \% 

rmse(test_sim_23$pr_gam_no_23, test_sim_23$y_23) # 412.3539 
mape(test_sim_23$pr_gam_no_23, test_sim_23$y_23) # 0.1881194

plot_gam_no_23 <- ggplot() +
  geom_line(data=sim_23, aes(x=Date, y=y_23)) +
  geom_line(data=train_sim_23, aes(x=Date, y=fit_gam_no_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_no_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_no_23_upr), col='blue') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_no_23_lwr), col='blue')+
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no_23

# LINEAR MODEL WITH INTERACTION 
lm_23 <- brm(y_23 ~ trend * first_sum + trend * second_sum, iter=4000, family = gaussian(), prior = NULL, data = train_sim_23, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_23))
train_sim_23$fit_lm_23 <- fit$Estimate
pred_lm_23 <- as.data.frame(predict(lm_23, newdata = test_sim_23, type = 'response'))
test_sim_23$pr_lm_23 <- pred_lm_23$Estimate
test_sim_23$pr_lm_23_upr <- pred_lm_23$Q97.5
test_sim_23$pr_lm_23_lwr <- pred_lm_23$Q2.5
#90%
pred_lm_23_90 <- as.data.frame(predict(lm_23, newdata = test_sim_23, type = 'response', probs = c(0.05, 0.95)))
test_sim_23$pr_lm_23_90lwr <- pred_lm_23_90$Q5
test_sim_23$pr_lm_23_90upr <- pred_lm_23_90$Q95
test_sim_23$lm_23_CI95_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_lm_23_lwr | test_sim_23$y_23 > test_sim_23$pr_lm_23_upr, 1, 0) 
100 - mean(test_sim_23$lm_23_CI95_out)*100 # 68.75 \%
test_sim_23$lm_23_CI90_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_lm_23_90lwr | test_sim_23$y_23 > test_sim_23$pr_lm_23_90upr, 1, 0) 
100 - mean(test_sim_23$lm_23_CI90_out)*100 #  58.750 \% 

rmse(test_sim_23$pr_lm_23, test_sim_23$y_23) # 345.0818 
mape(test_sim_23$pr_lm_23, test_sim_23$y_23) # 0.1419703

plot_lm_23 <- ggplot() +
  geom_line(data=sim_23, aes(x=Date, y=y_23)) +
  geom_line(data=train_sim_23, aes(x=Date, y=fit_lm_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_lm_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_lm_23_upr), col='blue') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_lm_23_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm_23

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam_23 <- brm(y_23 ~ s(trend, first_sum, second_sum, k=30), iter=4000, family = gaussian(), prior = NULL, data = train_sim_23, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_23))
train_sim_23$fit_gam_23 <- fit$Estimate
pred_gam_23 <- as.data.frame(predict(gam_23, newdata = test_sim_23, type = 'response'))
test_sim_23$pr_gam_23 <- pred_gam_23$Estimate
test_sim_23$pr_gam_23_upr <- pred_gam_23$Q97.5
test_sim_23$pr_gam_23_lwr <- pred_gam_23$Q2.5
#90%
pred_gam_23_90 <- as.data.frame(predict(gam_23, newdata = test_sim_23, type = 'response', probs = c(0.05, 0.95)))
test_sim_23$pr_gam_23_90lwr <- pred_gam_23_90$Q5
test_sim_23$pr_gam_23_90upr <- pred_gam_23_90$Q95
test_sim_23$gam_23_CI95_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_gam_23_lwr | test_sim_23$y_23 > test_sim_23$pr_gam_23_upr, 1, 0) 
100 - mean(test_sim_23$gam_23_CI95_out)*100 #  25 \%
test_sim_23$gam_23_CI90_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_gam_23_90lwr | test_sim_23$y_23 > test_sim_23$pr_gam_23_90upr, 1, 0)
100 - mean(test_sim_23$gam_23_CI90_out)*100 #  21.25 \% 

rmse(test_sim_23$pr_gam_23, test_sim_23$y_23) #  246.3039
mape(test_sim_23$pr_gam_23, test_sim_23$y_23) # 0.1653217

plot_gam_23 <- ggplot() +
  geom_line(data=sim_23, aes(x=Date, y=y_23)) +
  geom_line(data=train_sim_23, aes(x=Date, y=fit_gam_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_23), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_23_upr), col='blue') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_23_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Nonparametric Model (NPM)")
plot_gam_23

# ADDITIVE MODEL COMPLEX 
gam_23_comp <- brm(y_23 ~ s(trend) + s(first_sum, k=6) + s(second_sum, k=6) + s(trend, first_sum) + s(trend, second_sum), iter=4000, family = gaussian(), prior = NULL, data = train_sim_23, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_23_comp))
train_sim_23$fit_gam_23_comp <- fit$Estimate
pred_gam_23_comp <- as.data.frame(predict(gam_23_comp, newdata = test_sim_23, type = 'response'))
test_sim_23$pr_gam_23_comp <- pred_gam_23_comp$Estimate
test_sim_23$pr_gam_23_comp_upr <- pred_gam_23_comp$Q97.5
test_sim_23$pr_gam_23_comp_lwr <- pred_gam_23_comp$Q2.5
#90%
pred_gam_23_comp_90 <- as.data.frame(predict(gam_23_comp, newdata = test_sim_23, type = 'response', probs = c(0.05, 0.95)))
test_sim_23$pr_gam_23_comp_90lwr <- pred_gam_23_comp_90$Q5
test_sim_23$pr_gam_23_comp_90upr <- pred_gam_23_comp_90$Q95
test_sim_23$gam_23_comp_CI95_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_gam_23_comp_lwr | test_sim_23$y_23 > test_sim_23$pr_gam_23_comp_upr, 1, 0) 
100 - mean(test_sim_23$gam_23_comp_CI95_out)*100 #   90 \%
test_sim_23$gam_23_comp_CI90_out <- ifelse(test_sim_23$y_23 < test_sim_23$pr_gam_23_comp_90lwr | test_sim_23$y_23 > test_sim_23$pr_gam_23_comp_90upr, 1, 0)
100 - mean(test_sim_23$gam_23_comp_CI90_out)*100 # 86.25  \% 

rmse(test_sim_23$pr_gam_23_comp, test_sim_23$y_23) #  212.0902
mape(test_sim_23$pr_gam_23_comp, test_sim_23$y_23) # 0.1230715


plot_gam_23_comp <- ggplot() +
  geom_line(data=sim_23, aes(x=Date, y=y_23)) +
  geom_line(data=train_sim_23, aes(x=Date, y=fit_gam_23_comp), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_23_comp), col='red') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_23_comp_upr), col='blue') +
  geom_line(data=test_sim_23, aes(x=Date, y=pr_gam_23_comp_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') +  ylab('y') +
  ggtitle("Additive Model With Interaction (AMI)")
plot_gam_23_comp

plots_23 <- grid.arrange(plot_lm_no_23, plot_gam_no_23, plot_lm_23, plot_gam_23_comp, plot_gam_23, ncol=2)

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/R Codes/2nd_order_complex.pdf", plots_23,  width = 20, height = 17, units = "cm")
ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/2nd_order_complex.pdf", plots_23,  width = 20, height = 17, units = "cm")


@



\begin{table} 
\caption{\label{RMSFE_sim_1st}1st order Fourier Simulations: Root Mean Square Forecast Error (RMSFE) and Mean Average Percentage Error (MAPE) and  prediction interval coverage for the Bayesian Linear Model with no Interaction (LMNI), the Additive Model with no Interaction (AMNI), the Linear Model with Interaction (LMI), the Additive Model with Interactions (AMI) and the Nonparametric Model (NPM).} \\[1.8ex] 
\begin{center}
\fbox{%
\resizebox{\columnwidth}{0.065\textheight}{
\begin{tabular}{*{13}{c}}
&  \multicolumn{4}{c}{$sim_1$}&\multicolumn{4}{c}{$sim_2$} &\multicolumn{4}{c}{$sim_3$}\\
\cline{2-5} \cline{6-9} \cline{10-13} \\[-1.8ex] 
&  RMSFE & MAPE& 95\% & 90\% & RMSFE & MAPE & 95\% & 90\% & RMSFE & MAPE & 95\% & 90\% \\
  \hline 
LMNI           & 76  & 0.06 & 64  & 60 & 6441  & 3.12 & 45 & 36 & 127144 & 0.94 & 92.5 & 86 \\ % 1st
AMNI           & 89  & 0.06 & 52.5&50  & 6461  & 2.82 & 49 & 40 & 105791 & 0.90 & 90   & 79  \\ % 1st
LMI            & 77  & 0.06 & 65  & 60 & 3139  & 0.72 & 55 & 36 & 121757 & 0.88 & 82.5 & 76  \\ % 1st
AMI            & 61  & 0.05 & 66  & 55 & 3363  & 0.77 & 50 & 44 &  91830 & 0.72 & 76   & 76  \\ %1st
NPM            & 93  & 0.07 & 49  & 45 & 3354  & 0.75 & 50 & 45 &  92478 & 0.76 & 76   & 76  \\ %1st
\end{tabular}} 
}
\end{center}
\end{table}



\begin{table}
\caption{\label{RMSFE_sim_2nd}2nd order Fourier Simulations: Root Mean Square Forecast Error (RMSFE) and Mean Average Percentage Error (MAPE) and  prediction interval coverage for the Bayesian Linear Model with no Interaction (LMNI), the Additive Model with no Interaction (AMNI), the Linear Model with Interaction (LMI), the Additive Model with Interactions (AMI) and the Nonparametric Model (NPM).} 
\begin{center} 
\fbox{%
\resizebox{\columnwidth}{0.065\textheight}{
\begin{tabular}{*{13}{c}}
&  \multicolumn{4}{c}{$sim_4$}&\multicolumn{4}{c}{$sim_5$} &\multicolumn{4}{c}{$sim_6$}\\
\cline{2-5} \cline{6-9} \cline{10-13} \\ [-1.8ex]
&  RMSFE & MAPE & 95\% & 90\% & RMSFE & MAPE & 95\% & 90\% & RMSFE & MAPE & 95\% & 90\% \\
\hline
LMNI           & 30 & 0.10 & 80   & 74 & 1462 & 1.17  & 59 & 49  &  498 & 0.23 & 60   & 51  \\  % 2nd
AMNI           & 28 & 0.09 & 94   & 86 & 1426 & 1.28  &57.5& 51  &  412 & 0.19 & 92.5 & 81    \\ % 2nd
LMI            & 36 & 0.13 & 80   & 75 &  847 & 1.46  & 55 & 51  &  345 & 0.14 &  69  & 59  \\ % 2nd
AMI            & 30 & 0.19 & 87.5 & 76 &  815 & 1.39  & 60 & 40  &  212 & 0.12 &  90  & 87  \\ % 2nd
NPM            & 36 & 0.13 & 85   & 75 &  250 & 0.12  & 60 & 49  &  246 & 0.17 &  25  & 21  \\ % 2nd
\end{tabular}} 
}
\end{center}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time Series Analysis of Migration Data}\label{empirical analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Swiss Immigration Data 1981-2013}\label{Empirics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We take advantage of the monthly released Swiss migration data to elaborate an empirical implementation of the model. In Switzerland the migratory inflows data stem from the Swiss Federal Statistical Office (SFSO). The sources used for the immigration of the non-Swiss population come from PETRA (Statistics for the Resident Population of Foreign Nationality), for the period 1981-2009, and from STATPOP (Population and Households Statistics) for the subsequent years.\\*
The combination of different counts creates a minor discrepancy. PETRA reports the month and the year of the residence permit emission, while STATPOP records the date of movement of the migrants. Since people arriving in Switzerland can obtain their \textit{Ausl\"{a}nderausweis} (Residential Permit) few months after their arrival, profiting of legal arrangements like the tourist allowances, trivial incongruities could emerge. The present study accounts for the year 2010 people who arrived in 2010, but who obtained their permit in 2011. Due to this choice, inconsistencies can be detected between the gross immigration data used in this paper and the ones reported by the SFSO. The magnitude of the disparities amounts to about 9.000-10.000 migrants per year for the period 2011-2013. This procedure might be imperfect, since from 1981 to 2010 we consider the date in which the permit was obtained while from 2011 to 2013 the arrival data. However, we think that this solution is better than the withdrawing of all the observations for which the arrival month was not available. \\
The resulting data format is a time series, which has two peculiarities compared to datasets normally used to produce immigration forecasts. On one side, it uses monthly, rather than annual, data. This choice is grounded in the idea to capture, otherwise unobservable, short-term fluctuations \citep{disney2015evaluation}, while avoiding the inconsistencies which characterize the long term forecasts based on annual observations \citep{hajnal1955prospects, Taeuber1969}. On the other side, the forecasts are performed over the aggregate time series rather than over its single age components. This decision has several upsides. The models set-up costs are reduced to a minimum, any "sum-back" process is avoided and all the typical problems related to the forecasting of aggregate values starting from disaggregated ones are avoided \citep{hendry2006forecasting, bermingham2014understanding}. Furthermore, given the invariance of the age frequencies in our sample, see Figure \ref{fig:imageplot}, in order to obtain the age profile of the migrants, it is sufficient to multiply the aggregate outcome by the age frequencies. A disaggregated age prediction is performed in section \ref{disaggregated results} in order to show the versatility of our approach and its applicability to longitudinal datasets. 

<<age distribution, cache=TRUE, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE, eval=TRUE>>=

ts_age <- read.csv("/path/to/file/All Age time series.csv", sep = ",", header = TRUE, stringsAsFactors = FALSE)
ts_age$X <-NULL
ts_age <- ts_age[which(ts_age$Month < 13),]

#New Variables
ts_age$agg.month <- ave(ts_age$x, ts_age$Date, FUN=sum) 
ts_age$frequency <- ts_age$x /ts_age$agg.month
ts_age$agg.year <- ave(ts_age$x, ts_age$Year, FUN=sum) 
ts_age$y.x <- ave(ts_age$x, ts_age$Year, ts_age$Age, FUN=sum) 
ts_age$y.frequency <- ts_age$y.x /ts_age$agg.year
ts_age$av.frequency <- ave(ts_age$frequency, ts_age$Age, FUN=mean)
ts_age0 <- ts_age[, c('Date')]
ts_age0 <- ts_age0[!duplicated(ts_age0)]
ts_age0 <- as.data.frame(ts_age0)
colnames(ts_age0)[1] <- "Date"
ts_age0$t <- 1:396
ts_age <- merge(ts_age, ts_age0, by=c('Date'))

x <- 1:396
y <- 0:100
ts_age0 <- ts_age[, c('Age', 'frequency', 'Date')]
w <- reshape(ts_age0, 
             timevar   = c("Date"),
             idvar     = c("Age"),
             direction = "wide")
z <- w[,-1]
z <- as.matrix(t(z))

@

<<imageplot, cache=TRUE, echo=FALSE, fig.width=8, fig.height=3, fig.lp="fig:", fig.pos='!ht', fig.cap = 'Image plot showing migration frequencies, where dark gray indicates low frequency and light gray high frequency, by age from 01.01.1981 to 01.12.2013 (T=396).'>>=
#pdf(file = "/home/alice/Dropbox/Thesis/Latex/imageplot-1.pdf", paper = "special", height = 3, width = 6.5, family='serif')
#image.plot(x, y, z, xlab='Time periods', ylab='Age', family='serif')
#dev.off()

image(x, y, z, xlab='Time periods', ylab='Age', family='serif', col = gray((0:10)/10))

@



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model Validation} \label{results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The previous sections point out how the construction of a predictive model is performed at two stages: the analysis of the observed data and the incorporation of new available information. Hereafter we focus on both stages, first by undertaking a descriptive analysis of the immigration time series and then by elaborating suitable priors. 
The starting point is to decompose the logarithm of the number of monthly arrivals into a time and a seasonal trend, as in equation (\ref{decompose}), see Figure \ref{fig:decomposition}.

<<decomposition, cache=TRUE, echo=FALSE, fig.width=8, fig.height=4.5, fig.lp="fig:", fig.pos='!ht', fig.cap = 'Swiss Immigrant Flows Decomposition obtained using a locally weighted scatter-plot smoother (LOESS curve).  From the top to the bottom, the plots show: observed data, global trend, seasonal trend and random noise. '>>=

dec <- stl(ts1, t.window=100, s.window=7, robust=TRUE)
plot(dec)

ts_dec <- as.data.frame(dec$time.series)
ts$seasonal_dec <- ts_dec$seasonal


@

Given that the best trade trade-off between parsimony and accuracy is achieved with a 2nd order Fourier seasonality the reference model with interaction becomes,
\begin{align}\label{fourier_2nd_lm}
y_{t}= \beta_0 + \beta_1 \text{trend}_t + \sum_{i=1}^2 \beta_{2i} (\cos_{it} + \sin_{it}) + \sum_{i=1}^2 \beta_{3i} \text{trend}_t* (\cos_{it} + \sin_{it}) + \epsilon_{t}.
\end{align}
For the coefficients of equation (\ref{fourier_2nd_lm}) we use as prior a multivariate normal distribution,  
\begin{align}\label{matrix prior1}
\begin{bmatrix}
\beta_{0} \\
\beta_{1}\\
\beta_{21}\\
\beta_{22}\\
\beta_{31} \\
\beta_{32} 
\end{bmatrix}
\sim N
\begin{bmatrix}
\begin{pmatrix}
9 \\
0 \\
0 \\
0 \\
0 \\
0 
\end{pmatrix}
,
\begin{pmatrix}
0.5 & 0 & 0 & 0 & 0 & 0  \\ 
0 & 0.5 & 0 & 0 & 0 & 0  \\
0 & 0 & 0.5 & 0 & 0 & 0  \\
0 & 0 & 0 & 0.5 & 0 & 0  \\
0 & 0 & 0 & 0 & 0.5 & 0 \\
0 & 0 & 0 & 0 &  0  & 0.5
\end{pmatrix}
\end{bmatrix},
\end{align}
where the mean of the intercept has been set to 9, which is the average value of the logarithm of the immigration for the observed periods, while the standard deviation is 0.5 since we are confident that the mean can be neither lower than 7.4 nor higher than 10.6. The mean of the other coefficients is centered on 0 with a standard deviation of 0.5. This choice imposes the means of the $\beta$s of global trend, sine, cosine and trend-(sin+cos) interaction to be between -2 and 2. Note that the absence of covariance between the different coefficients allows to vectorize the prior and speed up the convergence rate of the algorithm. The same prior is used for the only coefficient of the equivalent additive model,
\begin{align}\label{fourier_2nd_gam}
y_{t}=\beta_{0}+ f_{1}(\text{trend}_{t})+\sum_{i=1}^{2}f_{2i}(\cos_{it}+\sin_{it})+\sum_{i=1}^{2}f_{3i}(\text{trend}_{t}, \cos_{it}+\sin_{it})+\epsilon_{t},
\end{align}
such that $\beta_0 \sim N(9, 0.5)$. The coefficients which identify the splines are assumed to be normally distributed with variances equal to 1, while the priors of the standard deviation of the error and of $\lambda$ are set to half Cauchy, see \citet{StanPrior}, 
\begin{align}\label{variance}
\sigma_{\epsilon} \sim HC(0,2) \quad \quad \sigma_{\lambda} \sim HC(0,2).
\end{align}

Therefore, the coefficients' priors should be considered as empirical informative, while the variances' ones weakly informative, see Figure \ref{fig:weak priors}.

<<weak priors, cache=TRUE, echo=FALSE, fig.width=10, fig.height=2.5, fig.lp="fig:", fig.pos='!ht', fig.cap = 'Representation of the priors for the coefficients and the variances.'>>=

p1 <- ggplot(data.frame(x = c(-5, 13)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 9, sd = .5),  size = .75, aes(linetype = "Intercept: N(9,0.5)")) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = .5),  size = .75, aes(linetype = "Trend: N(0,0.5)")) +
  scale_linetype_manual(name="",  values=c("Intercept: N(9,0.5)"=1, "Trend: N(0,0.5)"=2)) +
  theme_bw() +
  theme(text=element_text(family="serif"), legend.position="bottom")

p2 <- ggplot(data.frame(x = c(0, 13)), aes(x = x)) +
  stat_function(fun = dhalfcauchy, args = list(scale = 2), size = .75, aes(linetype = "HC(0,2)")) +
  scale_linetype_manual(name="Variances",  values=c("HC(0,2)"=1)) +
  theme_bw() +
  theme(text=element_text(family="serif"), legend.position="bottom")  

grid.arrange(p1, p2, ncol = 2)


@


Given the aim of our analysis, we need to check which of the specifications presented in section \ref{methodology} produces the best forecasts. The first step in this direction requires to split the data into two subsets. The \textit{training data} (from January 1981 till December 2003) are used to fit the model, and the \textit{test data} (from January 2004 till December 2013), are used to predict the immigration flows. Once this first comparison has been done, it is possible to check the forecast's accuracy. Table \ref{RMSFE} reports three standard measures to compare the \textit{ex post} average forecast with the observed values.

<<computation, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=

#Fourier
fourier <- as.data.table(fourier(ts(ts$xlog, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12

ts <- cbind(ts, fourier)
train <- cbind(train, fourier[1:276,])
test <- cbind(test, fourier[277:396,])

# Set Prior
prior <- c(set_prior("normal(9,.5)", class = "Intercept"),
           set_prior("normal(0,.5)", class = "b"),
           set_prior("cauchy(0,2)", class = "sigma"))

prior1 <- c(set_prior("normal(9,.5)", class = "Intercept"),
            set_prior("normal(0,.5)", class = "b"),
            set_prior("cauchy(0,2)", class = "sigma"),
            set_prior("cauchy(0,2)", class = "sds"))

set.seed(123456)
########################################################
# FITTING SHORT RUN ORDER 2
########################################################
# LINEAR MODEL NO INTERACTION 
lm_no <- brm(xlog ~ trend + first_sum + second_sum, iter=4000, family = gaussian(), prior = prior, data = train, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no))
pred_lm_no <- as.data.frame(predict(lm_no, newdata = test, type = 'response'))
test$pr_lm_no <- pred_lm_no$Estimate
test$pr_lm_no_upr <- pred_lm_no$Q97.5
test$pr_lm_no_lwr <- pred_lm_no$Q2.5

#90%
pred_lm_no_90 <- as.data.frame(predict(lm_no, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_lm_no_90lwr <- pred_lm_no_90$Q5
test$pr_lm_no_90upr <- pred_lm_no_90$Q95
test$lm_no_CI95_out <- ifelse(test$xlog < test$pr_lm_no_lwr | test$xlog > test$pr_lm_no_upr, 1, 0) 
100 - mean(test$lm_no_CI95_out)*100 #   94.16667 \%
test$lm_no_CI90_out <- ifelse(test$xlog < test$pr_lm_no_90lwr | test$xlog > test$pr_lm_no_90upr, 1, 0)
100 - mean(test$lm_no_CI90_out)*100 # 87.5  \% 

rmse(test$pr_lm_no, test$xlog) # 0.3238802
mape(test$pr_lm_no, test$xlog) # 0.02899339

# check the residuals 
residuals_lm_no <- as.data.frame(residuals(lm_no))
residuals_lm_no <- ts(residuals_lm_no$Estimate)
auto.arima(residuals_lm_no, seasonal = TRUE, d = 0) # ARIMA(1,0,3)

# check posterior
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Check_lm_no.pdf", onefile = T, paper = "special", family = 'serif')
par(mfrow = c(2,1))
plot(lm_no, ask=F)
dev.off()

plot_lm_no <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm_no), col='red') +
  geom_line(aes(y=pr_lm_no_upr), col='blue') +
  geom_line(aes(y=pr_lm_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no


# LINEAR MODEL WITH INTERACTION 
lm <- brm(xlog ~ trend * first_sum + trend*second_sum, iter=4000, family = gaussian(), prior = prior, data = train, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(lm))
train$fit_lm <- fit$Estimate
pred_lm <- as.data.frame(predict(lm, newdata = test, type = 'response'))
test$pr_lm <- pred_lm$Estimate
test$pr_lm_upr <- pred_lm$Q97.5
test$pr_lm_lwr <- pred_lm$Q2.5

#90%
pred_lm_90 <- as.data.frame(predict(lm, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_lm_90lwr <- pred_lm_90$Q5
test$pr_lm_90upr <- pred_lm_90$Q95
test$lm_CI95_out <- ifelse(test$xlog < test$pr_lm_lwr | test$xlog > test$pr_lm_upr, 1, 0) 
100 - mean(test$lm_CI95_out)*100 #   94.16667 \%
test$lm_CI90_out <- ifelse(test$xlog < test$pr_lm_90lwr | test$xlog > test$pr_lm_90upr, 1, 0)
100 - mean(test$lm_CI90_out)*100 # 88.33333  \% 

rmse(test$pr_lm, test$xlog) # 0.323987
mape(test$pr_lm, test$xlog) # 0.0290128

# check the residuals 
residuals_lm <- as.data.frame(residuals(lm))
residuals_lm <- ts(residuals_lm$Estimate)
Box.test(residuals_lm) # p-value < 2.2e-16
pdf('/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Residuals_LM_ST.pdf', height = 3.5, width = 6)
acf(residuals_lm, main = "Linear Model with Interaction (LMI)", ylim = c(-0.5,1))
dev.off()

auto.arima(residuals_lm, seasonal = TRUE, d = 0) # ARIMA(1,0,1)

# check posterior
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Check_lm.pdf", onefile = T, paper = "special", family = 'serif')
par(mfrow = c(2,1))
plot(lm, ask=F)
dev.off()

plot_lm <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm), col='red') +
  geom_line(aes(y=pr_lm_upr), col='blue') +
  geom_line(aes(y=pr_lm_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm


# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no <- brm(xlog ~ s(trend) + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = prior1, data = train, 
              control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(gam_no))
train$fit_gam_no <- fit$Estimate
pred_gam_no <- as.data.frame(predict(gam_no, newdata = test, type = 'response'))
test$pr_gam_no <- pred_gam_no$Estimate
test$pr_gam_no_upr <- pred_gam_no$Q97.5
test$pr_gam_no_lwr <- pred_gam_no$Q2.5
#90%
pred_gam_no_90 <- as.data.frame(predict(gam_no, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_gam_no_90lwr <- pred_gam_no_90$Q5
test$pr_gam_no_90upr <- pred_gam_no_90$Q95
test$gam_no_CI95_out <- ifelse(test$xlog < test$pr_gam_no_lwr | test$xlog > test$pr_gam_no_upr, 1, 0) 
100 - mean(test$gam_no_CI95_out)*100 #   97.5 \%
test$gam_no_CI90_out <- ifelse(test$xlog < test$pr_gam_no_90lwr | test$xlog > test$pr_gam_no_90upr, 1, 0)
100 - mean(test$gam_no_CI90_out)*100 # 95  \% 

rmse(test$pr_gam_no, test$xlog) # 0.2806513
mape(test$pr_gam_no, test$xlog) # 0.02305808

# check the residuals 
residuals_gam_no <- as.data.frame(residuals(gam_no))
residuals_gam_no <- ts(residuals_gam_no$Estimate)
auto.arima(residuals_gam_no, seasonal = TRUE, d = 0) # ARIMA(3,0,2)

# check posterior
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Check_gam_no.pdf", onefile = T, paper = "special", family = 'serif')
par(mfrow = c(2,1))
plot(gam_no, ask=F)
dev.off()

plot_gam_no <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_no), col='red') +
  geom_line(aes(y=pr_gam_no_upr), col='blue') +
  geom_line(aes(y=pr_gam_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam <- brm(xlog ~ s(trend,first_sum, second_sum), iter=4000, family = gaussian(), prior = prior1, data = train, 
           control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
#gam_comp <- brm(xlog ~  s(trend)  + s(trend, first_sum, second_sum, k=11), iter=4000, family = gaussian(), prior = prior1, data = train, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.2702101, k=11
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.02298522, k=11
fit <- as.data.frame(fitted(gam))
train$fit_gam <- fit$Estimate
pred_gam <- as.data.frame(predict(gam, newdata = test, type = 'response'))
test$pr_gam <- pred_gam$Estimate
test$pr_gam_upr <- pred_gam$Q97.5
test$pr_gam_lwr <- pred_gam$Q2.5
#90%
pred_gam_90 <- as.data.frame(predict(gam, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_gam_90lwr <- pred_gam_90$Q5
test$pr_gam_90upr <- pred_gam_90$Q95
test$gam_CI95_out <- ifelse(test$xlog < test$pr_gam_lwr | test$xlog > test$pr_gam_upr, 1, 0) 
100 - mean(test$gam_CI95_out)*100 #   50 \%
test$gam_CI90_out <- ifelse(test$xlog < test$pr_gam_90lwr | test$xlog > test$pr_gam_90upr, 1, 0)
100 - mean(test$gam_CI90_out)*100 # 39.16667  \% 

rmse(test$pr_gam, test$xlog) # 0.5863969
mape(test$pr_gam, test$xlog) # 0.05978012

# check the residuals 
residuals_gam <- as.data.frame(residuals(gam))
residuals_gam <- ts(residuals_gam$Estimate)
auto.arima(residuals_gam, seasonal = TRUE, d = 0) # ARIMA(1,0,1)

# check posterior
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Check_gam.pdf", onefile = T, paper = "special", family = 'serif')
par(mfrow = c(2,1))
plot(gam, ask=F)
dev.off()

plot_gam <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam), col='red') +
  geom_line(aes(y=pr_gam_upr), col='blue') +
  geom_line(aes(y=pr_gam_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Nonparametric Model (NPM)")
plot_gam


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 

gam_comp <- brm(xlog ~  s(trend) + s(first_sum, k=6) + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = prior1, data = train, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)


fit <- as.data.frame(fitted(gam_comp))
train$fit_gam_comp <- fit$Estimate
pred_gam_comp <- as.data.frame(predict(gam_comp, newdata = test, type = 'response'))
test$pr_gam_comp <- pred_gam_comp$Estimate
test$pr_gam_comp_upr <- pred_gam_comp$Q97.5
test$pr_gam_comp_lwr <- pred_gam_comp$Q2.5
#90%
pred_gam_comp_90 <- as.data.frame(predict(gam_comp, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_gam_comp_90lwr <- pred_gam_comp_90$Q5
test$pr_gam_comp_90upr <- pred_gam_comp_90$Q95
test$gam_comp_CI95_out <- ifelse(test$xlog < test$pr_gam_comp_lwr | test$xlog > test$pr_gam_comp_upr, 1, 0) 
100 - mean(test$gam_comp_CI95_out)*100 #   97.5 \%
test$gam_comp_CI90_out <- ifelse(test$xlog < test$pr_gam_comp_90lwr | test$xlog > test$pr_gam_comp_90upr, 1, 0)
100 - mean(test$gam_comp_CI90_out)*100 # 94.16667  \% 

rmse(test$pr_gam_comp, test$xlog) # 0.2605061
mape(test$pr_gam_comp, test$xlog) # 0.02254154

# check the residuals 
residuals_gam_comp <- as.data.frame(residuals(gam_comp))
residuals_gam_comp <- ts(residuals_gam_comp$Estimate)
Box.test(residuals_gam_comp) # p-value = 0.002177
pdf('/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Residuals_AM_ST.pdf', height = 3.5, width = 6)
acf(residuals_gam_comp, main = "Additive Model with Interaction (AMI)")
dev.off()
auto.arima(residuals_gam_comp, seasonal = TRUE, d = 0) # ARIMA(1,0,2)

# check posterior
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Check_gam_comp.pdf", onefile = T, paper = "special", family = 'serif')
par(mfrow = c(2,1))
plot(gam_comp, ask=F)
dev.off()
pp_check(gam_comp)

fit_gam_comp <- gam_comp$fit
intercept_posterior <- fit_gam_comp$b_Intercept

plot_gam_comp <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_comp), col='red') +
  geom_line(aes(y=pr_gam_comp_upr), col='blue') +
  geom_line(aes(y=pr_gam_comp_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_comp

#                  TI VOGLIO BENE MOSTRO!

par(mfrow = c(3, 2))
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Residuals.pdf", paper = "special", height = 3.5, width = 6, family='serif', onefile = TRUE)
acf(residuals_lm_no)
acf(residuals_lm)
acf(residuals_gam_no)
acf(residuals_gam_comp)
acf(residuals_gam)
dev.off()
par(mfrow = c(1, 1))

plots_short <- grid.arrange(plot_lm_no, plot_gam_no, plot_lm, plot_gam_comp, plot_gam, ncol=2)

write.csv(test, "/path/to/file/Results Bayesian 2006.csv")

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Short_run_rsults.pdf", plots_short,  width = 15, height = 12.5, units = "cm")


####################################################
# AUTOREGRESSIVE AR(1) SHORT TERM FORECASTS
####################################################

# LINEAR MODEL NO INTERACTION 
lm_no <- brm(xlog ~ trend + first_sum + second_sum, iter=4000, family = gaussian(), prior = prior, data = train, 
             cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(lm_no))
pred_lm_no <- as.data.frame(predict(lm_no, newdata = test, type = 'response'))
test$pr_lm_no <- pred_lm_no$Estimate
test$pr_lm_no_upr <- pred_lm_no$Q97.5
test$pr_lm_no_lwr <- pred_lm_no$Q2.5

#90%
pred_lm_no_90 <- as.data.frame(predict(lm_no, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_lm_no_90lwr <- pred_lm_no_90$Q5
test$pr_lm_no_90upr <- pred_lm_no_90$Q95
test$lm_no_CI95_out <- ifelse(test$xlog < test$pr_lm_no_lwr | test$xlog > test$pr_lm_no_upr, 1, 0) 
100 - mean(test$lm_no_CI95_out)*100 #   95.83333 \%
test$lm_no_CI90_out <- ifelse(test$xlog < test$pr_lm_no_90lwr | test$xlog > test$pr_lm_no_90upr, 1, 0)
100 - mean(test$lm_no_CI90_out)*100 # 94.16667  \% 

rmse(test$pr_lm_no, test$xlog) # 0.2550613
mape(test$pr_lm_no, test$xlog) # 0.0221146

# check the residuals 
residuals_lm_no <- as.data.frame(residuals(lm_no))
residuals_lm_no <- ts(residuals_lm_no$Estimate)
Box.test(residuals_lm_no) # a small p-value is evidence against independence
#p-value = 0.002167
auto.arima(residuals_lm_no, seasonal = TRUE, d = 0) # ARIMA(3,0,1)


plot_lm_no <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm_no), col='red') +
  geom_line(aes(y=pr_lm_no_upr), col='blue') +
  geom_line(aes(y=pr_lm_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no


# LINEAR MODEL WITH INTERACTION 
lm <- brm(xlog ~ trend * first_sum + trend*second_sum, iter=4000, family = gaussian(), prior = prior, data = train, 
          cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(lm))
train$fit_lm <- fit$Estimate
pred_lm <- as.data.frame(predict(lm, newdata = test, type = 'response'))
test$pr_lm <- pred_lm$Estimate
test$pr_lm_upr <- pred_lm$Q97.5
test$pr_lm_lwr <- pred_lm$Q2.5

#90%
pred_lm_90 <- as.data.frame(predict(lm, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_lm_90lwr <- pred_lm_90$Q5
test$pr_lm_90upr <- pred_lm_90$Q95
test$lm_CI95_out <- ifelse(test$xlog < test$pr_lm_lwr | test$xlog > test$pr_lm_upr, 1, 0) 
100 - mean(test$lm_CI95_out)*100 #   95.83333 \%
test$lm_CI90_out <- ifelse(test$xlog < test$pr_lm_90lwr | test$xlog > test$pr_lm_90upr, 1, 0)
100 - mean(test$lm_CI90_out)*100 # 93.33333  \% 

rmse(test$pr_lm, test$xlog) # 0.2477069
mape(test$pr_lm, test$xlog) # 0.02051319

# check the residuals 
residuals_lm <- as.data.frame(residuals(lm))
residuals_lm <- ts(residuals_lm$Estimate)
Box.test(residuals_lm) # a small p-value is evidence against independence
#p-value = 0.002167
auto.arima(residuals_lm, seasonal = TRUE, d = 0) # ARIMA(3,0,1)


plot_lm <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm), col='red') +
  geom_line(aes(y=pr_lm_upr), col='blue') +
  geom_line(aes(y=pr_lm_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm


# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no <- brm(xlog ~ s(trend) + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = prior1, data = train, 
              cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_no))
train$fit_gam_no <- fit$Estimate
pred_gam_no <- as.data.frame(predict(gam_no, newdata = test, type = 'response'))
test$pr_gam_no <- pred_gam_no$Estimate
test$pr_gam_no_upr <- pred_gam_no$Q97.5
test$pr_gam_no_lwr <- pred_gam_no$Q2.5
#90%
pred_gam_no_90 <- as.data.frame(predict(gam_no, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_gam_no_90lwr <- pred_gam_no_90$Q5
test$pr_gam_no_90upr <- pred_gam_no_90$Q95
test$gam_no_CI95_out <- ifelse(test$xlog < test$pr_gam_no_lwr | test$xlog > test$pr_gam_no_upr, 1, 0) 
100 - mean(test$gam_no_CI95_out)*100 #    97.5 \%
test$gam_no_CI90_out <- ifelse(test$xlog < test$pr_gam_no_90lwr | test$xlog > test$pr_gam_no_90upr, 1, 0)
100 - mean(test$gam_no_CI90_out)*100 # 94.16667  \% 

rmse(test$pr_gam_no, test$xlog) # 0.2640756
mape(test$pr_gam_no, test$xlog) # 0.02124978

# check the residuals 
residuals_gam_no <- as.data.frame(residuals(gam_no))
residuals_gam_no <- ts(residuals_gam_no$Estimate)
Box.test(residuals_gam_no) # a small p-value is evidence against independence
#p-value = 0.002167
auto.arima(residuals_gam_no, seasonal = TRUE, d = 0) # ARIMA(3,0,2)


plot_gam_no <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_no), col='red') +
  geom_line(aes(y=pr_gam_no_upr), col='blue') +
  geom_line(aes(y=pr_gam_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam <- brm(xlog ~ s(trend,first_sum, second_sum), iter=4000, family = gaussian(), prior = prior1, data = train, 
           cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
#gam_comp <- brm(xlog ~  s(trend)  + s(trend, first_sum, second_sum, k=11), iter=4000, family = gaussian(), prior = prior1, data = train, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.2702101, k=11
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.02298522, k=11
fit <- as.data.frame(fitted(gam))
train$fit_gam <- fit$Estimate
pred_gam <- as.data.frame(predict(gam, newdata = test, type = 'response'))
test$pr_gam <- pred_gam$Estimate
test$pr_gam_upr <- pred_gam$Q97.5
test$pr_gam_lwr <- pred_gam$Q2.5
#90%
pred_gam_90 <- as.data.frame(predict(gam, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_gam_90lwr <- pred_gam_90$Q5
test$pr_gam_90upr <- pred_gam_90$Q95
test$gam_CI95_out <- ifelse(test$xlog < test$pr_gam_lwr | test$xlog > test$pr_gam_upr, 1, 0) 
100 - mean(test$gam_CI95_out)*100 #   93.33333 \%
test$gam_CI90_out <- ifelse(test$xlog < test$pr_gam_90lwr | test$xlog > test$pr_gam_90upr, 1, 0)
100 - mean(test$gam_CI90_out)*100 # 88.33333  \% 

rmse(test$pr_gam, test$xlog) # 0.2712896
mape(test$pr_gam, test$xlog) # 0.02595663

residuals_gam <- as.data.frame(residuals(gam))
residuals_gam <- ts(residuals_gam$Estimate)
Box.test(residuals_gam) # a small p-value is evidence against independence
#p-value = 0.002167
auto.arima(residuals_gam, seasonal = TRUE, d = 0) # ARIMA(1,0,3)

plot_gam <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam), col='red') +
  geom_line(aes(y=pr_gam_upr), col='blue') +
  geom_line(aes(y=pr_gam_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Nonparametric Model (NPM)")
plot_gam


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 

gam_comp <- brm(xlog ~  s(trend) + s(first_sum, k=6) + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = prior1,  data = train, cor_arma(formula = ~1, p = 1),  control = list(adapt_delta=0.99,  max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(gam_comp))
train$fit_gam_comp <- fit$Estimate
pred_gam_comp <- as.data.frame(predict(gam_comp, newdata = test, type = 'response'))
test$pr_gam_comp <- pred_gam_comp$Estimate
test$pr_gam_comp_upr <- pred_gam_comp$Q97.5
test$pr_gam_comp_lwr <- pred_gam_comp$Q2.5
#90%
pred_gam_comp_90 <- as.data.frame(predict(gam_comp, newdata = test, type = 'response', probs = c(0.05, 0.95)))
test$pr_gam_comp_90lwr <- pred_gam_comp_90$Q5
test$pr_gam_comp_90upr <- pred_gam_comp_90$Q95
test$gam_comp_CI95_out <- ifelse(test$xlog < test$pr_gam_comp_lwr | test$xlog > test$pr_gam_comp_upr, 1, 0) 
100 - mean(test$gam_comp_CI95_out)*100 #   98.33333 \%
test$gam_comp_CI90_out <- ifelse(test$xlog < test$pr_gam_comp_90lwr | test$xlog > test$pr_gam_comp_90upr, 1, 0)
100 - mean(test$gam_comp_CI90_out)*100 # 95  \% 

rmse(test$pr_gam_comp, test$xlog) # 0.2465899
mape(test$pr_gam_comp, test$xlog) # 0.02059422

# check the residuals 
residuals_gam_comp <- as.data.frame(residuals(gam_comp))
residuals_gam_comp <- ts(residuals_gam_comp$Estimate)
auto.arima(residuals_gam_comp) # ARIMA(4,0,2)

plot_gam_comp <- ggplot(data=test, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_comp), col='red') +
  geom_line(aes(y=pr_gam_comp_upr), col='blue') +
  geom_line(aes(y=pr_gam_comp_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 10.5)) +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_comp


par(mfrow = c(3, 2))
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Residuals_AR1.pdf", paper = "special", height = 3.5, width = 6, family='serif', onefile = TRUE)
acf(residuals_lm_no)
acf(residuals_lm)
acf(residuals_gam_no)
acf(residuals_gam_comp)
acf(residuals_gam)
dev.off()
par(mfrow = c(1, 1))

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Short_run_residuals_AR1.pdf", residuals_short_ar,  width = 15, height = 12.5, units = "cm")

plots_short_ar <- grid.arrange(plot_lm_no, plot_gam_no, plot_lm, plot_gam_comp, plot_gam, ncol=2)

write.csv(test, "/path/to/file/Results Bayesian 2006 ar1.csv")

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Short_run_results_AR1.pdf", plots_short_ar,  width = 15, height = 12.5, units = "cm")



plots_short_ar <- grid.arrange(plot_lm_no, plot_gam_no, plot_lm, plot_gam_comp, plot_gam, ncol=2)

write.csv(test, "/path/to/file/Results Bayesian 2006 ar1.csv")

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Short_run_rsults_AR1.pdf", plots_short_ar,  width = 15, height = 12.5, units = "cm")



@

 
\begin{table}
\caption{\label{RMSFE}2nd order Fourier models on Swiss Immigration Aggregated Data for Short and Long Run Predictions.} 
\begin{center} 
\fbox{%
\resizebox{\columnwidth}{0.1\textheight}{
\begin{tabular}{*{9}{c}}
&  \multicolumn{4}{c}{Short Run Predictions}&\multicolumn{4}{c}{Long Run Predictions} \\
\cline{2-5} \cline{6-9}  \\
&  RMSFE & MAPE & 95\% & 90\% & RMSFE & MAPE & 95\% & 90\% \\
  \hline 
LMNI           & 0.324 & 0.029 &  94  & 88   & 0.293 & 0.025 &  98 &  94    \\  % 2nd
AMNI           & 0.281 & 0.023 &  98  & 94   & 0.281 & 0.024 &  98 &  94    \\ % 2nd
LMI            & 0.324 & 0.029 &  94  & 88   & 0.287 & 0.025 &  99 &  96    \\ % 2nd
AMI            & 0.260 & 0.022 &  89  & 94   & 0.270 & 0.023 &  99 & 95.5   \\ % 2nd
NPM            & 0.586 & 0.060 &  50  & 39   & 1.572 & 0.183 &  21 &  17    \\ % 2nd
\end{tabular}}
}
\end{center}
\end{table}


According to the RMSFE and the MAPE the AMI emerges as the most accurate model followed by the AMNI, the two linear models, LMNI and LMI, and far behind the NPM. Also in terms of Credible Intervals (CI) the AMI and the AMNI achieve the best coverage with a 98\% for the 95\% CI and a 94\% for the 90\% CI. The poor performance of the NPM seems driven by the usual overfitting of non-additive models. Such specification returns a decreasing trend, while the observed one is increasing. The results' comparison between AMNI and NPM suggests that, in the Swiss data, the necessity of modelling the trend is stronger than the one of the interaction. Nevertheless, a certain degree of non-linear trend-seasonal interplay is still present making the AMI the best predictive model. Figure \ref{fig:prediction} visually portrays the gains quantitatively evaluated in Table \ref{RMSFE}. More detailed results are given in the Additional Material \\

\begin{figure}
\centering
\makebox{\includegraphics[scale=1]{Short_run_rsults.pdf}}
\caption{\label{fig:prediction}Aggregated forecasts 2004-2013 with $95\%$ prediction credible interval from the posterior predictive distribution.}
\end{figure}


Nonetheless, the short time horizon chosen to implement the forecasts might influence the performance results' in favor of the semiparametric models. In fact, the smooth trend term is computed using a piece-wise regression where the \textit{optimal} number of knots is obtained from the training, not the testing data. Therefore, in presence of a highly non-linear trend, the number of knots might be quite high. While this could be of no particular problem in the short run, when the probability of maintaining a trend close to the one of the last periods is realistic, in the long run this might be more problematic. Hence, a safer choice is either to use a low number of knots or to substitute $f_1(\cdot)$ with a parametric component in order to stabilize the semiparametric tendency to overfit out-of-sample forecasts, such that the AMNI becomes,
\begin{align}\label{fourier_gam_no_int_long}
y_{t}=\beta_{0}+\beta_{1}\text{trend}_{t}+\sum_{i=1}^{2}f_{2i}(\cos_{it}+\sin_{it})+\epsilon_{t},
\end{align}

while the AMI is,

\begin{align}\label{fourier_gam_int_long}
y_{t}=\beta_{0}+\beta_{1}\text{trend}_{t}+\sum_{i=1}^{2}f_{2i}(\cos_{it}+\sin_{it})+\sum_{i=1}^{2}f_{3i}(\text{trend}_{t}, \cos_{it}+\sin_{it})+\epsilon_{t},
\end{align}

and the NPM
\begin{align}\label{fourier_non_par_long}
y_{t} = \beta_{0} + \beta_{1}\text{trend}_{t} + f\bigg(\text{trend}_t, \sum_{i=1}^{2} (\cos_{it} + \sin_{it}) \bigg) + \epsilon_{t}.
\end{align}

The results show that the long term predictions highly benefit from a linear trend reducing the RMSFE by 600\% and the MAPE by 900\%. Note that the addition of the parametric term $ \beta_{1} $ is strictly related to the proposed estimation technique, i.e. the thin plate regression splines. If, for example, $ f_{1}(.) $ would have been estimated using a local polynomial approximation the required stabilization would have been embodied in the estimation methodology. 
The results for the forecast period 1998-2013 mainly reflect the ones of the shorter term, with a slight improvement of the LMI over the LMNI. In general, both time horizons show a premium for the AMI specification. It is also interesting to notice how the NPM still overfits the data regardless the additional linear trend term.

\begin{figure}
\centering
\makebox\includegraphics[scale=1]{Long_run_rsults.pdf}}
\caption{\label{fig:prediction new horizons}Forecasts 1998-2013 with $95\%$ prediction credible interval from the posterior predictive distribution.}
\end{figure}


<<different horizon, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=

#Subset time series into train data and test data
train0 <- ts[which(ts$Year < 1999),]
test0 <- ts[which(ts$Year > 1998),]



# Set Prior
prior <- c(set_prior("normal(9,.5)", class = "Intercept"),
           set_prior("normal(0,.5)", class = "b"),
           set_prior("cauchy(0,2)", class = "sigma"))

prior1 <- c(set_prior("normal(9,.5)", class = "Intercept"),
            set_prior("normal(0,.5)", class = "b"),
            set_prior("cauchy(0,2)", class = "sigma"),
            set_prior("cauchy(0,2)", class = "sds"))

set.seed(123456)

########################################################
# FITTING LONG RUN ORDER 2
########################################################
# LINEAR MODEL NO INTERACTION 
lm_no <- brm(xlog ~ trend + first_sum + second_sum, iter=4000, family = gaussian(), prior = prior, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm_no))
train0$fit_lm_no <- fit$Estimate
pred_lm_no <- as.data.frame(predict(lm_no, newdata = test0, type = 'response'))
test0$pr_lm_no <- pred_lm_no$Estimate
test0$pr_lm_no_upr <- pred_lm_no$Q97.5
test0$pr_lm_no_lwr <- pred_lm_no$Q2.5

#90%
pred_lm_no_90 <- as.data.frame(predict(lm_no, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_lm_no_90lwr <- pred_lm_no_90$Q5
test0$pr_lm_no_90upr <- pred_lm_no_90$Q95
test0$lm_no_CI95_out <- ifelse(test0$xlog < test0$pr_lm_no_lwr | test0$xlog > test0$pr_lm_no_upr, 1, 0) 
100 - mean(test0$lm_no_CI95_out)*100 # 98.33333   \%
test0$lm_no_CI90_out <- ifelse(test0$xlog < test0$pr_lm_no_90lwr | test0$xlog > test0$pr_lm_no_90upr, 1, 0)
100 - mean(test0$lm_no_CI90_out)*100 #94.44444   \% 

rmse(test0$pr_lm_no, test0$xlog) # 0.2937312
mape(test0$pr_lm_no, test0$xlog) # 0.02486158

plot_lm_no <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm_no), col='red') +
  geom_line(aes(y=pr_lm_no_upr), col='blue') +
  geom_line(aes(y=pr_lm_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no


# LINEAR MODEL WITH INTERACTION 
lm <- brm(xlog ~ trend * first_sum + trend*second_sum, iter=4000, family = gaussian(), prior = prior, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm))
train0$fit_lm <- fit$Estimate
pred_lm <- as.data.frame(predict(lm, newdata = test0, type = 'response'))
test0$pr_lm <- pred_lm$Estimate
test0$pr_lm_upr <- pred_lm$Q97.5
test0$pr_lm_lwr <- pred_lm$Q2.5

#90%
pred_lm_90 <- as.data.frame(predict(lm, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_lm_90lwr <- pred_lm_90$Q5
test0$pr_lm_90upr <- pred_lm_90$Q95
test0$lm_CI95_out <- ifelse(test0$xlog < test0$pr_lm_lwr | test0$xlog > test0$pr_lm_upr, 1, 0) 
100 - mean(test0$lm_CI95_out)*100 #   98.88889 \%
test0$lm_CI90_out <- ifelse(test0$xlog < test0$pr_lm_90lwr | test0$xlog > test0$pr_lm_90upr, 1, 0)
100 - mean(test0$lm_CI90_out)*100 # 96.11111  \% 

rmse(test0$pr_lm, test0$xlog) # 0.2869398
mape(test0$pr_lm, test0$xlog) # 0.02492606

plot_lm <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm), col='red') +
  geom_line(aes(y=pr_lm_upr), col='blue') +
  geom_line(aes(y=pr_lm_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm


# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no <- brm(xlog ~ trend + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
# gam_no <- brm(xlog ~ s(trend) + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
# 2.034755
# 0.2596056
fit <- as.data.frame(fitted(gam_no))
train0$fit_gam_no <- fit$Estimate
pred_gam_no <- as.data.frame(predict(gam_no, newdata = test0, type = 'response'))
test0$pr_gam_no <- pred_gam_no$Estimate
test0$pr_gam_no_upr <- pred_gam_no$Q97.5
test0$pr_gam_no_lwr <- pred_gam_no$Q2.5
#90%
pred_gam_no_90 <- as.data.frame(predict(gam_no, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_gam_no_90lwr <- pred_gam_no_90$Q5
test0$pr_gam_no_90upr <- pred_gam_no_90$Q95
test0$gam_no_CI95_out <- ifelse(test0$xlog < test0$pr_gam_no_lwr | test0$xlog > test0$pr_gam_no_upr, 1, 0) 
100 - mean(test0$gam_no_CI95_out)*100 #   97.77778 \%
test0$gam_no_CI90_out <- ifelse(test0$xlog < test0$pr_gam_no_90lwr | test0$xlog > test0$pr_gam_no_90upr, 1, 0)
100 - mean(test0$gam_no_CI90_out)*100 # 94.44444 \% 

rmse(test0$pr_gam_no, test0$xlog) # 0.2814052
mape(test0$pr_gam_no, test0$xlog) # 0.0242659

plot_gam_no <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_no), col='red') +
  geom_line(aes(y=pr_gam_no_upr), col='blue') +
  geom_line(aes(y=pr_gam_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam <- brm(xlog ~ trend + s(trend, first_sum, second_sum), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
#gam <- brm(xlog ~  s(trend)  + s(trend, first_sum, second_sum, k=11), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
# 2.514913
# 0.3520861
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.2702101, k=11
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.02298522, k=11
fit <- as.data.frame(fitted(gam))
train0$fit_gam <- fit$Estimate
pred_gam <- as.data.frame(predict(gam, newdata = test0, type = 'response'))
test0$pr_gam <- pred_gam$Estimate
test0$pr_gam_upr <- pred_gam$Q97.5
test0$pr_gam_lwr <- pred_gam$Q2.5
#90%
pred_gam_90 <- as.data.frame(predict(gam, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_gam_90lwr <- pred_gam_90$Q5
test0$pr_gam_90upr <- pred_gam_90$Q95
test0$gam_CI95_out <- ifelse(test0$xlog < test0$pr_gam_lwr | test0$xlog > test0$pr_gam_upr, 1, 0) 
100 - mean(test0$gam_CI95_out)*100 #  21.11111  \%
test0$gam_CI90_out <- ifelse(test0$xlog < test0$pr_gam_90lwr | test0$xlog > test0$pr_gam_90upr, 1, 0)
100 - mean(test0$gam_CI90_out)*100 #  16.66667 \% 

rmse(test0$pr_gam, test0$xlog) # 1.572675
mape(test0$pr_gam, test0$xlog) # 0.1833885

plot_gam <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam), col='red') +
  geom_line(aes(y=pr_gam_upr), col='blue') +
  geom_line(aes(y=pr_gam_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Nonparametric Model (NPM)")
plot_gam


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
gam_comp <- brm(xlog ~  trend + s(first_sum, k=6) + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
#gam_comp <- brm(xlog ~  s(trend)  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
# 2.211443
# 0.2917702
fit <- as.data.frame(fitted(gam_comp))
train0$fit_gam_comp <- fit$Estimate
pred_gam_comp <- as.data.frame(predict(gam_comp, newdata = test0, type = 'response'))
test0$pr_gam_comp <- pred_gam_comp$Estimate
test0$pr_gam_comp_upr <- pred_gam_comp$Q97.5
test0$pr_gam_comp_lwr <- pred_gam_comp$Q2.5
#90%
pred_gam_comp_90 <- as.data.frame(predict(gam_comp, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_gam_comp_90lwr <- pred_gam_comp_90$Q5
test0$pr_gam_comp_90upr <- pred_gam_comp_90$Q95
test0$gam_comp_CI95_out <- ifelse(test0$xlog < test0$pr_gam_comp_lwr | test0$xlog > test0$pr_gam_comp_upr, 1, 0) 
100 - mean(test0$gam_comp_CI95_out)*100 #   99.44444 \%
test0$gam_comp_CI90_out <- ifelse(test0$xlog < test0$pr_gam_comp_90lwr | test0$xlog > test0$pr_gam_comp_90upr, 1, 0)
100 - mean(test0$gam_comp_CI90_out)*100 # 95.55556  \% 

rmse(test0$pr_gam_comp, test0$xlog) # 0.2705777
mape(test0$pr_gam_comp, test0$xlog) # 0.02354029

plot_gam_comp <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_comp), col='red') +
  geom_line(aes(y=pr_gam_comp_upr), col='blue') +
  geom_line(aes(y=pr_gam_comp_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_comp


par(mfrow = c(3, 2))
pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Residuals_Long_Run.pdf", paper = "special", height = 3.5, width = 6, family='serif', onefile = TRUE)
acf(residuals_lm_no)
acf(residuals_lm)
acf(residuals_gam_no)
acf(residuals_gam_comp)
acf(residuals_gam)
dev.off()
par(mfrow = c(1, 1))

plots_long <- grid.arrange(plot_lm_no, plot_gam_no, plot_lm, plot_gam_comp, plot_gam, ncol=2)


ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Long_run_rsults.pdf", plots_long,  width = 15, height = 12.5, units = "cm")


####################################################
# AUTOREGRESSIVE AR(1) LONG TERM FORECASTS
####################################################

# LINEAR MODEL NO INTERACTION 
lm_no <- brm(xlog ~ trend + first_sum + second_sum, iter=4000, family = gaussian(), prior = prior, data = train0, 
             cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(lm_no))
train0$fit_lm_no <- fit$Estimate
pred_lm_no <- as.data.frame(predict(lm_no, newdata = test0, type = 'response'))
test0$pr_lm_no <- pred_lm_no$Estimate
test0$pr_lm_no_upr <- pred_lm_no$Q97.5
test0$pr_lm_no_lwr <- pred_lm_no$Q2.5

#90%
pred_lm_no_90 <- as.data.frame(predict(lm_no, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_lm_no_90lwr <- pred_lm_no_90$Q5
test0$pr_lm_no_90upr <- pred_lm_no_90$Q95
test0$lm_no_CI95_out <- ifelse(test0$xlog < test0$pr_lm_no_lwr | test0$xlog > test0$pr_lm_no_upr, 1, 0) 
100 - mean(test0$lm_no_CI95_out)*100 # 95   \%
test0$lm_no_CI90_out <- ifelse(test0$xlog < test0$pr_lm_no_90lwr | test0$xlog > test0$pr_lm_no_90upr, 1, 0)
100 - mean(test0$lm_no_CI90_out)*100 # 94.44444   \% 

rmse(test0$pr_lm_no, test0$xlog) # 0.2507887
mape(test0$pr_lm_no, test0$xlog) # 0.02181983

plot_lm_no <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm_no), col='red') +
  geom_line(aes(y=pr_lm_no_upr), col='blue') +
  geom_line(aes(y=pr_lm_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Linear Model No Interaction (LMNI)")
plot_lm_no


# LINEAR MODEL WITH INTERACTION 
lm <- brm(xlog ~ trend * first_sum + trend*second_sum, iter=4000, family = gaussian(), prior = prior, data = train0, 
          cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(lm))
train0$fit_lm <- fit$Estimate
pred_lm <- as.data.frame(predict(lm, newdata = test0, type = 'response'))
test0$pr_lm <- pred_lm$Estimate
test0$pr_lm_upr <- pred_lm$Q97.5
test0$pr_lm_lwr <- pred_lm$Q2.5

#90%
pred_lm_90 <- as.data.frame(predict(lm, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_lm_90lwr <- pred_lm_90$Q5
test0$pr_lm_90upr <- pred_lm_90$Q95
test0$lm_CI95_out <- ifelse(test0$xlog < test0$pr_lm_lwr | test0$xlog > test0$pr_lm_upr, 1, 0) 
100 - mean(test0$lm_CI95_out)*100 #   96.66667 \%
test0$lm_CI90_out <- ifelse(test0$xlog < test0$pr_lm_90lwr | test0$xlog > test0$pr_lm_90upr, 1, 0)
100 - mean(test0$lm_CI90_out)*100 # 94.44444  \% 

rmse(test0$pr_lm, test0$xlog) # 0.2516066
mape(test0$pr_lm, test0$xlog) # 0.02123502

plot_lm <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_lm), col='red') +
  geom_line(aes(y=pr_lm_upr), col='blue') +
  geom_line(aes(y=pr_lm_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Linear Model with Interaction (LMI)")
plot_lm


# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no <- brm(xlog ~ trend + s(first_sum, k=6) + s(second_sum, k=6), iter=4000, family = gaussian(), prior = prior1, data = train0, 
              cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

fit <- as.data.frame(fitted(gam_no))
train0$fit_gam_no <- fit$Estimate
pred_gam_no <- as.data.frame(predict(gam_no, newdata = test0, type = 'response'))
test0$pr_gam_no <- pred_gam_no$Estimate
test0$pr_gam_no_upr <- pred_gam_no$Q97.5
test0$pr_gam_no_lwr <- pred_gam_no$Q2.5
#90%
pred_gam_no_90 <- as.data.frame(predict(gam_no, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_gam_no_90lwr <- pred_gam_no_90$Q5
test0$pr_gam_no_90upr <- pred_gam_no_90$Q95
test0$gam_no_CI95_out <- ifelse(test0$xlog < test0$pr_gam_no_lwr | test0$xlog > test0$pr_gam_no_upr, 1, 0) 
100 - mean(test0$gam_no_CI95_out)*100 #   97.22222 \%
test0$gam_no_CI90_out <- ifelse(test0$xlog < test0$pr_gam_no_90lwr | test0$xlog > test0$pr_gam_no_90upr, 1, 0)
100 - mean(test0$gam_no_CI90_out)*100 # 93.33333 \% 

rmse(test0$pr_gam_no, test0$xlog) # 0.2372657
mape(test0$pr_gam_no, test0$xlog) # 0.02152241

plot_gam_no <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_no), col='red') +
  geom_line(aes(y=pr_gam_no_upr), col='blue') +
  geom_line(aes(y=pr_gam_no_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Additive Model No Interaction (AMNI)")
plot_gam_no

# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam <- brm(xlog ~ trend + s(trend,first_sum, second_sum), iter=4000, family = gaussian(), prior = prior1, data = train0, 
           cor_arma(formula = ~1, p = 1), control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

#gam <- brm(xlog ~  s(trend)  + s(trend, first_sum, second_sum, k=11), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
# 2.514913
# 0.3520861
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.2702101, k=11
#  s(trend)  + s(trend, first_sum, second_sum, k=11) 0.02298522, k=11
fit <- as.data.frame(fitted(gam))
train0$fit_gam <- fit$Estimate
pred_gam <- as.data.frame(predict(gam, newdata = test0, type = 'response'))
test0$pr_gam <- pred_gam$Estimate
test0$pr_gam_upr <- pred_gam$Q97.5
test0$pr_gam_lwr <- pred_gam$Q2.5
#90%
pred_gam_90 <- as.data.frame(predict(gam, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_gam_90lwr <- pred_gam_90$Q5
test0$pr_gam_90upr <- pred_gam_90$Q95
test0$gam_CI95_out <- ifelse(test0$xlog < test0$pr_gam_lwr | test0$xlog > test0$pr_gam_upr, 1, 0) 
100 - mean(test0$gam_CI95_out)*100 #  70  \%
test0$gam_CI90_out <- ifelse(test0$xlog < test0$pr_gam_90lwr | test0$xlog > test0$pr_gam_90upr, 1, 0)
100 - mean(test0$gam_CI90_out)*100 #  53.33333 \% 

rmse(test0$pr_gam, test0$xlog) # 0.549979
mape(test0$pr_gam, test0$xlog) # 0.05354016

plot_gam <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam), col='red') +
  geom_line(aes(y=pr_gam_upr), col='blue') +
  geom_line(aes(y=pr_gam_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Nonparametric Model (NPM)")
plot_gam


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
gam_comp <- brm(xlog ~  trend + s(first_sum, k=6) + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, 
                family = gaussian(), prior = prior1,  data = train0, cor_arma(formula = ~1, p = 1),  control = list(adapt_delta=0.99,  max_treedepth=15), cores=4)

#gam_comp <- brm(xlog ~  s(trend)  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = prior1, data = train0, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
# 2.211443
# 0.2917702
fit <- as.data.frame(fitted(gam_comp))
train0$fit_gam_comp <- fit$Estimate
pred_gam_comp <- as.data.frame(predict(gam_comp, newdata = test0, type = 'response'))
test0$pr_gam_comp <- pred_gam_comp$Estimate
test0$pr_gam_comp_upr <- pred_gam_comp$Q97.5
test0$pr_gam_comp_lwr <- pred_gam_comp$Q2.5
#90%
pred_gam_comp_90 <- as.data.frame(predict(gam_comp, newdata = test0, type = 'response', probs = c(0.05, 0.95)))
test0$pr_gam_comp_90lwr <- pred_gam_comp_90$Q5
test0$pr_gam_comp_90upr <- pred_gam_comp_90$Q95
test0$gam_comp_CI95_out <- ifelse(test0$xlog < test0$pr_gam_comp_lwr | test0$xlog > test0$pr_gam_comp_upr, 1, 0) 
100 - mean(test0$gam_comp_CI95_out)*100 #   97.22222 \%
test0$gam_comp_CI90_out <- ifelse(test0$xlog < test0$pr_gam_comp_90lwr | test0$xlog > test0$pr_gam_comp_90upr, 1, 0)
100 - mean(test0$gam_comp_CI90_out)*100 # 94.44444  \% 

rmse(test0$pr_gam_comp, test0$xlog) # 0.2318742
mape(test0$pr_gam_comp, test0$xlog) # 0.02045177

plot_gam_comp <- ggplot(data=test0, aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_gam_comp), col='red') +
  geom_line(aes(y=pr_gam_comp_upr), col='blue') +
  geom_line(aes(y=pr_gam_comp_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8), 
        axis.text=element_text(size=8), axis.title=element_text(size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(6, 10.5)) +
  ggtitle("Additive Model with Interaction (AMI)")
plot_gam_comp


plots_long_ar <- grid.arrange(plot_lm_no, plot_gam_no, plot_lm, plot_gam_comp, plot_gam, ncol=2)


ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Long_run_results_AR1.pdf", plots_long_ar,  width = 15, height = 12.5, units = "cm")

@

A closer look to the reminders plotted at the bottom of Figure \ref{fig:decomposition} may suggest the presence of autocorrelation among the residuals. Potentially, such autocorrelation might persist also in the residuals of the LMNI, AMNI, LMI, AMI and NPM models. Hence, we try to re-estimate all the models, both for the short and the long run, by allowing their residuals to be autocorrelated of order 1 (AR(1)):

\begin{align}\label{AR(1)}
\epsilon_{t} = \alpha \epsilon_{t-1} + \xi_{t}, \quad -1 < \alpha < 1, \quad \xi_{t} \sim N(0,\sigma_{\xi}^2).
\end{align}

Table \ref{RMSFE AR1} confirms an improvement in prediction accuracy with a reduction of the RMSFE of 11\%, of the MAPE of 33\% and an extra coverage of the 95\% and 90\% credible intervals of respectively of 1.3\% and 15\%. The amelioration can also be traced in Figures \ref{fig:prediction AR(1) short} and \ref{fig:prediction AR(1) long}. In general models' performances are in line with the one of Table \ref{RMSFE}. However, the comparative advantage of the AMI with respect to the LMI, while being improved in the long run, is reduced for the short run. 
A possible explanation can be found in the origin of the autocorrelation among errors. For example, if there are logarithmic or exponential terms in the data generatin process, the LMs would most probably generate autocorrelated errors. Therefore, equation \ref{fourier_2nd_lm} benefits from the introduction of (\ref{AR(1)}). To the contrary, the AMs, by construction, tend to solve autocorrelation generated by functional form misspecifications, gaining less from the inclusion of an AR(1) component. Figure \ref{fig:residuals} shows how the LMI reports stronger evidence of residuals dependence than the AMI. A Box-Pierce test statistic for examining the null hypothesis of independence of the residuals is rejected with a p-value $<$ 2.2e-16 for the LMI and a p-value = 0.0022 for the AMI.
 

\begin{table} 
\caption{\label{RMSFE AR1} 2nd order Fourier models on Swiss Immigration Aggregated Data for Short and Long Run Predictions with AR(1) errors.} 
\begin{center} 
\fbox{%
\resizebox{\columnwidth}{0.1\textheight}{
  \begin{tabular}{@{\extracolsep{5pt}}lcccccccc} 
&  \multicolumn{4}{c}{Short Run Predictions}&\multicolumn{4}{c}{Long Run Predictions} \\
\cline{2-5} \cline{6-9}  \\
&  RMSFE & MAPE & 95\% & 90\% &  RMSFE & MAPE & 95\% & 90\%  \\
  \hline 
LMNI           & 0.255 & 0.022 &  96    & 94   & 0.251 & 0.022 & 95 & 94    \\  % 2nd
AMNI           & 0.264 & 0.021 &  97.5  & 94   & 0.247 & 0.020 & 97 & 93    \\ % 2nd
LMI            & 0.248 & 0.021 &  96    & 93   & 0.252 & 0.021 & 97 & 94    \\ % 2nd
AMI            & 0.247 & 0.021 &  98    & 95   & 0.232 & 0.020 & 100 & 99  \\ % 2nd
NPM            & 0.271 & 0.026 &  94    & 88   & 0.550 & 0.053 & 70 & 53    \\ % 2nd
\end{tabular}}
} 
\end{center}
\end{table}

\begin{figure}
\centering
\makebox{\includegraphics[scale=1]{Short_run_results_AR1.pdf}}
\caption{\label{fig:prediction AR(1) short}Aggregated forecasts 2004-2013 with $95\%$ prediction credible interval from the posterior predictive distribution for models with autocorrelated residuals of order one (AR(1)).}
\end{figure}

\begin{figure}
\centering
\makebox{\includegraphics[scale=1]{Long_run_results_AR1.pdf}}
\caption{\label{fig:prediction AR(1) long}Forecasts 1998-2013 with $95\%$ prediction credible interval from the posterior predictive distribution for models with autocorrelated residuals of order one (AR(1)).}
\end{figure}

\begin{figure}
\begin{minipage}{.5\textwidth}
\centering 
\makebox{\includegraphics[scale=.5]{Residuals_LM_ST.pdf}}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering 
\makebox{\includegraphics[scale=.5]{Residuals_AM_ST.pdf}}
\end{minipage}
\caption{\label{fig:residuals}Estimates of the autocovariance or autocorrelation function for the linear model with interaction (left plot) and for the additive model with interaction (right plot).}
\end{figure}

While the previous sections have shown how the semiparametric models tend to outperform the other alternatives both with simulated and historical data, we now illustrate how robust these results are with respect to the setting of different priors. \\
The first alternative is an uninformative prior defined over $ \mathbb{R} $ for $ \beta $ combined again with two standardized half t-student with three degrees of freedom for $\sigma_{\epsilon}$ and $\sigma_{\lambda}$ \citep{buerkner2016package}. A second possibility is to use the horseshoe hierarchical shrinkage prior with parameter 1 for $ \beta $ combined again with two standardized half t-student for the variances of $ \epsilon $ and $ \lambda $. The outcomes confirm that the AMI is robust to these alternative specifications, see Figure \ref{fig:comparison}.


<<sensitivity, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
##################################################
# FORECASTS
#################################################


#################
# NORMAL
#################

prior1 <- c(set_prior("normal(9,.5)", class = "Intercept"),
           set_prior("normal(0,.5)", class = "b"),
           set_prior("cauchy(0,2)", class = "sigma"),
           set_prior("cauchy(0,2)", class = "sds"))

model <- brm(xlog ~  trend  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = prior1, data = train,  control = list(adapt_delta=.999,  max_treedepth = 15), cores=4)

pred <- as.data.frame(predict(model, newdata = test, type = 'response'))
test$pr_b_vc <- pred$Estimate
test$pr_b_vc_upr <- pred$Q97.5
test$pr_b_vc_lwr <- pred$Q2.5

#################
# HORSESHOE
#################

prior_horse <- c(set_prior("horseshoe(1)"),
                 set_prior("student_t(3,0,1)", class="sigma"),
                 set_prior("student_t(3,0,1)", class="sds"))

model1 <- brm(xlog ~  trend  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = prior_horse, data = train,  control = list(adapt_delta=.999,  max_treedepth = 15), cores=4)
pred <- as.data.frame(predict(model1, newdata = test, type = 'response'))

test$pr_b_vc1 <- pred$Estimate
test$pr_b_vc_upr1 <- pred$Q97.5
test$pr_b_vc_lwr1 <- pred$Q2.5


#################
#UNINFORMATIVE 
#################

model2 <-brm(xlog ~  trend  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=4000, family = gaussian(), prior = NULL, data = train,  control = list(adapt_delta=.999,  max_treedepth = 15), cores=4)
pred <- as.data.frame(predict(model2, newdata = test, type = 'response'))
test$pr_b_vc_un <- pred$Estimate
test$pr_b_vc_upr_un <- pred$Q97.5
test$pr_b_vc_lwr_un <- pred$Q2.5


#RMSFE
rmse(test$pr_b_vc, test$xlog) # 0.3009534
rmse(test$pr_b_vc1, test$xlog) # 0.3056708
rmse(test$pr_b_vc_un, test$xlog) # 0.2992661


####################################
# PLOTS
####################################

test$Date <- as.Date(test$Date, by="1 mon")
ts$Date <- as.Date(ts$Date, by="1 mon")


p1 <- ggplot(data=test, aes(x=Date)) + 
  geom_ribbon(data= test, aes(ymin=pr_b_vc, ymax=pr_b_vc1), fill="blue", alpha="0.5") +
  geom_line(aes(y=pr_b_vc), color="black") +
  geom_line(data= test, aes(y=pr_b_vc1), color="red") +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') + ylab("log(Immigration)") +
  ggtitle("Original Prior vs Horseshoe")
p1

p2 <- ggplot(data=test, aes(x=Date)) + 
  geom_ribbon(data= test, aes(ymin=pr_b_vc, ymax=pr_b_vc_un), fill="blue", alpha="0.5") +
  geom_line(aes(y=pr_b_vc), color="black") +
  geom_line(data= test, aes(y=pr_b_vc_un), color="red") +
  ggtitle("Original Prior vs Uninformative") +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5)) +
  xlab('Year') + ylab("log(Immigration)") 
p2

p <-plot_grid(p1, p2, ncol = 1, nrow = 2)

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/forecasts_comparison.pdf", p,  width = 21, height = 10, units = "cm")



@


\begin{figure}
\centering
\makebox{\includegraphics[scale=0.7]{forecasts_comparison.pdf}}
\caption{\label{fig:comparison}Priors' robustness comparison for the predictions over the period 2004-2014. The black line denotes the forecasts obtained with the original prior distributions used in the analysis. The red lines are the forecasts obtained with comparative priors, the horseshoe in the top and the uninformative in the bottom plot. The difference is the blue area.}
\end{figure}

So far we have shown how the Bayesian Additive Model with Interaction outperforms the alternatives in predicting the behaviour of the test data starting from the fit of the training data. However, our argument in favour of Bayesian statistics was rooted in its capacity to introduce informative believes through the choice of the prior distributions. Therefore, we propose, in the next Subsection, an illustrative example which directly shows the advantages of adopting different priors in the predictions.

\subsection{Forecast Exercise}\label{forecast}

In this subsection we fit the AMI on the Swiss data for all the available years (1981-2013) and we try to forecast immigration flows until 2023. 
Since the priors are used, at the same time, to estimate the model and, indirectly, to implement the forecasts, their distributions link historical knowledge with future expectations. Therefore, if migration is foreseen to look like the past, flat priors with relatively high variances should be chosen. To the contrary, if migration is anticipated to change, informative priors, with smaller variances around the expected means should be selected. The latter case, however, may give room to divergent transitions within the sampling. \\ 
For this study we set up three scenarios. The first pictures the sentiment of a future migration in line with its historical average.
Therefore, it relies on weakly informative priors. For example, the intercept's prior takes the same values as in the model validation exercise with a mean of 9 and a standard deviation of 0.5. In the same way, the trend's prior is centered around the posterior obtained from section \ref{results}, but with a wider variance to convey a minimal impact on its posterior distribution. 
The second scenario is a middle story line, which reflects the possibility of a small shock. In this case, the trend's prior is centered around 1.5, rather than 0, with a variance of 0.2, which suggests an increased return of the trend. 
The third is a scene, which mirrors the expectation of a more evident structural break on the trend's historical impact on $y_{t}$. This last case is achieved by increasing the expected mean up to 2, while shrinking its variance to 0.1.   \\*
In order to make sure that the difference between the three scenarios is only about the researcher's expectations about structural changes we do not modify the priors for the standard deviation of the error term ($\sigma_{\xi}$) and of the smoothing parameter ($\sigma_{\lambda}$) assuming that in every case they are distributed as Half-Cauchy with a scale parameter of 2, like in section \ref{results}.
Instead, we play mostly with the priors of the error's autocorrelation term ($\alpha$) assuming increasing path dependency coherently following the discussion on the trend's priors. 
All the distributions are described in Table \ref{prior distribution 1} and portrayed in Figure \ref{fig:posteriors vs priors}.

<<gam_comp all, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
#Fourier
fourier <- as.data.table(fourier(ts(ts$xlog, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12

ts <- cbind(ts, fourier)
train <- cbind(train, fourier[1:276,])
test <- cbind(test, fourier[277:396,])

prior1 <- c(set_prior("normal(9,.5)", class = "Intercept"),
            set_prior("normal(0,.5)", class = "b"),
            set_prior("cauchy(0,2)", class = "sigma"),
            set_prior("cauchy(0,2)", class = "sds"))

gam_comp <- brm(xlog ~  trend + s(first_sum, k=6) + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior1, data = ts, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

#summary(gam_comp)

posteriors_gam_comp <- posterior_samples(gam_comp)

@




<<posteriors, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=

p_intercept <- ggplot() +
#Intercept
   geom_density(data = posteriors_gam_comp, aes(x = b_Intercept, linetype = "Intercept Posterior"), size = .75) + 
# Intercept Low - Mid - High Scenario
  stat_function(fun = dnorm, args = list(mean = 9, sd = .5),  size = .75, aes(linetype = "Intercept Low, Mid and High Scenario")) +
  scale_linetype_manual(name="",  values=c("Intercept Posterior" = 1, "Intercept Low, Mid and High Scenario" = 2)) +
   theme_bw() + xlab("") +
   guides(linetype=guide_legend(ncol=3)) + ggtitle("Intercept") +  xlim(c(4, 14)) + 
   theme(text=element_text(family="serif"), legend.position="bottom", plot.title = element_text(hjust = 0.5))

p_intercept

p_trend <- ggplot() +
# Trend 
   geom_density(data = posteriors_gam_comp, aes(x = b_trend, linetype = "Trend Posterior"), size = .75) + 
# Trend Low Scenario
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),  size = .75, aes(linetype = "Trend Low Scenario")) +
# Trend Mid Scenario
  stat_function(fun = dnorm, args = list(mean = 1.5, sd = .2),  size = .75, aes(linetype = "Trend Mid Scenario")) +
#   geom_density(data = posteriors_mid_gam, aes(x = b_trend, linetype = "Trend Mid Scenario"), size = .75) +
# Trend High Scenario
  stat_function(fun = dnorm, args = list(mean = 2, sd = .1),  size = .75, aes(linetype = "Trend High Scenario")) +
#   geom_density(data = posteriors_high_gam, aes(x = b_trend, linetype = "Trend High Scenario"), size = .75) +  
   scale_linetype_manual(name="",  values=c("Trend Posterior" = 1, "Trend Low Scenario" = 2, "Trend Mid Scenario" = 3, "Trend High Scenario" = 4)) +
   theme_bw() + xlab("") +
   guides(linetype=guide_legend(ncol=3)) + ggtitle("Linear Trend") +  xlim(c(-5, 5)) + 
   theme(text=element_text(family="serif"), legend.position="bottom", plot.title = element_text(hjust = 0.5))

p_trend



p_ar1 <- ggplot() +
# AR1
   geom_density(data = posteriors_gam_comp, aes(x = `ar[1]`, linetype = "AR1 Posterior"), size = .75) + 
# AR1 Low Scenario
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),  size = .75, aes(linetype = "AR1 Low Scenario")) +
# AR1 Mid Scenario
  stat_function(fun = dnorm, args = list(mean = .5, sd = .25),  size = .75, aes(linetype = "AR1 Mid Scenario")) +
# AR1 High Scenario
  stat_function(fun = dnorm, args = list(mean = .8, sd = .1),  size = .75, aes(linetype = "AR1 High Scenario")) +
   scale_linetype_manual(name="",  values=c("AR1 Posterior" = 1, "AR1 Low Scenario" = 2, "AR1 Mid Scenario" = 3, "AR1 High Scenario" = 4)) +
   theme_bw() + xlab("") +
   guides(linetype=guide_legend(ncol=3)) + ggtitle("AR1") +  xlim(c(0, 1)) + 
   theme(text=element_text(family="serif"), legend.position="bottom", plot.title = element_text(hjust = 0.5))

p_ar1

p_sigma <- ggplot() +
# Sigma
   geom_density(data = posteriors_gam_comp, aes(x = sigma, linetype = "Error sd Posterior"), size = .75) + 
# Sigma Low-Mid-High Scenario
    stat_function(fun = dhalfcauchy, args = list(scale = 2), size = .75,  aes(linetype = "Error sd Low, Mid and High Scenario")) +
    scale_linetype_manual(name="",  values=c("Error sd Posterior" = 1, "Error sd Low, Mid and High Scenario" = 2)) +
   theme_bw() + xlab("") +
   guides(linetype=guide_legend(ncol=3)) + ggtitle("Error Standard Deviation") +  xlim(c(0, 2)) + 
   theme(text=element_text(family="serif"), legend.position="bottom", plot.title = element_text(hjust = 0.5))

p_sigma

p_sds <- ggplot() +
# Sds First Sum
   geom_density(data = posteriors_gam_comp, aes(x = sds_sfirst_sum_1, linetype = "Lambda sd First Sum Posterior"), size = .75) + 
# Sds Second Sum
   geom_density(data = posteriors_gam_comp, aes(x = sds_ssecond_sum_1, linetype = "Lambda sd Second Sum Posterior"), size = .75) + 
# Sds Trend First Sum
   geom_density(data = posteriors_gam_comp, aes(x = sds_strendfirst_sum_1, linetype = "Lambda sd Trend-First Sum Posterior"), size = .75) + 
# Sds Trend Second Sum
   geom_density(data = posteriors_gam_comp, aes(x = sds_strendsecond_sum_1, linetype = "Lambda sd Trend-Second Sum Posterior"), size = .75) + 
#  Low Scenario
    stat_function(fun = dhalfnorm, args = list(scale = 1), size = .75,  aes(linetype = "Lambda sd Low, Mid and High Scenario")) +
   scale_linetype_manual(name="",  values=c("Lambda sd First Sum Posterior" = 1, "Lambda sd Second Sum Posterior" = 1, "Lambda sd Trend-First Sum Posterior" = 1, "Lambda sd Trend-Second Sum Posterior" = 1, "Lambda sd Low, Mid and High Scenario" = 3)) +
   theme_bw() + xlab("") +
   guides(linetype=guide_legend(ncol=2)) + ggtitle("Lambda Standard Deviation") +  xlim(c(0, 1)) + 
   theme(text=element_text(family="serif"), legend.position="bottom", plot.title = element_text(hjust = 0.5))

p_sds


pdf(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/posterior.pdf", paper = "special", height = 9, width = 10, family='serif')
grid.arrange(p_intercept, p_trend, p_ar1, p_sigma, p_sds, ncol = 2)
dev.off()

@

\begin{figure}
\centering
\makebox{\includegraphics[scale=0.6]{posterior.pdf}}
\caption{\label{fig:posteriors vs priors}The plots compare the posterior distributions obtained from the AMI with autoregressive errors of order one for the period 1981-2013 (solid line) with the priors chosen to set the different forecast scenarios (dashed and dotted lines). From top to bottom and left to right the graphs show the distributions for the intercept, the linear trend's coefficient, the error autoregressive coefficient (AR1), the error's standard deviation and the standard deviation of the smoothing parameter.}
\end{figure}

\begin{table}[ht]  
\caption{\label{prior distribution 1}  Prior Distributions for the Scenario Analysis} 
\begin{center}
\fbox{%
\resizebox{.7\columnwidth}{0.08\textheight}{
  \begin{tabular}{@{\extracolsep{5pt}}lcccc} 
 & Historical Scenario & Middle Scenario & High Scenario \\  
  \cline{2-4} 
 \\
$\beta_0$                   & N(9;0.5)    & N(9;0.5)      & N(9;0.5)  \\
$\beta_1$                   & N(0;1)      & N(1.5;0.2)    & N(2;0.1)  \\
$\alpha$                    & N(0;1)      & N(0.5;0.25)   & N(0.8;0.1) \\
$\sigma_{\xi}$              & HC(0;2)     & HC(0;2)       & HC(0;2)  \\
$\sigma_{\lambda}$          & HC(0;2)     & HC(0;2)       & HC(0;2)  \\
  \end{tabular}}
} 
\end{center}
\end{table}



<<forecast, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=



##################################################
# FORECASTS
#################################################

prediction <- read.csv('/path/to/file/Prediction time series.csv', sep = ",", header = TRUE, stringsAsFactors = FALSE)
#New Variables
prediction$xlog <- log(prediction$x)

#Fourier
fourier <- as.data.table(fourier(ts(prediction$xlog, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12

prediction <- cbind(prediction, fourier)
prediction$Date <- as.Date(prediction$Date, by="1 mon")

set.seed(123456)

##################################################
# LINEAR TREND
##################################################

#################
# LOW VOLATILITY
#################
#Empirical prior

prior_l <- c(set_prior("normal(0,1)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(0,1)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
low_gam <- brm(xlog ~  trend + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_l,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)

pred <- as.data.frame(predict(low_gam, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vcl <- NA
prediction$pr_b_vcl_upr <- NA
prediction$pr_b_vcl_lwr <- NA

prediction[397:492,]$pr_b_vcl <- pred$Estimate
prediction[397:492,]$pr_b_vcl_upr <- pred$Q97.5
prediction[397:492,]$pr_b_vcl_lwr <- pred$Q2.5

plot_low_gam <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vcl), col='red') +
  geom_line(aes(y=pr_b_vcl_upr), col='blue') +
  geom_line(aes(y=pr_b_vcl_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("Historical Scenario Linear Trend")
plot_low_gam

#################
# MID VOLATILITY
#################

prior_m <- c(set_prior("normal(0.5,0.25)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(1.5,.2)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
mid_gam <- brm(xlog ~  trend  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12),  cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_m,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)

pred <- as.data.frame(predict(mid_gam, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vcm <- NA
prediction$pr_b_vcm_upr <- NA
prediction$pr_b_vcm_lwr <- NA

prediction[397:492,]$pr_b_vcm <- pred$Estimate
prediction[397:492,]$pr_b_vcm_upr <- pred$Q97.5
prediction[397:492,]$pr_b_vcm_lwr <- pred$Q2.5

plot_mid_gam <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vcm), col='red') +
  geom_line(aes(y=pr_b_vcm_upr), col='blue') +
  geom_line(aes(y=pr_b_vcm_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("Middle Scenario Linear Trend")
plot_mid_gam

#################
# HIGH VOLATILITY
#################

prior_h <- c(set_prior("normal(0.8,0.1)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(2,.1)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 



# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
high_gam <-  brm(xlog ~  trend  + s(first_sum, k=4)  + s(second_sum, k=4) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), 
                          cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_h,  data = prediction[1:396,], 
                          control = list(adapt_delta=0.99), cores=4)


pred <- as.data.frame(predict(high_gam, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vch <- NA
prediction$pr_b_vch_upr <- NA
prediction$pr_b_vch_lwr <- NA

prediction[397:492,]$pr_b_vch <- pred$Estimate
prediction[397:492,]$pr_b_vch_upr <- pred$Q97.5
prediction[397:492,]$pr_b_vch_lwr <- pred$Q2.5

plot_high_gam <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vch), col='red') +
  geom_line(aes(y=pr_b_vch_upr), col='blue') +
  geom_line(aes(y=pr_b_vch_lwr), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("High Scenario Linear Trend")
plot_high_gam



plots_fore <- grid.arrange(plot_low_gam, plot_mid_gam, plot_high_gam)

##################################################
# SEMIPARAMETRIC TREND s(trend, k=4)
##################################################

#################
# LOW VOLATILITY 
#################

prior_l <- c(set_prior("normal(0,1)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(0,1)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
low_gam_s <- brm(xlog ~  s(trend, k=4)  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_l,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)
pred <- as.data.frame(predict(low_gam_s, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vcl_s <- NA
prediction$pr_b_vcl_upr_s <- NA
prediction$pr_b_vcl_lwr_s <- NA

prediction[397:492,]$pr_b_vcl_s <- pred$Estimate
prediction[397:492,]$pr_b_vcl_upr_s <- pred$Q97.5
prediction[397:492,]$pr_b_vcl_lwr_s <- pred$Q2.5

plot_low_gam_s <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vcl_s), col='red') +
  geom_line(aes(y=pr_b_vcl_upr_s), col='blue') +
  geom_line(aes(y=pr_b_vcl_lwr_s), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("Historical Scenario Semiparametric Trend k=4")
plot_low_gam_s

#################
# MID VOLATILITY
#################

prior_m <- c(set_prior("normal(0.5,0.25)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(1.5,.2)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
mid_gam_s <- brm(xlog ~  s(trend, k=4)  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_m,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)
pred <- as.data.frame(predict(mid_gam_s, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vcm_s <- NA
prediction$pr_b_vcm_upr_s <- NA
prediction$pr_b_vcm_lwr_s <- NA

prediction[397:492,]$pr_b_vcm_s <- pred$Estimate
prediction[397:492,]$pr_b_vcm_upr_s <- pred$Q97.5
prediction[397:492,]$pr_b_vcm_lwr_s <- pred$Q2.5

plot_mid_gam_s <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vcm_s), col='red') +
  geom_line(aes(y=pr_b_vcm_upr_s), col='blue') +
  geom_line(aes(y=pr_b_vcm_lwr_s), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("Middle Scenario Semiparametric Trend k=4")
  plot_mid_gam_s


#################
# HIGH VOLATILITY
#################


prior_h <- c(set_prior("normal(0.8,0.1)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(2,.1)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
high_gam_s <- brm(xlog ~  s(trend, k=4)  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_h,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)
pred <- as.data.frame(predict(high_gam_s, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vch_s <- NA
prediction$pr_b_vch_upr_s <- NA
prediction$pr_b_vch_lwr_s <- NA

prediction[397:492,]$pr_b_vch_s <- pred$Estimate
prediction[397:492,]$pr_b_vch_upr_s <- pred$Q97.5
prediction[397:492,]$pr_b_vch_lwr_s <- pred$Q2.5

plot_high_gam_s <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vch_s), col='red') +
  geom_line(aes(y=pr_b_vch_upr_s), col='blue') +
  geom_line(aes(y=pr_b_vch_lwr_s), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("High Scenario Semiparametric Trend k=4")
plot_high_gam_s

plots_fore_s <- grid.arrange(plot_low_gam_s, plot_mid_gam_s, plot_high_gam_s, ncol=1)


##################################################
# SEMIPARAMETRIC TREND s(trend)
##################################################

#################
# LOW VOLATILITY
#################

prior_l <- c(set_prior("normal(0,1)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(0,1)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
low_gam_s1 <- brm(xlog ~  s(trend, k=6)  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_l,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)
pred <- as.data.frame(predict(low_gam_s1, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vcl_s1 <- NA
prediction$pr_b_vcl_upr_s1 <- NA
prediction$pr_b_vcl_lwr_s1 <- NA

prediction[397:492,]$pr_b_vcl_s1 <- pred$Estimate
prediction[397:492,]$pr_b_vcl_upr_s1 <- pred$Q97.5
prediction[397:492,]$pr_b_vcl_lwr_s1 <- pred$Q2.5

plot_low_gam_s1 <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vcl_s1), col='red') +
  geom_line(aes(y=pr_b_vcl_upr_s1), col='blue') +
  geom_line(aes(y=pr_b_vcl_lwr_s1), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("Historical Scenario Semiparametric Trend k=6")
plot_low_gam_s1

#################
# MID VOLATILITY
#################
prior_m <- c(set_prior("normal(0.5,0.25)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(1.5,.2)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
mid_gam_s1 <- brm(xlog ~  s(trend, k=6)  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_m,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)
pred <- as.data.frame(predict(mid_gam_s1, newdata = prediction[397:492,], type = 'response'))

prediction$pr_b_vcm_s1 <- NA
prediction$pr_b_vcm_upr_s1 <- NA
prediction$pr_b_vcm_lwr_s1 <- NA

prediction[397:492,]$pr_b_vcm_s1 <- pred$Estimate
prediction[397:492,]$pr_b_vcm_upr_s1 <- pred$Q97.5
prediction[397:492,]$pr_b_vcm_lwr_s1 <- pred$Q2.5

plot_mid_gam_s1 <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vcm_s1), col='red') +
  geom_line(aes(y=pr_b_vcm_upr_s1), col='blue') +
  geom_line(aes(y=pr_b_vcm_lwr_s1), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("Middle Scenario Semiparametric Trend k=6")
plot_mid_gam_s1


#################
# NORMAL HIGH
#################
#################
# HIGH VOLATILITY
#################

prior_h <- c(set_prior("normal(0.8,0.1)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(2,.1)", class = "b",
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
high_gam_s1 <- brm(xlog ~  s(trend, k=6)  + s(first_sum, k=6)  + s(second_sum, k=6)  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), cor_arma(formula = ~1, p = 1), iter=1000, family = gaussian(), prior = prior_h,  data = prediction[1:396,], control = list(adapt_delta=0.99), cores=4)
pred <- as.data.frame(predict(high_gam_s1, newdata = prediction[397:492,], type = 'response'))

gam1 <- gam(xlog ~  s(trend)  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12),  data = prediction[1:396,])
coefficients(gam1)

prediction$pr_b_vch_s1 <- NA
prediction$pr_b_vch_upr_s1 <- NA
prediction$pr_b_vch_lwr_s1 <- NA

prediction[397:492,]$pr_b_vch_s1 <- pred$Estimate
prediction[397:492,]$pr_b_vch_upr_s1 <- pred$Q97.5
prediction[397:492,]$pr_b_vch_lwr_s1 <- pred$Q2.5

plot_high_gam_s1 <- ggplot(data=prediction[which(prediction$Year > 2000),], aes(x=Date)) +
  geom_line(aes(y=xlog)) +
  geom_line(aes(y=pr_b_vch_s1), col='red') +
  geom_line(aes(y=pr_b_vch_upr_s1), col='blue') +
  geom_line(aes(y=pr_b_vch_lwr_s1), col='blue') +
  theme_bw() +
  theme(text=element_text(family="serif"), plot.title = element_text(hjust = 0.5, size=8)) +
  xlab('Year') + ylab("log(Immigration)") + ylim(c(7.5, 11.5)) +
  ggtitle("High Scenario Semiparametric Trend k=6")
plot_high_gam_s1

plots_fore_s1 <- grid.arrange(plot_low_gam_s1, plot_mid_gam_s1, plot_high_gam_s1, ncol=1)


mean(prediction$pr_b_vcl, na.rm=T) # 9.414833
mean(prediction$pr_b_vcm, na.rm=T) # 9.414604
mean(prediction$pr_b_vch, na.rm=T) # 9.41505

mean(prediction$pr_b_vcl_s, na.rm=T) # 9.503702 s(trend, k=4)
mean(prediction$pr_b_vcm_s, na.rm=T) # 9.59956 s(trend, k=4)
mean(prediction$pr_b_vch_s, na.rm=T) # 9.941904 s(trend, k=4)

mean(prediction$pr_b_vcl_s1, na.rm=T) # 8.898753 s(trend, k=6)
mean(prediction$pr_b_vcm_s1, na.rm=T) # 9.175398 s(trend, k=6)
mean(prediction$pr_b_vch_s1, na.rm=T) # 9.866497 s(trend, k=6)

plots_fore_tot <- grid.arrange(plot_low_gam, plot_mid_gam, plot_high_gam, 
                              plot_low_gam_s, plot_mid_gam_s, plot_high_gam_s,
                               plot_low_gam_s1, plot_mid_gam_s1, plot_high_gam_s1, ncol=3)

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Forecast_results_new.pdf", plots_fore_tot,  width = 25, height = 20, units = "cm")

prediction_y1 <- aggregate(cbind(x, xlog) ~ Year, data = prediction[1:396,], FUN=sum)
prediction_y2 <- aggregate(cbind(exp(pr_b_vcl), exp(pr_b_vcm), exp(pr_b_vch), exp(pr_b_vcl_s), exp(pr_b_vcm_s), exp(pr_b_vch_s), exp(pr_b_vcl_s1), exp(pr_b_vcm_s1), exp(pr_b_vch_s1)) ~ Year, data = prediction[397:492,], FUN=sum)
colnames(prediction_y2) <- c('Year', 'pr_b_vcl', 'pr_b_vcm', 'pr_b_vch', 'pr_b_vcl_s', 'pr_b_vcm_s', 'pr_b_vch_s', 'pr_b_vcl_s1', 'pr_b_vcm_s1', 'pr_b_vch_s1')

median(prediction_y1$x) # 99070
median(prediction_y1[which(prediction_y1$Year > 2003),]$x) # 126842
median(prediction_y1[which(prediction_y1$Year > 2007),]$x) # 133372.5

median_pr_b_vcl <- median(prediction_y2$pr_b_vcl) # 147646.4
median_pr_b_vcm <- median(prediction_y2$pr_b_vcm) # 148313.4
median_pr_b_vch <- median(prediction_y2$pr_b_vch) # 151872.7
lin_tr <- (median_pr_b_vcl + median_pr_b_vcm + median_pr_b_vch) / 3 # 149277.5
(lin_tr  - 99070) / 99070 # 0.5067881

median_pr_b_vcl_s <- median(prediction_y2$pr_b_vcl_s) #  166943.2
median_pr_b_vcm_s <- median(prediction_y2$pr_b_vcm_s) # 185752.9
median_pr_b_vch_s <- median(prediction_y2$pr_b_vch_s) # 265757.7
k_4 <- (median_pr_b_vcl_s + median_pr_b_vcm_s + median_pr_b_vch_s) / 3 # 206151.3
(k_4  - 99070) / 99070 # 1.080865 

median_pr_b_vcl_s1 <- median(prediction_y2$pr_b_vcl_s1) # 82701.26
median_pr_b_vcm_s1 <- median(prediction_y2$pr_b_vcm_s1) # 109584.1
median_pr_b_vch_s1 <- median(prediction_y2$pr_b_vch_s1) # 233786.3
k_6 <- (median_pr_b_vcl_s1 + median_pr_b_vcm_s1 + median_pr_b_vch_s1) / 3 # 142023.9
(k_6  - 99070) / 99070 # 0.4335709 

#Stable
(median_pr_b_vcl_s1 - median_pr_b_vcl) / median_pr_b_vcl # -0.4398694
#Middle
(median_pr_b_vcl_s1 - median_pr_b_vcm) / median_pr_b_vcm # -0.4423885



# Stable average increase
(median_pr_b_vcl  - 99070) / 99070 # 0.4903238
(median_pr_b_vcl_s - 99070) / 99070 # 0.6851034
(median_pr_b_vcl_s1 - 99070) / 99070 # -0.165224
# Middle average increase
(median_pr_b_vcm  - 99070) / 99070 # 0.4970566
(median_pr_b_vcm_s - 99070) / 99070 # 0.8749658
(median_pr_b_vcm_s1 - 99070) / 99070 # 0.1061277
# High average increase
(median_pr_b_vch  - 99070) / 99070 # 0.5329838
(median_pr_b_vch_s - 99070) / 99070 # 1.682525
(median_pr_b_vch_s1 - 99070) / 99070 # 1.359809

@

For each scenario we model the global trend in three different ways. The choice stems from the reasoning already presented in the previous sections, where we outlined the potential problems raising from a smooth trend with a considerable number of knots for long term forecasts. Thus, the three models include as alternatives a linear trend ($\beta_{1}\text{trend}_{t}$) and two smooth trends with respectively four, $f_{1}(\text{trend}_{t}, \text{k}=4)$, and six knots, $f_{1}(\text{trend}_{t}, \text{k}=6)$. The difference in the results is more visible for the trend fitted on six knots, especially in the case of historical and middle scenarios, where the global trend is down-warding. In general when the trend is nonparametrically computed with k=6, the average future immigration is approximately 40\% lower than in the linear case for the historical and the middle scenario.   \\* 
In the historical scenario, the median number of immigrants rises from its historic value of 99,070 people per year to 147,646 for the linear trend, to 206,151 for the smooth trend with k=4 and shrinks to 82,701 for the smooth trend with k=6, showing respectively an increase of 49\% and 68\% and a decrease of 17\%. In the middle scenario the median number of immigrants becomes 148,313 for the linear trend, 185,752 for the smoothing trend with k=4 and 109,584 for k=6 corresponding respectively to a 50\%, 87\% and 11\% increase. Finally, the high scenario finds a growth rate of 53\% for the linear trend, of 168\% for the smoothing trend with k=4 and of 136\% for k=6. Figure \ref{fig:forecast scenarios} visualizes the results. Note that all the growth rates would be down-sized if we would look at the median immigration of the last 10 and 5 years rather than the one from 1981 till 2013. \\*

\begin{figure}
\centering
\makebox{\includegraphics[scale=0.6]{Forecast_results_new.pdf}}
\caption{\label{fig:forecast scenarios}Forecasts 2013-2021 with $95\%$ prediction credible interval from the posterior predictive distribution for three different priors.}
\end{figure}

As a general remark from our forecast exercise, practitioners should be careful in treating the global trend either parametrically or nonparametrically, as well as, in the degree of volatility conveyed by the priors. 
In general a large number of knots requires more coefficients (one for each knots interval) and, consequently, a loss in degrees of freedom. Said differently, including an excessive number of knots reduces the degrees of freedom available to estimate the parameters' variability, potentially having a negative impact on the forecast's quality. 
Furthermore, since the paper stresses the benefits of modelling semiparametrically the trend-seasonal interaction, estimating the trend in  parsimonious ways allows to devote a larger number of degrees of freedom to fit the interaction without worsening the prediction accuracy.


\section{Longitudinal Analysis of Age Categorized Data} \label{disaggregated results}
\subsection{Model Validation} \label{model_disaggregation}

To illustrate the adaptability of our model to a disaggregated problem we employ longitudinal data categorizing the monthly number of arrivals by age. Further distinctions by gender or nationality can, at any rate, be implemented, but they are not considered here. \\
Splitting our data into different ages allows us to check the persistence of non-linearity and trend-cycle interactions. We add  $Age$ as an explanatory variable to the models estimated for the time series. The latter is introduced as a parametric term, $\beta_4 Age_{age,t}$, in LMNI and LMI, as a smooth function $ f_{4}(Age_{age,t})$ in AMNI and AMI \citep{dodd2018smoothing}, while for the the nonparametric model (NPM), we add it to the main smoothing function $f$. To estimate the Bayesian models we adopt the same priors as in the aggregate exercise, with in addition $\beta_4 \sim N(0,0.5)$. 


<<computation_age, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=

ts_a <- read.csv("/path/to/file/All Age time series.csv")

#Fourier
fourier <- as.data.table(fourier(ts(ts_a$xlog, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12

ts_a <- cbind(ts_a, fourier)
ts_a$Date <- as.Date(ts_a$Date, by="1 mon")
ts_a <- ts_a[which(ts_a$Age < 97),] 
train_age <- ts_a[which(ts_a$Year < 2006),]
test_age <- ts_a[which(ts_a$Year > 2005),]

# Set Prior
prior <- c(set_prior("normal(9,.5)", class = "Intercept"),
           set_prior("normal(0,.5)", class = "b"),
           set_prior("cauchy(0,2)", class = "sigma"))

prior1 <- c(set_prior("normal(9,.5)", class = "Intercept"),
            set_prior("normal(0,.5)", class = "b"),
            set_prior("cauchy(0,2)", class = "sigma"),
            set_prior("cauchy(0,2)", class = "sds"))

set.seed(123456)

########################################################
# FITTING SHORT RUN ORDER 2
########################################################
# LINEAR MODEL NO INTERACTION 
lm_no <- brm(xlog ~ trend + first_sum + second_sum + Age, iter=1000, family = gaussian(), prior = prior, data = train_age, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

res_lm_no  <- residuals(lm_no, incl_autocor = TRUE)

fit <- as.data.frame(fitted(lm_no))
train_age$fit_lm_no <- fit$Estimate
pred_lm_no <- as.data.frame(predict(lm_no, newdata = test_age, type = 'response', allow_new_levels = TRUE))
test_age$pr_lm_no <- pred_lm_no$Estimate
test_age$pr_lm_no_upr <- pred_lm_no$Q97.5
test_age$pr_lm_no_lwr <- pred_lm_no$Q2.5

#90%
pred_lm_no_90 <- as.data.frame(predict(lm_no, newdata = test_age, type = 'response', probs = c(0.05, 0.95)))
test_age$pr_lm_no_90lwr <- pred_lm_no_90$Q5
test_age$pr_lm_no_90upr <- pred_lm_no_90$Q95
test_age$lm_no_CI95_out <- ifelse(test_age$xlog < test_age$pr_lm_no_lwr | test_age$xlog > test_age$pr_lm_no_upr, 1, 0) 
100 - mean(test_age$lm_no_CI95_out)*100 # 95.20334 
test_age$lm_no_CI90_out <- ifelse(test_age$xlog < test_age$pr_lm_no_90lwr | test_age$xlog > test_age$pr_lm_no_90upr, 1, 0)
100 - mean(test_age$lm_no_CI90_out)*100 #  81.103
rmse(test_age$pr_lm_no, test_age$xlog) #  1.165527
mape(test_age$pr_lm_no, test_age$xlog) #  0.3144452

# LINEAR MODEL WITH INTERACTION 
lm <- brm(xlog ~ trend * first_sum + trend*second_sum + Age, iter=1000, family = gaussian(), prior = prior, data = train_age, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(lm))
train_age$fit_lm <- fit$Estimate
pred_lm <- as.data.frame(predict(lm, newdata = test_age, type = 'response'))
test_age$pr_lm <- pred_lm$Estimate
test_age$pr_lm_upr <- pred_lm$Q97.5
test_age$pr_lm_lwr <- pred_lm$Q2.5

#90%
pred_lm_90 <- as.data.frame(predict(lm, newdata = test_age, type = 'response', probs = c(0.05, 0.95)))
test_age$pr_lm_90lwr <- pred_lm_90$Q5
test_age$pr_lm_90upr <- pred_lm_90$Q95
test_age$lm_CI95_out <- ifelse(test_age$xlog < test_age$pr_lm_lwr | test_age$xlog > test_age$pr_lm_upr, 1, 0) 
100 - mean(test_age$lm_CI95_out)*100 #   95.20334
test_age$lm_CI90_out <- ifelse(test_age$xlog < test_age$pr_lm_90lwr | test_age$xlog > test_age$pr_lm_90upr, 1, 0)
100 - mean(test_age$lm_CI90_out)*100 #  81.103
rmse(test_age$pr_lm, test_age$xlog) # 1.165527
mape(test_age$pr_lm, test_age$xlog) # 0.3143187

# SEMIPARAMETRIC MODEL NO INTERACTION 
gam_no <- brm(xlog ~ s(trend) + s(first_sum, k=6) + s(second_sum, k=6) + s(Age), iter=1000, family = gaussian(), prior = prior1, data = train_age, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_no))
train_age$fit_gam_no <- fit$Estimate
pred_gam_no <- as.data.frame(predict(gam_no, newdata = test_age, type = 'response'))
test_age$pr_gam_no <- pred_gam_no$Estimate
test_age$pr_gam_no_upr <- pred_gam_no$Q97.5
test_age$pr_gam_no_lwr <- pred_gam_no$Q2.5
#90%
pred_gam_no_90 <- as.data.frame(predict(gam_no, newdata = test_age, type = 'response', probs = c(0.05, 0.95)))
test_age$pr_gam_no_90lwr <- pred_gam_no_90$Q5
test_age$pr_gam_no_90upr <- pred_gam_no_90$Q95
test_age$gam_no_CI95_out <- ifelse(test_age$xlog < test_age$pr_gam_no_lwr | test_age$xlog > test_age$pr_gam_no_upr, 1, 0) 
100 - mean(test_age$gam_no_CI95_out)*100 #   89.01634 \%
test_age$gam_no_CI90_out <- ifelse(test_age$xlog < test_age$pr_gam_no_90lwr | test_age$xlog > test_age$pr_gam_no_90upr, 1, 0)
100 - mean(test_age$gam_no_CI90_out)*100 #  79.50411\% 

rmse(test_age$pr_gam_no, test_age$xlog) # 0.6616615
mape(test_age$pr_gam_no, test_age$xlog) # 0.2447627


# SEMIPARAMETRIC MODEL WITH INTERACTION 
gam <- brm(xlog ~ s(trend, first_sum, second_sum, Age), iter=1000, family = gaussian(), prior = prior1, data = train_age, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam))
train_age$fit_gam <- fit$Estimate
pred_gam <- as.data.frame(predict(gam, newdata = test_age, type = 'response'))
test_age$pr_gam <- pred_gam$Estimate
test_age$pr_gam_upr <- pred_gam$Q97.5
test_age$pr_gam_lwr <- pred_gam$Q2.5
#90%
pred_gam_90 <- as.data.frame(predict(gam, newdata = test_age, type = 'response', probs = c(0.05, 0.95)))
test_age$pr_gam_90lwr <- pred_gam_90$Q5
test_age$pr_gam_90upr <- pred_gam_90$Q95
test_age$gam_CI95_out <- ifelse(test_age$xlog < test_age$pr_gam_lwr | test_age$xlog > test_age$pr_gam_upr, 1, 0) 
100 - mean(test_age$gam_CI95_out)*100 #   90.59205 \%
test_age$gam_CI90_out <- ifelse(test_age$xlog < test_age$pr_gam_90lwr | test_age$xlog > test_age$pr_gam_90upr, 1, 0)
100 - mean(test_age$gam_CI90_out)*100 # 82.92203  \% 

rmse(test_age$pr_gam, test_age$xlog) # 0.5466413
mape(test_age$pr_gam, test_age$xlog) # 0.33032 


# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
gam_comp <- brm(xlog ~  s(trend)  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12) + s(Age), iter=1000, family = gaussian(), prior = prior1, data = train_age, control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
fit <- as.data.frame(fitted(gam_comp))
train_age$fit_gam_comp <- fit$Estimate
pred_gam_comp <- as.data.frame(predict(gam_comp, newdata = test_age, type = 'response'))
test_age$pr_gam_comp <- pred_gam_comp$Estimate
test_age$pr_gam_comp_upr <- pred_gam_comp$Q97.5
test_age$pr_gam_comp_lwr <- pred_gam_comp$Q2.5
#90%
pred_gam_comp_90 <- as.data.frame(predict(gam_comp, newdata = test_age, type = 'response', probs = c(0.05, 0.95)))
test_age$pr_gam_comp_90lwr <- pred_gam_comp_90$Q5
test_age$pr_gam_comp_90upr <- pred_gam_comp_90$Q95
test_age$gam_comp_CI95_out <- ifelse(test_age$xlog < test_age$pr_gam_comp_lwr | test_age$xlog > test_age$pr_gam_comp_upr, 1, 0) 
100 - mean(test_age$gam_comp_CI95_out)*100 #  86.83814  \%
test_age$gam_comp_CI90_out <- ifelse(test_age$xlog < test_age$pr_gam_comp_90lwr | test_age$xlog > test_age$pr_gam_comp_90upr, 1, 0)
100 - mean(test_age$gam_comp_CI90_out)*100 #  77.62716 \% 

rmse(test_age$pr_gam_comp, test_age$xlog) # 0.6339528
mape(test_age$pr_gam_comp, test_age$xlog) # 0.2347434



@

The results in Table \ref{RMSFE_age}  show an increase in uncertainty produced by the disaggregation. On average the RMSFE increases by 130\% and the MAPE by 770\%.  The improvements given by the use of semiparametric and nonparametric models are reinforced, dropping the RMSFE from 1.16 (LMNI, LMI) to 0.66 (AMNI, AMI) and to 0.54 (NPM). However, the relative forecast accuracy measure, the mean absolute percentage error, portrays a different picture. The AMNI and the AMI achieve the most precise predictions with a MAPE of 0.24, followed by the LMNI and the LMI with a MAPE of 0.31 and lastly by the NPM with a MAPE of 0.33. The difference reflects the specific peculiarities of the absolute (RMSFE) vs the relative (MAPE) accuracy measures. In our case the NPM is doing better than the alternatives in minimizing the big error generated by the predictions of outlier observations, i.e. low RMSFE, but it produces a poorer performance, on average, when predicting smaller values, i.e. high MAPE. On the other hand, AMNI and AMI minimize the errors between, let say 0.11 and 0.12 rater than the ones between 0.81 and 0.82. In light of such considerations, the semiparametric models seem a more cautious choice over the nonparametric one. \\
In general, the disaggregation, while it does not generate any significant linearization, confirming the better performance of the semiparametric models, it minimizes the importance of the trend-seasonal interaction, which is reflected in the absence of a significant difference between the models with and without it. 

\begin{table}
\caption{\label{RMSFE_age} 2nd order Fourier models on Swiss Immigration Disaggregated Data: Root Mean Square Forecast Error (RMSFE) and Mean Average Percentage Error (MAPE) and prediction interval coverage}
\begin{center}
\fbox{%
\resizebox{.5\columnwidth}{0.08\textheight}{
  \begin{tabular}{@{\extracolsep{5pt}}lcccc} 
  \\[-1.8ex]\hline 
  \hline \\[-1.8ex] 
&  \multicolumn{1}{c}{RMSFE}&\multicolumn{1}{c}{MAPE} &\multicolumn{1}{c}{95\%}&\multicolumn{1}{c}{90\%} \\
  \hline \\[-1.8ex] 
LMNI           & 1.165 & 0.314 &  95  & 81      \\  % 2nd
AMNI           & 0.662 & 0.245 &  89  & 79.5          \\ % 2nd
LMI            & 1.165 & 0.314 &  95  & 81     \\ %2nd
AMI            & 0.662 & 0.246 &  90  & 79       \\ % 2nd
NPM            & 0.547 & 0.330 &  91  & 83       \\ % 2nd

  \hline 
  \hline \\[-1.8ex] 
  \end{tabular}}
} 
\end{center}
\end{table}

\subsection{Forecast Exercise}\label{disaggregated forecast}

The previous section validates the additive models as the preferable options to predict future immigration flows by age. This section presents a forecast exercise on the Swiss data for the period 2014-2021, which replicates the one in section \ref{forecast}, for the longitudinal case. Even though the difference between the AMNI and the AMI is not particularly relevant, we use the AMI, not only to keep a certain degree of consistency with section \ref{forecast}, but also because the AMI has a faster rate of convergence. Using the same priors as in the time series analysis, see Table \ref{prior distribution 1}, we check the performances of different trends: linear, smooth with three (k=4) and six (k=6) knots. Figure \ref{fig:trend comparison} reports the results under the low volatility scenario averaged over all the forecast periods. While the linear and the smooth trend with three knots have a very similar behaviour, the smoothest trend suffers from the same down-ward trend as in the aggregate case portrayed in Figure \ref{fig:prediction}. Due to the non significant difference between the blue (linear) and the red (k=4) line, we use the former to produce the disaggregated forecast scenario analysis. 
<<trend comparison, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=
future_age <- read.csv('/path/to/file/Prediction Age Time Series.csv')
future_age$Date <- as.Date(future_age$Date, by="1 mon")
fourier <- as.data.table(fourier(ts(future_age$xlog, frequency=12), K=2))
colnames(fourier) <- c("S1_12", "C1_12", "S2_12", "C2_12")
fourier$four_sum <- rowSums(fourier)
fourier$first_sum <- fourier$S1_12 + fourier$C1_12
fourier$second_sum <- fourier$S2_12 + fourier$C2_12

set.seed(123456)

#################
# LOW VOLATILITY
################
#Empirical prior
prior_l <- c(set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(0,1)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
low_gam <- brm(xlog ~  trend + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12) + s(trend, second_sum, k=12)  + s(Age), iter=1000, family = gaussian(), prior = prior_l,  data = future_age[which(future_age$Year < 2014),], control = list(adapt_delta=0.99, max_treedepth=15), cores=4)

pred <- as.data.frame(predict(low_gam, newdata = future_age[which(future_age$Year >= 2014),], type = 'response'))
future_age$pr_b_vcl <- NA
future_age[which(future_age$Year >= 2014),]$pr_b_vcl <- exp(pred$Estimate)
# Aggregate over Months
agg_pred_lm_no <- aggregate(pr_b_vcl ~ Year + Age, data = future_age[which(future_age$Year >= 2014),], FUN = sum)
# Plot
plot_lm_no <- ggplot(data = agg_pred_lm_no, aes(y=pr_b_vcl, x=Age, colour=as.factor(Year))) +
  geom_line()
plot_lm_no
# Average data over years
ave_pred_lm_no <- aggregate(pr_b_vcl ~ Age, data = agg_pred_lm_no, FUN = mean)
ave_pred_lm_no$Type <- "linear"


# trend, k=4
low_gam_s <- brm(xlog ~  s(trend, k=4) + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12)  + s(Age), iter=1000, family = gaussian(), prior = prior_l,  data = future_age[which(future_age$Year < 2014),], control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
pred_s <- as.data.frame(predict(low_gam_s, newdata = future_age[which(future_age$Year >= 2014),], type = 'response'))
future_age$pr_b_vcl <- NA
future_age[which(future_age$Year >= 2014),]$pr_b_vcl <- exp(pred_s$Estimate)
# Average data over months
agg_pred_lm_no_s <- aggregate(pr_b_vcl ~ Year + Age, data = future_age[which(future_age$Year >= 2014),], FUN = sum)
agg_pred_lm_no_s$pr_b_vcl_log <- log(agg_pred_lm_no_s$pr_b_vcl)
# Average data over years
ave_pred_lm_no_s <- aggregate(pr_b_vcl ~ Age, data = agg_pred_lm_no_s, FUN = mean)
ave_pred_lm_no_s$Type <- "k=4"

# trend, k=6
low_gam_s1 <- brm(xlog ~  s(trend, k=6) + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12)  + s(Age), iter=1000, family = gaussian(), prior = prior_l,  data = future_age[which(future_age$Year < 2014),], control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
pred_s1 <- as.data.frame(predict(low_gam_s1, newdata = future_age[which(future_age$Year >= 2014),], type = 'response'))
future_age$pr_b_vcl <- NA
future_age[which(future_age$Year >= 2014),]$pr_b_vcl <- exp(pred_s1$Estimate)
# Average data over months
agg_pred_lm_no_s1 <- aggregate(pr_b_vcl ~ Year + Age, data = future_age[which(future_age$Year >= 2014),], FUN = sum)
agg_pred_lm_no_s1$pr_b_vcl_log <- log(agg_pred_lm_no_s1$pr_b_vcl)
# Plot
plot_lm_no_s1 <- ggplot(data = agg_pred_lm_no_s1, aes(y=pr_b_vcl, x=Age, colour=as.factor(Year))) +
  geom_line()
plot_lm_no_s1
# Average data over years
ave_pred_lm_no_s1 <- aggregate(pr_b_vcl ~ Age, data = agg_pred_lm_no_s1, FUN = mean)
ave_pred_lm_no_s1$Type <- "k=6"


#Plot the differences in trend
ave_data <- rbind(ave_pred_lm_no, ave_pred_lm_no_s, ave_pred_lm_no_s1)

plot_trend <- ggplot(data = ave_data, aes(x=Age, y=pr_b_vcl, colour = Type)) +
  geom_line() +
  theme_bw() +
  theme(text=element_text(family="serif"), legend.position = 'bottom') +
  xlab('Age') + ylab("Immigration") 
plot_trend

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Plot_trend_dis.pdf", plot_trend,  width = 10, height = 6, units = "cm")


@
\begin{figure}
\centering
\makebox{\includegraphics[scale=1]{Plot_trend_dis.pdf}}
\caption{\label{fig:trend comparison}Effect of a different trend specification on the predictions of age-specific immigration forecast with on the y-axis the average yearly number of immigrants for the predicted period (2014-2021) and on the x-axis the age. The blue line denotes the results obtained with a linear trend, the red one with a smooth trend with 3 knots (k=4) and the green one with a smooth trend with 6 knots (k=6).}
\end{figure}

The results are depicted in Figure \ref{fig:forecast age}. In all the three cases, the expected amount of immigration by age resembles a normal distribution centered around 33, with a third moment bigger than zero. All the scenarios roughly maintain the same immigrant population age structure as the historical data. Nevertheless, as volatility increases, the degree of smoothness decreases. The same is true as the forecast horizon augments. \\
A final remark that needs to be made when considering disaggregated forecasts is how to manage the "pooling-back" when the final interest is to know the future of Swiss migration as a whole. In fact, stacking the average forecasts by age implies also stacking the credibility intervals which might be difficult to handle and risk to degenerate in an uninformative explosion of uncertainty. In light of the results, while disaggregation is always a possibility, the age pooling seems a safer choice, which guarantees forecast accuracy, as well as stability.


<<forecast age, cache=TRUE, echo=FALSE, eval=FALSE, include=FALSE>>=

future_age <- read.csv('/path/to/file/Prediction Age Time Series')
future_age$Date <- as.Date(future_age$Date, by="1 mon")

#Average Age
future_age_new <- aggregate(x ~ Year + Age, data = future_age, FUN = sum)
future_age_new <- future_age_new %>%
  group_by(Year) %>%
  mutate(cumsum = cumsum(x))
future_age_new <- future_age_new %>%
  group_by(Age) %>%
  summarise(cumsum = sum(cumsum))

mean(future_age_new$cumsum) # 2,342,339 corresponding age: 33
summary(future_age_new$cumsum)
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  14389 1344791 3023500 2342339 3264485 3291483
  
set.seed(123456)

#################
# LOW VOLATILITY
#################
#Empirical prior
prior_l <- c(set_prior("normal(0,1)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(0,1)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
low_gam <- brm(xlog ~ + s(first_sum, k=6)  + s(second_sum, k=6)+ s(trend, first_sum, k=12)  + s(trend, second_sum, k=12) + s(Age), iter=1000, family = gaussian(), prior = prior_l,  data = future_age[which(future_age$Year < 2014),], control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
pred <- as.data.frame(predict(low_gam, newdata = future_age[which(future_age$Year >= 2014),], type = 'response'))
colnames(pred) <- c('Estimate')
future_age$pr_b_vcl <- NA
future_age[which(future_age$Year >= 2014),]$pr_b_vcl <- exp(pred$Estimate)
# Aggregate over Months
agg_pred_gam_l <- aggregate(pr_b_vcl ~ Year + Age, data = future_age[which(future_age$Year >= 2014),], FUN = sum)
agg_pred_gam_l$Type <- "Low Volatility Scenario"
colnames(agg_pred_gam_l) <- c("Year","Age","pr_b_vc","Type")
# Plot
plot_gam_l <- ggplot(data = agg_pred_gam_l, aes(y=pr_b_vc, x=Age, colour=as.factor(Year))) +
  geom_line() +
   theme_bw() +
  theme(text=element_text(family="serif"), legend.position = 'none',plot.title = element_text(hjust = 0.5)) +
  ggtitle("Middle-Low Scenario linear trend") +
  xlab('Age') + ylab("Immigration") 
plot_gam_l


#################
# MID VOLATILITY
#################

prior_m <- c(set_prior("normal(0.5,0.25)", class = "ar", lb = -1, ub = 1),
             set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(1.5,.2)", class = "b"),
             set_prior("cauchy(0,2)", class = "sigma"),
             set_prior("cauchy(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
mid_gam <- brm(xlog ~  trend  + s(first_sum, k=6)  + s(second_sum, k=6) + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=1000, family = gaussian(), prior = prior_m,  data = future_age[which(future_age$Year < 2014),], control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
pred <- as.data.frame(predict(mid_gam, newdata = future_age[which(future_age$Year >= 2014),], type = 'response'))
colnames(pred) <- c('Estimate')
future_age$pr_b_vcm <- NA
future_age[which(future_age$Year >= 2014),]$pr_b_vcm <- exp(pred$Estimate)
# Aggregate over Months
agg_pred_gam_m <- aggregate(pr_b_vcm ~ Year + Age, data = future_age[which(future_age$Year >= 2014),], FUN = sum)
agg_pred_gam_m$Type <- "Middle Volatility Scenario"
colnames(agg_pred_gam_m) <- c("Year","Age","pr_b_vc","Type")
# Plot
plot_gam_m <- ggplot(data = agg_pred_gam_m, aes(y=pr_b_vc, x=Age, colour=as.factor(Year))) +
  geom_line() +
    theme_bw() +
  theme(text=element_text(family="serif"), legend.position = 'none', plot.title = element_text(hjust = 0.5)) +
  ggtitle("Middle Scenario linear trend") +
  xlab('Age') + ylab("Immigration") 
plot_gam_m

#################
# HIGH VOLATILITY
#################

prior_h <- c(set_prior("normal(9,0.5)", class = "Intercept"),
             set_prior("normal(2,0.1)", class = "b"),
             set_prior("normal(0,2)", class = "sigma"),
             set_prior("normal(0,2)", class = "sds")) 

# SEMIPARAMETRIC MODEL WITH COMPLEX INTERACTION 
high_gam <- brm(xlog ~  trend + s(first_sum, k=6)  + s(second_sum, k=6)  + s(trend, first_sum, k=12)  + s(trend, second_sum, k=12), iter=1000, family = gaussian(), prior = prior_h,  data = future_age[which(future_age$Year < 2014),], control = list(adapt_delta=0.99, max_treedepth=15), cores=4)
pred <- as.data.frame(predict(high_gam, newdata = future_age[which(future_age$Year >= 2014),], type = 'response'))
colnames(pred) <- c('Estimate')
future_age$pr_b_vch <- NA
future_age[which(future_age$Year >= 2014),]$pr_b_vch <- exp(pred$Estimate)
# Aggregate over Months
agg_pred_gam_h <- aggregate(pr_b_vch ~ Year + Age, data = future_age[which(future_age$Year >= 2014),], FUN = sum)
agg_pred_gam_h$Type <- "High Volatility Scenario"
colnames(agg_pred_gam_h) <- c("Year","Age","pr_b_vc","Type")

# Plot
plot_gam_h <- ggplot(data = agg_pred_gam_h, aes(y=pr_b_vc, x=Age, colour=as.factor(Year))) +
  geom_line() +
    theme_bw() +
  theme(text=element_text(family="serif"), legend.position = 'none', plot.title = element_text(hjust = 0.5)) +
  ggtitle("Middle-High Scenario linear trend") +
  xlab('Age') + ylab("Immigration") 
plot_gam_h

#Aveage Age
new_data_for <- rbind(agg_pred_gam_l, agg_pred_gam_m, agg_pred_gam_h)
new_data_for <- new_data_for %>%
  group_by(Year, Type) %>%
  mutate(cumsum = cumsum(pr_b_vc))
new_data_for <- new_data_for %>%
  group_by(Age, Type) %>%
  summarise(cumsum = sum(cumsum))

mean(new_data_for[which(new_data_for$Type=="Low Volatility Scenario"),]$cumsum) # 895777.7, corresponding age: 32
summary(new_data_for[which(new_data_for$Type=="Low Volatility Scenario"),]$cumsum) # 895777.7, corresponding age: 32
mean(new_data_for[which(new_data_for$Type=="Middle Volatility Scenario"),]$cumsum) # 895777.7, corresponding age: 32
summary(new_data_for[which(new_data_for$Type=="Middle Volatility Scenario"),]$cumsum) # 895777.7, corresponding age: 32
mean(new_data_for[which(new_data_for$Type=="High Volatility Scenario"),]$cumsum) # 869155.7, corresponding age: 33
summary(new_data_for[which(new_data_for$Type=="High Volatility Scenario"),]$cumsum) # 869155.7, corresponding age: 33


plot_sce <- ggplot(data = new_data_for, aes(y=pr_b_vc, x=Age, colour=Type)) +
  geom_line() +
  theme_bw() +
  facet_wrap(~ Year) +
  theme(text=element_text(family="serif"), legend.position = 'bottom', plot.title = element_text(hjust = 0.5)) +
  xlab('Age') + ylab("Immigration") 
plot_sce

ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Plot_forecasts_age.pdf", plot_sce,  width = 15, height = 10, units = "cm")


plots_for_age <- grid.arrange(plot_gam_l, plot_gam_m, plot_gam_h, ncol = 1)
ggsave(file = "/home/alice/Dropbox/GAM for demographic projection/R files/Paper/Plot_forecasts_age.pdf", plots_for_age,  width = 15, height = 15, units = "cm")


@

\begin{figure}
\centering
\makebox{\includegraphics[scale=.8]{Plot_forecasts_age.pdf}}
\caption{\label{fig:forecast age}Disaggregated monthly forecasts averaged by year over the period 2014 (orange) - 2021 (pink) for three different scenarios from the posterior predictive distribution.}
\end{figure}


\section{Conclusion}\label{conclusion}
Migration gained the reputation of being an unpredictable component of population change \citep{pijpers2008problematising, bijak2010bayesian}. The current paper tries to show how merging Bayesian statistics with semiparametric methods can help to handle the uncertainty surrounding the number of future incomers. \\
The core of the research lies in the choice to consider migration as a seasonal, rather than an annual, phenomenon and to exploit the monthly frequency of the output to deal with eventual trend-seasonal interactions. Even if such focus limits the methodology's application to countries which dispose of high frequency data, it also potentially opens new perspectives for analyzing new migration trends, which show high seasonality, like the ones of the recent refugee crisis \citep{eu2017quantitative}.\\
The message our results deliver is mainly twofold. On one side, the semiparametric models can represent an appealing alternative in presence of non-linear trend-cycle interactions. On the other, a Bayesian prospective can be proactively embraced through the choice of informative prior distributions to build forecast scenarios accounting for unprecedented events. The latter adds the possibility to condition the forecast on a set of macroeconomic projections \citep{bijak2010forecasting}. \\
Despite the model's choice belongs to the researcher discretionality, we have a few recommendations for future users in light of our investigation. Semiparametric models can be preferable in case of foreseen growing volatility since their flexibility can be fully exploited. However, they tend to exhibit increasing instability when dealing with long forecast horizons due to the usual overfitting of nonparametric models in out-of-sample performances. In such cases, a Bayesian perspective can help by setting boundaries on the prior distribution of the structural coefficients. 

\section*{Acknowledgements}\label{acknowledgements}
The research leading to these results has received funding from the Swiss National Science Foundation in the context of the NCCR on the move.
We also would like to thank Prof. Philippe Wanner, Dr. Reto Bürgin, Dr. Didier Ruedin for their comments. An earlier version was presented at the Population Association of America Annual Conference 2017. I am also grateful for the comments from the editor and two anonymous reviewers, which substantially improved the analysis.

\bibliography{Ref_proj}

\end{document}


